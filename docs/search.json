[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monte Carlo Methods Lecture Notes",
    "section": "",
    "text": "Preface\n⚠️ Note: These notes are very much under construction and subject to significant changes.\nThese are the lecture notes for Monte Carlo Methods (EN.553.433). The notes are loosely based on course materials by Dr. Jim Spall and the textbook by Rubinstein & Kroese (Rubinstein and Kroese 2017).\nThese notes represent only half of the course content. The remaining material consists of Jupyter notebooks assigned as homework, which are not published online.\nThanks to Kyle Beaty for help with cleaning these notes.\n\n\n\n\nRubinstein, R. Y., and D. P. Kroese. 2017. Simulation and the Monte Carlo Method. 3rd ed. USA: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "",
    "text": "2.1 What are Monte Carlo Methods?\nMonte Carlo methods are a powerful class of computational algorithms that harness the power of random sampling to solve complex numerical problems. Named after the famous Monte Carlo Casino in Monaco, these methods transform deterministic problems into probabilistic ones, allowing us to approximate solutions through statistical simulation.\nAt their core, Monte Carlo methods rely on repeated random sampling to obtain numerical results for problems that might be difficult or impossible to solve analytically. By generating large numbers of random samples and analyzing their statistical properties, we can approximate solutions with quantifiable uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#introduction",
    "href": "chapters/intro.html#introduction",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "",
    "text": "numerical integration and estimation,\nsimulating physical and mathematical systems,\noptimization.\n\n\n\nForming a mathematical model for a real world system, usually involving some random variables.\nDetermining the distribution of the random variables, either completely or partially.\nGenerating samples from the proposed distribution to run simulations.\nCompare results from simulated data with real world observations to update the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html",
    "href": "chapters/estimation/estimating_pi.html",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "",
    "text": "4 The Geometric Method\nWe begin with a classic Monte Carlo application: estimating the value of \\(\\pi\\) using geometric probability. This example beautifully illustrates the core principles of Monte Carlo simulation while providing an intuitive geometric interpretation.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#questions",
    "href": "chapters/estimation/estimating_pi.html#questions",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "",
    "text": "How accurate is this estimate of \\(\\pi\\) in terms of \\(N\\)?\nCan we improve the estimate by choosing a different sampling technique (other than uniformly sampling on the square)?",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/variance_reduction.html",
    "href": "chapters/estimation/variance_reduction.html",
    "title": "5  Variance Reduction",
    "section": "",
    "text": "5.0.1 Markov Chain Monte Carlo (MCMC)\nIn this notebook we will discuss two methods of variance reduction: importance sampling and antithetic variates. These methods are used to reduce the variance of an estimator, which can lead to more accurate and efficient estimates. We’ll start by reviewing the basic concepts of estimation theory.\nWhen the random variables \\(X_i\\) are not independent, the variance of the sum is given by\n\\[\n\\text{Var}(\\hat{\\ell}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\text{Var}(f(X_i)) + \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1, j \\neq i}^{N} \\text{Cov}(f(X_i), f(X_j)).\n\\]\nThere are \\(n^2\\) terms in the covariance sum, and in general, we cannot guarantee that the covariance is small. So the above confidence interval is not valid. In the case of ergodic Markov chains, the central limit theorem for Markov chains allows us to estimate the CI as \\[\n\\left[ \\hat{\\ell} -  z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}}, \\hat{\\ell} + z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}} \\right],\n\\] where \\(N_{eff}\\) is the effective sample size. We will not discuss the details of effective sample size here.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/variance_reduction.html#confidence-intervals",
    "href": "chapters/estimation/variance_reduction.html#confidence-intervals",
    "title": "5  Variance Reduction",
    "section": "",
    "text": "Inverse transform sampling\nAcceptance-rejection sampling\n\n\n\n\n\n\n5.1.1 Relative Confidence Intervals\nIt is common practice in simulation to use and report the relative widths of the confidence interval, defined as \\[\n\\text{Relative Width} = \\frac{\\text{Width of CI}}{\\hat{\\ell}} = 2z_{1-\\alpha/2} \\frac{S}{\\hat{\\ell}\\sqrt{N}}.\n\\]\nThe relative width of the confidence interval is a measure of the precision of the estimate.\n\n\n5.1.2 Markov Chain Monte Carlo (MCMC)\nWhen the random variables \\(X_i\\) are not independent, the variance of the sum is given by\n\\[\n\\text{Var}(\\hat{\\ell}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\text{Var}(f(X_i)) + \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1, j \\neq i}^{N} \\text{Cov}(f(X_i), f(X_j)).\n\\]\nThere are \\(n^2\\) terms in the covariance sum, and in general, we cannot guarantee that the covariance is small. So the above confidence interval is not valid. In the case of ergodic Markov chains, the central limit theorem for Markov chains allows us to estimate the CI as \\[\n\\left[ \\hat{\\ell} -  z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}}, \\hat{\\ell} + z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}} \\right],\n\\] where \\(N_{eff}\\) is the effective sample size. We will not discuss the details of effective sample size here.\n\nExample: Estimating \\(\\pi\\). In the first example we saw for estimating \\(\\pi\\), we used the Monte Carlo method to estimate \\(\\pi\\) by simulating random points in a square and counting the number of points that fall within a circle. The estimator we used was\n\\[\n\\hat{\\ell} = \\frac{4}{N} \\sum_{i=1}^{N} I(X_i),\n\\]\nwhere \\(I(X_i)\\) is an indicator function that is 1 if the point \\(X_i\\) is inside the circle and 0 otherwise. The variance of this estimator is given by \\[\n\\text{Var}(\\hat{\\ell}) = \\frac{16}{N} \\left( \\frac{\\pi}{4} \\left( 1 - \\frac{\\pi}{4} \\right) \\right).\n\\]\nNote: Using the variable \\(M\\) inside the estimator was a mistake in the worksheet.\n\n\nExample: Estimation of Rare-Event Probabilities. Consider estimation of the tail probability \\(\\ell = P(X &gt; \\gamma)\\) of some random variable \\(X\\) for a large number \\(\\gamma\\). We can use the following estimator: \\[\n\\hat{\\ell} = \\frac{1}{N} \\sum_{i=1}^{N} I_{&gt; \\gamma}(X_i),\n\\] where \\(I_{&gt; \\gamma}(X_i)\\) is an indicator function that is 1 if \\(X_i &gt; \\gamma\\) and 0 otherwise. The variance of this estimator is given by\n\\[\n\\text{Var}(\\hat{\\ell}) = \\frac{1}{N} \\left( \\ell (1 - \\ell) \\right).\n\\] The relative width of the confidence interval is given by \\[\n\\text{Relative Width} = \\frac{2z_{1-\\alpha/2} \\sqrt{\\ell (1 - \\ell)}}{\\hat{\\ell} \\sqrt{N}} \\approx \\frac{2z_{1-\\alpha/2}}{\\sqrt{N}} \\sqrt{\\frac{1 - \\ell}{\\ell}} \\approx \\frac{2z_{1-\\alpha/2}}{\\sqrt{N \\ell}}\n\\] When \\(\\ell\\) is small, the relative width of the confidence interval is large. This means that we need a large number of samples to get a good estimate of \\(\\ell\\).\n\n\nExample: Magnetization in a 2D Ising Model. In a previous worksheet, we used Gibbs sampling and Metropolis–Hastings sampling to estimate the magnetization of a 2D Ising model. We used the simple estimator \\[\n\\hat{M} = \\frac{1}{N} \\sum_{i=1}^{N} M(\\sigma_i),\n\\] where \\(M(\\sigma_i)\\) is the magnetization of the \\(i\\)-th sample.\nThe samples \\(\\sigma_i\\) are not independent, and so the variance of the estimator will be\n\\[\n\\text{Var}(\\hat{M}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\text{Var}(M(\\sigma_i)) + \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1, j \\neq i}^{N} \\text{Cov}(M(\\sigma_i), M(\\sigma_j)),\n\\] which leads to much larger confidence intervals than we would expect from independent samples.",
    "crumbs": [
      "Estimation Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/variance_reduction.html#importance-sampling",
    "href": "chapters/estimation/variance_reduction.html#importance-sampling",
    "title": "5  Variance Reduction",
    "section": "5.1 Importance Sampling",
    "text": "5.1 Importance Sampling\nImportance sampling is a method to reduce the variance of an estimator by changing the distribution from which we sample. The idea is to reduce the number of low probability events that contribute to the variance of the estimator.\nFor example, when estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\), if \\(\\ell\\) is small, then most of the samples will be in the region \\(X \\leq \\gamma\\), which contributes little to the estimate. However, we cannot just sample from the tail of the distribution as this would provide us no information about the rest of the distribution. Importance sampling allows us to sample from tail but then “fix” the estimate by weighting the samples appropriately.\n\nDefinition: Importance Sampling. Let \\(X\\) be a random variable with probability density function (pdf) \\(p(x)\\), and let \\(q(x)\\) be a proposal pdf such that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(p(x)\\). The importance sampling estimator of \\(\\ell = E[f(X)]\\) is given by \\[\n\\hat{\\ell} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) \\frac{p(X_i)}{q(X_i)},\n\\] where \\(X_1, X_2, \\ldots, X_N\\) are i.i.d. samples from the distribution with pdf \\(q(x)\\).\nFor clarity, we’ll denote the estimator in ?eq-crude-estimator as \\(\\hat{\\ell}_{crude}\\) and the importance sampling estimator as \\(\\hat{\\ell}_{IS}\\).\n\nSuppose \\(N = 1\\) so that the estimator is given by \\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}.\n\\] Note that here \\(X \\sim q(x)\\) and NOT \\(p(x)\\). We can check that this estimator is unbiased: \\[\n\\begin{aligned}\nE_q[\\hat{\\ell}_{IS}] &= E_q\\left[f(X) \\frac{p(X)}{q(X)}\\right] \\\\\n&= \\int f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\n&= \\int f(x) p(x) dx \\\\\n&= E[f(X)] \\\\\n&= \\ell.\n\\end{aligned}\n\\]\nHowever, \\[\n\\begin{aligned}\n\\text{Var}(\\hat{\\ell}_{IS})\n&\\neq \\text{Var}(\\hat{\\ell}_{crude}).\n\\end{aligned}\n\\]\nThis allows us to reduce the variance of the estimator by choosing \\(q(x)\\) appropriately.\nSuppose \\(f(X)\\) is a non-negative function. Then if we choose\n\\[\nq(x) \\propto p(x) f(x),\n\\]\nthen the importance sampling estimator for \\(N = 1\\)\n\\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}\n\\]\nis a constant and has zero variance! When \\(H\\) is not non-negative, we can show that\n\\[\nq(x) \\propto p(x) |f(x)|\n\\]\nminimizes the variance of the estimator \\(\\hat{\\ell}_{IS}\\).\nHowever, note that our goal is to estimate \\(\\ell = E[f(X)]\\), which means that we do not know \\(f(x)\\) in advance. So we cannot choose this \\(q(x)\\) in advance. Even if we could, we might not be able to sample from \\(q(x)\\) easily. In practice, we choose \\(q(x)\\) to be a distribution that is easy to sample from and that is “close” to \\(p(x)\\) in some sense.\n\nExample: Importance Sampling for Rare Events. Consider the problem of estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\) for a random variable \\(X\\) with standard normal distribution. We can use importance sampling to estimate this probability by choosing a proposal distribution \\(q(x)\\) that is concentrated in the tail region. One such distribution is the exponential distribution with parameter \\(\\lambda\\), which has pdf \\[\nq(x) = \\lambda e^{-\\lambda (x-2)}, \\quad x \\ge 2.\n\\] The plots below show the running averages of the crude and importance sampling estimators for \\(N = 2000\\) samples. The importance sampling estimator is much more stable and converges to the true value of \\(\\ell\\) much faster than the crude estimator.\n\n\n\n\n\n\n\n\n\n\nVariance of Importance Sampling Estimate: 0.00000\nRelative Error of Importance Sampling Estimate: 0.01234\nVariance of Crude Monte Carlo Estimate: 0.00001\nRelative Error of Crude Monte Carlo Estimate: 0.10862\n\n\n\n5.1.1 Remarks\n\nThe optimal choice of the proposal distribution \\(q(x)\\) is not always easy to find. Even if we can find it, we may not be able to sample from it easily. For importance sampling algorithm, we need to be able to sample from \\(q(x)\\). Often, we use a distribution that is easy to sample from and that is “close” to \\(|f(x)|p(x)\\) in some sense.\nUnlike rejection sampling and MCMC methods, for importance sampling we need to know the normalizing constant of the proposal distribution \\(q(x)\\) in order to compute the weights. This means that we are fairly limited in the choice of \\(q(x)\\). Some common choices are the exponential distribution, the normal distribution, and the uniform distribution, and a mixture of these distributions.\nIn order for the estimator to be well-defined, we need to ensure that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(f(x) p(x)\\). As in the case of rare-event estimation, the support of \\(f(x) p(x)\\) may be very small compared to the support of \\(p(x)\\). We only need \\(q(x)\\) to be positive in this smaller region.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/variance_reduction.html#antithetic-and-control-random-variates",
    "href": "chapters/estimation/variance_reduction.html#antithetic-and-control-random-variates",
    "title": "5  Variance Reduction",
    "section": "5.2 Antithetic and Control Random Variates",
    "text": "5.2 Antithetic and Control Random Variates\nIn this section, we will discuss two methods of variance reduction that are based on the idea of using correlated random variables: antithetic variables and control variates. Recall that the variance of a sum of two random variables is given by\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y) + 2\\text{cov}(X, Y).\n\\]\nOftentimes, having correlated random variables in undesirable as it reduces to an increase in variance and a decrease in the effective sample size. However, in some cases, we can use this correlation to our advantage.\n\n5.2.1 Antithetic Variates\nAntithetic variates are pairs of random variables that are negatively correlated. If we can find an estimator that uses sums to two antithetic random variables, we can reduce its variance.\nConsider the example of estimating the integral from ?sec-estimating-integrals. Suppose \\(f(x)\\) is a monotonic function over \\([a, b]\\). Then you’ll show on the homework that if \\(X \\sim U(a, b)\\) then\n\\[\n\\text{cov}(f(X), f(a + b - X)) \\le 0.\n\\]\nWe can see intuitively why this is happening - if \\(f(x)\\) is increasing the \\(f(b - x)\\) is decreasing and vice versa, and hence the two are negatively correlated. In this case, we can reduce the variance of the crude estimator by instead using\n\\[\n\\hat{\\ell}_{anti} = \\dfrac{(b - a)}{N} \\sum \\limits_{i = 1}^{2N} \\left(f(X) + f(a + b - X)\\right)\n\\]\nNote that if \\(X \\sim U(a, b)\\) then so is \\(b - X\\) and so \\(\\hat{\\ell}_{anti}\\) is an unbiased estimator.\n\nTheorem 5.1 \\(\\text{var}(\\hat{\\ell}_{anti}) \\le \\text{var}(\\hat{\\ell}_{crude})\\).\n\n\nExample 5.1 Example: Antithetic Variates. Consider the problem of estimating the integral \\(\\ell = \\int_0^1 (1 + x^2)^{-1} dx\\) using antithetic variates. The plots below show the running averages of the crude and antithetic variate estimators for \\(N = 100\\) samples. The antithetic variate estimator achieves a \\(50x\\) reduction in variance compared to the crude estimator.\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Monte Carlo Estimate: 0.785603, Variance: 1.442656e-04\nAntithetic Variates Estimate: 0.784906, Variance: 2.139616e-06\nVariance Reduction Factor: 67.43x\n\n\n\n\n5.2.2 Control Variates\nControl random variables are examples of random variables that are positively correlated. In this case, the difference\n\\[\n\\text{var}(X - Y) = \\text{var}(X) + \\text{var}(Y) - 2 \\text{cov}(X, Y)\n\\]\nwill have lower variance. Let \\(X \\sim p(x)\\). Suppose we want to estimate \\(\\ell = \\mathbb{E}[f(x)]\\) for some function \\(f(x)\\). We can use a control variate \\(Y = h(X)\\) for some function \\(h(x)\\) such that\n\n\\(\\mathbb{E}[h(X)]\\) is known, say \\(\\mathbb{E}[h(X)] = h_0\\).\n\\(h(x)\\) is strongly positively correlated with \\(f(x)\\), i.e., \\(\\text{cov}(f(X), h(X)) \\gg 0\\).\n\nThen we can use the control variate estimator\n\\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (h(X_i) - h_0)\\right]\n\\]\nwhere \\(\\beta\\) is a constant. It is easy to see that \\(\\hat{\\ell}_\\text{CV}\\) is an unbiased estimator of \\(\\ell\\). The variance of the control variate estimator is given by\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\ell}_\\text{CV})\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta^2 \\text{var}(h(X)) - 2 \\beta \\text{cov}(f(X), h(X))\\right] \\\\\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta \\text{var}(h(X)) \\left[ \\beta - 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\right]\\right]\n\\end{aligned}\n\\]\nBy choosing \\(\\beta &lt; 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\), we can reduce the variance of the control variate estimator. In practice, it is not easy to calculate the covariance between \\(f(X)\\) and \\(h(X)\\), so we\n\nPick a control variate \\(h(X)\\) that is strongly correlated with \\(f(X)\\), and whose expectation is known.\nExperiment with different values of \\(\\beta\\) to find the one that minimizes the variance of the control variate estimator.\n\n\nExample 5.2 In the example below, we estimate the integral of \\(x e^{-x}\\) over the interval \\([0, 1]\\) using control function \\(g(x) = x\\). We know that \\(\\mathbb{E}[g(X)] = \\frac{1}{2}\\) so the estimator is given by \\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (g(X_i) - \\frac{1}{2})\\right]\n\\] where \\(\\beta\\) is a constant and \\(X_i \\sim \\text{Uniform}(0, 1)\\) are i.i.d. We choose \\(\\beta \\approx 0.35\\) which minimizes the variance of the control variate estimator. This results in an 8-fold reduction in variance compared to the naive estimator.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/index.html",
    "href": "chapters/sampling/index.html",
    "title": "Sampling Techniques",
    "section": "",
    "text": "In this module, we will discuss how to sample from a probability distribution. Sampling from a probability distribution is a fundamental problem in statistics and machine learning. It is used in various applications like Monte Carlo methods, Bayesian inference, and reinforcement learning.\nWe’ll use this week to review many of the standard probability distributions and how to sample from them. We’ll also discuss the concept of the cumulative distribution function (CDF) and how it can be used to sample from a probability distribution using the inverse transform sampling method.\nFrom now on, we’ll assume that we have a reliable way to generate random numbers from the uniform distribution \\(U(0, 1)\\).\nTo understand what it means to sample from a probability distribution, let’s consider a simple example. Let \\(X\\) be a discrete random variable that takes values \\(1, 2, \\ldots, n\\) with probabilities \\(p_1, p_2, \\ldots, p_n\\). To sample from this distribution, we want to generate a random variable \\(X\\) such that \\(\\mathbb{P}(X = i) = p_i\\) for all \\(i = 1, 2, \\ldots, n\\) i.e. we want to select a random integer \\(i\\) with probability \\(p_i\\). If we generate enough samples \\(x_1, x_2, \\ldots, x_N\\) from this distribution, then the fraction of samples that are equal to \\(i\\) will be approximately equal to \\(p_i\\) for large \\(N\\).",
    "crumbs": [
      "Sampling Techniques"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html",
    "href": "chapters/sampling/rng.html",
    "title": "5  Random Number Generators",
    "section": "",
    "text": "5.1 Pseudo-random number generators\nIn the problem on estimating the value of \\(\\pi\\) using a Monte Carlo method, we encountered three main principles of Monte Carlo simulations:\nWe will keep these principles in mind and revisit them from time to time as we explore more advanced Monte Carlo simulations. For now, let’s focus on the first principle: generating random numbers.\nA random number generator is a function that produces a sequence of numbers that meet certain statistical requirements for randomness. True random number generators are based on physical processes that are fundamentally random, such as radioactive decay or thermal noise. Such systems are useful in cryptography and other applications where true randomness is important for security.\nBelow is an example of a true random number generator based on lava lamps called the “wall of entropy”. The lava lamps are used to generate random bits, which are then combined to generate random keys for encryption.\nThere are less exotic ways to generate random numbers. Computer chips have a hardware random number generator that uses thermal noise to generate random bits.\nHowever, true random number generators are slow and expensive. These are not useful for simulations as they are not reproducible. Instead, we use pseudo-random number generators (PRNGs) to generate random numbers for simulations.\nA pseudo-random number generator (PRNG) is a random number generator that produces a sequence of numbers that are not truly random, but are generated by a deterministic algorithm. The sequence of numbers produced by a PRNG is completely determined by the seed: if you know the seed, you can predict the entire sequence of numbers.\nThe reason for using PRNGs in simulations is to be able to reproduce the results. If you run a simulation with a given seed, you should get the same results every time. This is important for testing, debugging, and for sharing results with others.\nWe test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. A good PRNG should produce numbers that are indistinguishable from true random numbers. It is said to fool the statistical tests of randomness.\nPRNGs need to satisfy several statistical requirements to be useful in simulations. The most important requirements are:\nThese are all difficult requirements to satisfy simultaneously. In practice, most PRNGs are imperfect. You have to choose a PRNG that is appropriate for your application. You have to decide which statistical tests of randomness are most important for your application, and choose a PRNG that satisfies those tests.\nFor example,",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html#pseudo-random-number-generators",
    "href": "chapters/sampling/rng.html#pseudo-random-number-generators",
    "title": "5  Random Number Generators",
    "section": "",
    "text": "Uniformity: The numbers generated should be uniformly distributed between 0 and 1.\nIndependence: The numbers generated should be independent of each other. Knowing one number should not give you any information about the next number.\nSpeed: The PRNG should be fast. Generating random numbers is a common operation in simulations, so the PRNG should be as fast as possible.\n\n\n\n\nLinear congruential generators are simple and fast, but they have some statistical problems, as you’ll see in the exercises. These are good enough if you only need a few random numbers. These were the first PRNGs to be widely used and have stayed popular for a long time because of their simplicity.\nThe Mersenne Twister is a widely-used PRNG that is fast and has good statistical properties. It is the default PRNG in many programming languages, including Python. However, it is not suitable for cryptographic applications.\nCryptographically secure PRNGs are designed to be secure against cryptographic attacks. They are slower than other PRNGs, but they are necessary for applications where security is important.\n\n\n5.1.1 Cycle length\nIn practice, PRNGs generate a random integer between 0 and \\(M\\) for some large number \\(M\\), and then divide by \\(M\\) to get a random number between 0 and 1. The period or cycle length of a PRNG is the number of random numbers it can generate before it starts repeating itself. A good PRNG should have a long cycle length.\nHowever, relying solely on the cycle length to determine the quality of a PRNG is a mistake. A PRNG can have a long cycle length and still have poor statistical properties. For example, a PRNG that generates the sequence\n\\[1, 2, 3, 4, 5, \\ldots, M-1, M, 1, 2, 3, 4, 5,\\]\nhas a cycle length of \\(M\\), but it is a terrible PRNG!\n\n\n5.1.2 Linear Congruential Generator\nA linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized integers using a simple recurrence relation. The generator is defined by the recurrence relation:\n\\[\\begin{equation}\nX_{n+1} = (aX_n + c) \\mod m\n\\end{equation}\\]\nwhere: - \\(X_n\\) is the sequence of pseudo-randomized numbers - \\(a\\) is the multiplier - \\(c\\) is the increment\nThe above equation generates a sequence of integers between 0 and \\(m-1\\). To generate a sequence of random numbers between 0 and 1, we divide the sequence by \\(m\\):\n\\[\\begin{equation}\n  r_n = \\frac{X_n}{m}\n\\end{equation}\\]\nLCGs are simple to implement and are computationally efficient. However, as you’ll see in the homework, they have some statistical problems. The modulus \\(m\\) is the largest integer that the generator can produce. The modulus \\(m\\) is usually a power of 2, which makes the modulo operation fast.\nThe Hull-Dobell Theorem states that an LCG will have a full period for all seed values if and only if:\n\n\\(c\\) and \\(m\\) are relatively prime,\n\\(a - 1\\) is divisible by all prime factors of \\(m\\),\n\\(a - 1\\) is a multiple of 4 if \\(m\\) is a multiple of 4.\n\nIn the special case when \\(m\\) is a power of 2, the Hull-Dobell Theorem simplifies to:\n\n\\(c\\) is odd,\n\\(a\\) is congruent to 1 modulo 4.\n\nIt is in fact enough to take \\(c = 1\\). Thus when the modulus is a power of 2, a full period LCG will be of the form:\n\\[\\begin{equation}\nX_{n+1} = (aX_n + 1) \\mod 2^b,\n\\end{equation}\\]\nwith \\(a \\equiv 1 \\mod 4\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html#statistical-test-of-randomness",
    "href": "chapters/sampling/rng.html#statistical-test-of-randomness",
    "title": "5  Random Number Generators",
    "section": "5.2 Statistical test of randomness",
    "text": "5.2 Statistical test of randomness\nAs mentioned earlier, we test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. The more tests a PRNG passes, the better it is. There exist many “tests suites” that are used to evaluate the quality of PRNGs such as the Diehard tests, the TestU01 suite, and the NIST Statistical Test Suite.\nWe’ll look at the following three simple statistical tests of randomness from the NIST Statistical Test Suite:\n\nChi-square test: The chi-square test checks whether the observed frequency of the sequence is consistent with the expected frequency. If the sequence is truly random, then the observed frequency should be consistent with the expected frequency.\nMono-bit test: The mono-bit test checks whether the number of 0s and 1s in the sequence is approximately equal. If the sequence is truly random, then the number of 0s and 1s should be roughly equal. \nRuns test: The runs test checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. If the sequence is truly random, then the number of runs of 0s and 1s should be consistent with a random sequence.\n\nFor each of these tests, we’ll set up the null hypothesis and the alternative hypothesis as\n\n\\(H_0\\): The sequence is random.\n\\(H_1\\): The sequence is not random.\n\nWe’ll use the p-value to determine whether to reject the null hypothesis. If the p-value is less than the significance level \\(\\alpha\\), we reject the null hypothesis. If the p-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\n5.2.1 Chi-square test\nThe chi-square test is a one-sided statistical test that measures how well a sample of data matches a theoretical distribution. The chi-square test is used to test whether the observed data is consistent with the expected data. The test statistic is given by:\n\\[\\begin{equation}\n\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\\end{equation}\\]\nwhere: - \\(O_i\\) is the observed frequency of the \\(i\\)-th bin - \\(E_i\\) is the expected frequency of the \\(i\\)-th bin - \\(k\\) is the number of bins\nIn our case of testing the randomness of a sequence of random numbers, we divide the interval \\([0, 1]\\) into \\(k\\) bins and count the number of random numbers that fall into each bin. The expected frequency of each bin is \\(n/k\\), where \\(n\\) is the total number of random numbers.\nThe chi-square test is used to test the null hypothesis that the observed data is consistent with the expected data. If the chi-square test statistic is large, then the null hypothesis is rejected.\nThe critical value of the chi-square test statistic depends on the number of degrees of freedom. The degrees of freedom is given by \\(k-1\\). In python, you can use the scipy.stats.chi2.ppf function to perform the chi-square test.\nThe chi-square test is sensitive to the number of bins \\(k\\). If \\(k\\) is too small, then the test may not be sensitive enough to detect deviations from the expected distribution. If \\(k\\) is too large, then the test may be too sensitive and may detect deviations that are not significant.\n\n\nTo find the critical value, we find $x$ such that $P(X &gt; x) = 0.05$ for a chi-square distribution with 3 degrees of freedom. In the figure below, this is the area to the right of the green dashed line.\nIf your chi-square value is greater than the critical value, you can reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Mono-bit test\nThe mono-bit test is a two-sided statistical test that checks whether the number of 0s and 1s in the sequence is approximately equal. In a uniform binary sequence, roughly half the bits are 0s and half the bits are 1s.\nLet \\(X_i\\) be the \\(i\\)-th bit in the sequence. Then \\(X_i\\) is a Bernoulli random variable with probability \\(p = 0.5\\). Because the rv’s \\(X_i\\) are i.i.d., by the , their average\n\\[\\begin{equation}\n  X = \\dfrac{X_1 + X_2 + \\cdots + X_k}{k}\n\\end{equation}\\]\napproaches a normal distribution with mean \\(0.5\\) and variance \\(1/(4k)\\) as \\(k\\) approaches infinity. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by\n\\[\\begin{equation}\n  Z = \\dfrac{X - 0.5}{\\sqrt{1/(4k)}}.\n\\end{equation}\\]\nThe critical value of the z-test statistic depends on the significance level \\(\\alpha\\). In python, you can use the scipy.stats.norm.ppf function to perform the z-test. Note that because this is a two-sided test, we reject the null hypothesis if \\(Z &gt; z_{\\alpha/2}\\) or \\(Z &lt; -z_{\\alpha/2}\\) where \\(z_{\\alpha/2}\\) is the critical value of the z-test statistic at the significance level \\(\\alpha/2\\).\n\n\nTo find the critical value, we find $x$ such that $P(-x &lt; X &lt; x) = 0.95$ for a standard normal distribution. In the figure below, this is the area between the green dashed lines.\nIf your z-score is outside of the critical values, you can reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Runs test\nThe Wald-Wolfowitz runs test is a two-sided statistical test that checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. In a random sequence, the number of runs of 0s and 1s should be consistent with a random sequence.\nFor example, in the sequence 0011100110, there are 5 runs: 00, 111, 00, 11, 0. The runs test checks whether the number of runs of 0s and 1s is consistent with a random sequence.\nConsider a binary sequence of length \\(k\\). Define a random variable\n\\[\\begin{equation}\n  Z_i = \\begin{cases}\n  1 & \\text{ if the ${i+1}^{st}$ bit is different from the $i^{th}$ bit} \\\\\n  0 & \\text{ otherwise}\n  \\end{cases}\n\\end{equation}\\]\nOne can check that the number of runs in the sequence is given by the random variable\n\\[\\begin{equation}\n  R = 1 + \\sum_{i=1}^{k-1} Z_i.\n\\end{equation}\\]\nNote that the random variables \\(Z_i\\) are independent. Hence,\n\\[\\begin{align*}\n  \\text{E}[R]\n  &= 1 + \\sum_{i=1}^{k-1} \\text{E}[Z_i] \\\\\n  \\text{Var}[R]\n  &= \\sum_{i=1}^{k-1} \\text{Var}[Z_i].\n\\end{align*}\\]\nThe calculation of this expectation and variance is left as an exercise.\nWe assume that for large \\(k\\) the number of runs is approximately normally distributed. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by\n\\[\\begin{equation}\n  Z = \\dfrac{R - E[R]}{\\sigma[R]}.\n\\end{equation}\\]\nWe reject the null hypothesis if \\(Z &gt; z_{\\alpha/2}\\) or \\(Z &lt; -z_{\\alpha/2}\\) where \\(z_{\\alpha/2}\\) is the critical value of the z-test statistic at the significance level \\(\\alpha/2\\). Note that here we use \\(\\alpha/2\\) instead of \\(\\alpha\\) because this is a two-sided test.\n\n\n5.2.4 Spectral test\nThe previous tests fail to detect some of the problems with generating vectors using LCGs. One test for detecting these problems is the spectral test. The spectral test uses the Fast Fourier Transform (FFT) to analyze the spectral properties of the sequence. This test is beyond the scope of this class but you can see some examples in the homework. (This would be a good topic for a project!)\n\n\n5.2.5 Final remarks\nNote that by definition, a PRNG is not “random” in an absolute sense. The PRNG used in Python, the Mersenne Twister, is sufficiently random for most applications involving simulations. However, it is not suitable for cryptographic applications as it is possible to predict the entire sequence of numbers if you know a limited set of numbers. For cryptographic applications, you should use a cryptographically secure PRNG whose future numbers cannot be predicted easily from past numbers.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/discrete.html",
    "href": "chapters/sampling/discrete.html",
    "title": "6  Discrete Distributions",
    "section": "",
    "text": "6.1 Discrete Case\nLet’s consider a simple example where \\(X\\) is a discrete random variable that takes values \\(1, 2, 3\\) with probabilities \\(0.2, 0.3, 0.5\\) respectively. To sample from this distribution, we can use the following algorithm:\nThis algorithm can be easily extended to the case where \\(X\\) takes \\(n\\) values. This algorithm can be interpreted as a special case of the inverse transform sampling method described below.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/discrete.html#discrete-case",
    "href": "chapters/sampling/discrete.html#discrete-case",
    "title": "6  Discrete Distributions",
    "section": "",
    "text": "Generate a random number \\(u\\) from the uniform distribution \\(U(0, 1)\\).\nIf \\(u \\leq 0.2\\), set \\(X = 1\\).\nIf \\(0.2 &lt; u \\leq 0.5\\), set \\(X = 2\\).\nIf \\(0.5 &lt; u \\leq 1\\), set \\(X = 3\\).\nReturn \\(X\\).\n\n\n\n6.1.1 Binomial Distribution\nLet \\(X\\) be a random variable with binomial distribution with parameters \\(n\\) and \\(p\\). The probability mass function of \\(X\\) is given by\n\\[\\begin{align*}\n\\mathrm{Binomial}(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\ldots, n.\n\\end{align*}\\]\nOne way to sample from the binomial distribution is to treat it as a discrete distribution and use the above algorithm. However, we can use the Bernoulli distribution to sample from the binomial distribution.\nIf \\(Y_1, Y_2, \\ldots, Y_n\\) are independent random variables with Bernoulli distribution with parameter \\(p\\), then the random variable \\(X = Y_1 + Y_2 + \\ldots + Y_n\\) has binomial distribution with parameters \\(n\\) and \\(p\\). This gives us a simple algorithm to sample from the binomial distribution:\n\nGenerate \\(n\\) random numbers \\(u_1, u_2, \\ldots, u_n\\) from the uniform distribution \\(U(0, 1)\\).\nSet \\(Y_i = 1\\) if \\(u_i \\leq p\\) and \\(Y_i = 0\\) otherwise for \\(i = 1, 2, \\ldots, n\\).\nCompute \\(X = Y_1 + Y_2 + \\ldots + Y_n\\).\nReturn \\(X\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html",
    "href": "chapters/sampling/inverse_transform.html",
    "title": "9  Inverse Transform Sampling",
    "section": "",
    "text": "9.1 Exponential Distribution\nConsider a continuous random variable \\(X\\) with probability density function \\(f(x)\\). Let \\(U\\) be a random variable with uniform distribution \\(U(0, 1)\\).\nLet \\(X\\) be a random variable with exponential distribution with rate parameter \\(\\lambda &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\n\\mathrm{Exp}(\\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0.\n\\end{align*}\\]\nThe cumulative distribution function of \\(X\\) is given by\n\\[\\begin{align*}\nF(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0.\n\\end{align*}\\]\nThe inverse of the cumulative distribution function is given by\n\\[\\begin{align*}\nF^{-1}(u) = -\\frac{1}{\\lambda} \\log(1 - u).\n\\end{align*}\\]\nHence, the inverse transform sampling method can be used to sample from the exponential distribution.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#weibull-distribution",
    "href": "chapters/sampling/inverse_transform.html#weibull-distribution",
    "title": "9  Inverse Transform Sampling",
    "section": "9.2 Weibull Distribution",
    "text": "9.2 Weibull Distribution\nThe Weibull distribution is a generalization of the exponential distribution. Let \\(X\\) be a random variable with Weibull distribution with shape parameter \\(k &gt; 0\\) and scale parameter \\(\\lambda &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} e^{-(x/\\lambda)^k}, \\quad x \\geq 0.\n\\end{align*}\\]\nThe inverse transform sampling method can be used to sample from the Weibull distribution.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#triangular-distribution",
    "href": "chapters/sampling/inverse_transform.html#triangular-distribution",
    "title": "9  Inverse Transform Sampling",
    "section": "9.3 Triangular Distribution",
    "text": "9.3 Triangular Distribution\nLet \\(X\\) be a random variable with triangular distribution supported over the interval \\([a,b]\\) with maximum value at \\(c \\in [a,b]\\). We can use the inverse transform sampling method to sample from the triangular distribution.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#normal-distribution",
    "href": "chapters/sampling/inverse_transform.html#normal-distribution",
    "title": "9  Inverse Transform Sampling",
    "section": "9.4 Normal Distribution",
    "text": "9.4 Normal Distribution\nThe standard normal distribution with mean \\(0\\) and variance \\(1\\) is given has the probability density function\n\\[\\begin{align*}\nN(0, 1) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}.\n\\end{align*}\\]\nIt is not possible to sample from the normal distribution using the inverse transform sampling method because the cumulative distribution function of the normal distribution does not have a closed-form inverse. However, there are other methods to sample from the normal distribution. One such method is the Box-Muller transform. The Box-Muller transform is based on the following idea. Consider a 2D random variable \\((X, Y)\\) with standard normal distribution. The joint probability density function of \\((X, Y)\\) is given by\n\\[\\begin{align*}\nf(x, y) = \\frac{1}{2\\pi} e^{-(x^2 + y^2)/2}, \\quad -\\infty &lt; x, y &lt; \\infty.\n\\end{align*}\\]\nLet \\(R = \\sqrt{X^2 + Y^2}\\) and \\(\\Theta = \\arctan(Y/X)\\) be the polar coordinates of \\((X, Y)\\). Then notice that \\(\\theta\\) is uniformly distributed in \\([0, 2\\pi]\\). We can calculate the cdf of \\(R\\) as follows:\n\\[\\begin{align*}\n\\mathbb{P}(R \\leq r) &= \\mathbb{P}(X^2 + Y^2 \\leq r^2) \\\\\n&= \\int_{x^2 + y^2 \\leq r^2} f(x, y) \\, dx \\, dy \\\\\n&= \\int_{x^2 + y^2 \\leq r^2} e^{-(x^2 + y^2)/2} \\, dx \\, dy \\\\\n&= \\int_{0}^{2\\pi} \\int_{0}^{r} \\frac{1}{2\\pi} e^{-r^2/2} r \\, dr \\, d\\theta \\\\\n&= 1 - e^{-r^2/2}.\n\\end{align*}\\]\nWe can invert this to get the inverse cdf of \\(R\\):\n\\[\\begin{align*}\nF_R^{-1}(u) = \\sqrt{-2 \\log(1 - u)}.\n\\end{align*}\\]\nWe can summarize the above discussion in the following theorem.\n\nTheorem 9.2 (Box-Muller Transform): Let \\(U_1, U_2\\) be independent random variables with uniform distribution \\(U(0, 1)\\). Let \\(R = \\sqrt{-2 \\log U_1}\\) and \\(\\Theta = 2\\pi U_2\\). Then, the random variables \\(X = R \\cos(\\Theta)\\) and \\(Y = R \\sin(\\Theta)\\) are independent and have standard normal distribution.\n\nNote that we are using \\(U_1\\) instead of \\(1-U_1\\) in the formula for \\(R\\). This is because \\(1-U_1\\) is also uniformly distributed in \\([0, 1]\\).\nThis gives us the following algorithm to sample from the normal distribution:\n\nGenerate two random numbers \\(u_1, u_2\\) from the uniform distribution \\(U(0, 1)\\).\nCompute \\(R = \\sqrt{-2 \\log u_1}\\) and \\(\\Theta = 2\\pi u_2\\).\nCompute \\(X = R \\cos(\\Theta)\\) and \\(Y = R \\sin(\\Theta)\\).\nReturn \\(X\\) (or \\(Y\\)).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#poisson-distribution",
    "href": "chapters/sampling/inverse_transform.html#poisson-distribution",
    "title": "9  Inverse Transform Sampling",
    "section": "9.5 Poisson Distribution",
    "text": "9.5 Poisson Distribution\nThe Poisson distribution with parameter \\(\\lambda &gt; 0\\) is a discrete distribution that models the number of events occurring in a fixed interval of time or space, where \\(\\lambda\\) is the average rate of events. In unit time \\(T\\), the expected number of events is \\(\\lambda T\\). The probability mass function of the Poisson distribution is given by\n\\[\\begin{align*}\n\\mathrm{Pois}(n) = \\frac{e^{-\\lambda} \\lambda^n}{n!}, \\quad n \\in \\mathbb{N}.\n\\end{align*}\\]\nThe pmf of the Poisson distribution measures the probability of observing \\(n\\) events in time \\(T\\).\n\n9.5.1 Relation between Poisson and Binomial Distribution\nThe Poisson distribution can be approximated by the binomial distribution when the number of trials \\(n\\) is large and the probability of success \\(p\\) is small. Let \\(X\\) be a random variable with binomial distribution with parameters \\(n\\) and \\(p\\). As \\(n \\to \\infty\\) and \\(p \\to 0\\) such that \\(\\lambda = np\\) remains constant, the pmf of \\(X\\) converges to the pmf of the Poisson distribution with parameter \\(\\lambda\\). This gives us a simple algorithm to sample from the Poisson distribution:\n\nSet \\(X = 0\\).\nChoose \\(n\\) be a large integer (something like \\(n &gt; 10\\lambda\\)).\nSet \\(p = \\lambda/n\\).\nGenerate a \\(X\\) according to the binomial distribution with parameters \\(n\\) and \\(p\\).\n\nThis is a fast method to sample from the Binomial approximation to the Poisson distribution and is good when \\(\\lambda\\) is small. For large \\(\\lambda\\), the method described below is more efficient as the number of trials \\(n\\) required for the binomial distribution to approximate the Poisson distribution becomes very large.\n\n\n9.5.2 Relation between Poisson and Exponential Distribution\nWe exploit the relation between the Poisson distribution and the exponential distribution to sample from the Poisson distribution. When events occur at a constant rate \\(\\lambda\\), the time between events follows an exponential distribution with rate parameter \\(\\lambda\\). More precisely,\n\nTheorem 9.3 (Inter-arrival Times):\nLet \\(X_1, X_2, \\ldots\\) be independent random variables with exponential distribution with rate parameter \\(\\lambda\\). Define\n\\[\\begin{align*}\nN = \\max \\left\\{ n : X_1 + X_2 + \\dots + X_n \\le 1 \\right\\}.\n\\end{align*}\\]\nThen, \\(N\\) has Poisson distribution with parameter \\(\\lambda\\).\n\nThe proof of this requires us to understand the relation between exponential and gamma distributions. We will skip the proof for now. The Poisson-Exponential connection gives us a simple algorithm to sample from the Poisson distribution:\n\nSet \\(S = 0\\) and \\(N = 0\\).\nWhile True:\n\nGenerate a random number \\(x \\sim \\mathrm{Exp}(\\lambda)\\).\nSet \\(S = S + x\\).\nIf \\(S &gt; 1\\), return \\(N\\).\nElse, set \\(N = N + 1\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#beta-distribution",
    "href": "chapters/sampling/inverse_transform.html#beta-distribution",
    "title": "9  Inverse Transform Sampling",
    "section": "9.6 Beta Distribution",
    "text": "9.6 Beta Distribution\nThe Beta distribution is a continuous distribution defined on the interval \\([0, 1]\\). Let \\(X\\) be a random variable with Beta distribution with parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = cx^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1,\n\\end{align*}\\]\nwhere \\(c= \\frac{(\\alpha + \\beta - 1)!}{(\\alpha-1)!(\\beta-1)!}\\) is the normalizing constant. (Note that when \\(\\alpha\\) and \\(\\beta\\) are not integers, we use \\(\\Gamma\\) function to as a generalization of the factorial function.)\nThis is a simple function supported over the interval \\([0, 1]\\). The Beta distribution is used as a prior distribution in Bayesian statistics. When \\(\\alpha\\) and \\(\\beta\\) are non-zero, the cdf of the Beta distribution does not have a closed-form expression. However, we can use the inverse transform sampling method to sample from the Beta distribution. Instead, we can use the Beta-Order Statistics connection to sample from the Beta distribution.\n\nDefinition 9.1 (Order Statistics): Let \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables. The \\(k\\)-th order statistic, denoted \\(X_{(k)}\\), is the \\(k\\)-th smallest value among \\(X_1, X_2, \\ldots, X_n\\).\n\n\nTheorem 9.4 (Beta-Order Statistics): Let \\(U_1, U_2, \\ldots, U_n\\) be independent random variables with uniform distribution \\(U(0, 1)\\). Then the random variable \\(X = U_{(k)}\\) has Beta distribution with parameters \\(\\alpha = k\\) and \\(\\beta = n - k + 1\\).\n\n\nProof. We’ll work out a partial proof of the theorem. Let \\(X = U_{(k)}\\). The cumulative distribution function of \\(X\\) is given by\n\\[\\begin{align*}\nF(x)\n&= \\mathbb{P}(U_{(k)} \\leq x) \\\\\n&= \\mathbb{P}( \\text{at least } k \\text{ variables among } U_1, U_2, \\ldots, U_n \\text{ are less than } x)  \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} x^i (1-x)^{n-i}.\n\\end{align*}\\]\nWe differentiate both sides to get the probability density function of \\(X\\):\n\\[\\begin{align*}\nf(x) &= \\frac{d}{dx} F(x) \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} \\frac{d}{dx}x^i (1-x)^{n-i} \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} i x^{i-1} (1-x)^{n-i} - \\binom{n}{i} (n-i) x^i (1-x)^{n-i-1}.\n\\end{align*}\\]\nThe rest of the proof involves checking that the higher terms in the alternating sum cancel out and only the first term with \\(i = k\\) remains. \\(\\blacksquare\\)\n\nThis theorem gives us a simple algorithm to sample from the Beta distribution:\n\nGenerate \\(n\\) random numbers \\(u_1, u_2, \\ldots, u_n\\) from the uniform distribution \\(U(0, 1)\\).\nSort the numbers in increasing order \\(u_{(1)} \\leq u_{(2)} \\leq \\ldots \\leq u_{(n)}\\).\nReturn \\(u_{(k)}\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#mixture-distributions",
    "href": "chapters/sampling/inverse_transform.html#mixture-distributions",
    "title": "9  Inverse Transform Sampling",
    "section": "9.7 Mixture Distributions",
    "text": "9.7 Mixture Distributions\nA mixture distribution is a probability distribution that is formed by taking a weighted sum of two or more probability distributions. Let \\(X\\) be a random variable that is a mixture of distributions \\(f_1(x), f_2(x), \\ldots, f_n(x)\\) with weights \\(w_1, w_2, \\ldots, w_n\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = w_1 f_1(x) + w_2 f_2(x) + \\ldots + w_n f_n(x).\n\\end{align*}\\]\nMixture distributions are used to model complex distributions that cannot be modeled by a single distribution. We can sample from a mixture distribution by sampling from the component distributions and then taking a weighted sum of the samples \\(X = Y_1\\) with probability \\(w_1\\), \\(X = Y_2\\) with probability \\(w_2\\), \\(\\ldots, X = Y_n\\) with probability \\(w_n\\).\nTo sample from a mixture distribution, we can use the following algorithm:\n\nSample from the discrete distribution \\([w_1, w_2, \\ldots, w_n]\\) to select a component distribution.\nSample from the selected component distribution.\nReturn the sample.\n\nNote that this is not the same as constructing a linear combination of the component distributions. For example, if \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\(w_1 X_1 + w_2 X_2\\) is is a normal distribution with mean \\(w_1 \\mu_1 + w_2 \\mu_2\\) and variance \\(w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2\\). This is not the same as a mixture of two normal distributions.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/other_distributions.html",
    "href": "chapters/sampling/other_distributions.html",
    "title": "10  Other Distributions",
    "section": "",
    "text": "10.1 Mixture Distributions\nThe Beta distribution is a continuous distribution defined on the interval \\([0, 1]\\). Let \\(X\\) be a random variable with Beta distribution with parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = cx^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1,\n\\end{align*}\\]\nwhere \\(c= \\frac{(\\alpha + \\beta - 1)!}{(\\alpha-1)!(\\beta-1)!}\\) is the normalizing constant. (Note that when \\(\\alpha\\) and \\(\\beta\\) are not integers, we use \\(\\Gamma\\) function to as a generalization of the factorial function.)\nThis is a simple function supported over the interval \\([0, 1]\\). The Beta distribution is used as a prior distribution in Bayesian statistics. When \\(\\alpha\\) and \\(\\beta\\) are non-zero, the cdf of the Beta distribution does not have a closed-form expression. However, we can use the inverse transform sampling method to sample from the Beta distribution. Instead, we can use the Beta-Order Statistics connection to sample from the Beta distribution.\nThis theorem gives us a simple algorithm to sample from the Beta distribution:\nA mixture distribution is a probability distribution that is formed by taking a weighted sum of two or more probability distributions. Let \\(X\\) be a random variable that is a mixture of distributions \\(f_1(x), f_2(x), \\ldots, f_n(x)\\) with weights \\(w_1, w_2, \\ldots, w_n\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = w_1 f_1(x) + w_2 f_2(x) + \\ldots + w_n f_n(x).\n\\end{align*}\\]\nMixture distributions are used to model complex distributions that cannot be modeled by a single distribution. We can sample from a mixture distribution by sampling from the component distributions and then taking a weighted sum of the samples \\(X = Y_1\\) with probability \\(w_1\\), \\(X = Y_2\\) with probability \\(w_2\\), \\(\\ldots, X = Y_n\\) with probability \\(w_n\\).\nTo sample from a mixture distribution, we can use the following algorithm:\nNote that this is not the same as constructing a linear combination of the component distributions. For example, if \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\(w_1 X_1 + w_2 X_2\\) is is a normal distribution with mean \\(w_1 \\mu_1 + w_2 \\mu_2\\) and variance \\(w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2\\). This is not the same as a mixture of two normal distributions.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Other Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/other_distributions.html#mixture-distributions",
    "href": "chapters/sampling/other_distributions.html#mixture-distributions",
    "title": "10  Other Distributions",
    "section": "",
    "text": "Sample from the discrete distribution \\([w_1, w_2, \\ldots, w_n]\\) to select a component distribution.\nSample from the selected component distribution.\nReturn the sample.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Other Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html",
    "href": "chapters/sampling/accept_reject.html",
    "title": "11  Rejection Sampling",
    "section": "",
    "text": "11.1 Accept-Reject Method v1\nThe accept-reject Method, also called rejection sampling, is a simple and general technique for generating random variables. It is based on the idea of sampling from a simple distribution and then rejecting the samples that are not in the desired distribution. This method is particularly useful when the desired distribution is difficult to sample from directly, but it is easy to evaluate the density function of the distribution.\nFor the accept-reject method, we need to recall the following definitions: Let \\(X\\) and \\(Y\\) be discrete random variables.\nThe joint distribution of \\(X\\) and \\(Y\\) is given by the probability mass function\n\\[\\begin{align*}\nf_{X,Y}(x,y) = \\mathbb{P}(X=x, Y=y).\n\\end{align*}\\]\nThe marginal distribution of \\(X\\) is given by the probability mass function\n\\[\\begin{align*}\nf_X(x) = \\mathbb{P}(X=x) = \\sum_{y} f_{X,Y}(x,y).\n\\end{align*}\\]\nThe conditional probability of \\(X\\) given \\(Y\\) is defined as\n\\[\\begin{align*}\nf_{X|Y}(x|y) = \\mathbb{P}(X=x|Y=y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}.\n\\end{align*}\\]\nWhen \\(X\\) and \\(Y\\) are continuous random variables, the definitions are similar, but we replace the probability mass functions with probability density functions.\nThe accept-reject method is based on the following theorem:\nThus in order to sample from a distribution \\(p(x)\\), we want to come up with a way to sample from the joint distribution given by (Equation 11.1). The accept-reject method is a way to do this.\nSuppose \\(p(x)\\) is a probability distribution from which we want to sample. Suppose further that \\(p(x)\\) is supported over the interval \\([a, b]\\) and is bounded by \\(M\\), i.e., \\(p(x) \\leq M\\) for all \\(x \\in [a, b]\\).\nThe simplest version of the accept-reject method is as follows:",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#accept-reject-method-v1",
    "href": "chapters/sampling/accept_reject.html#accept-reject-method-v1",
    "title": "11  Rejection Sampling",
    "section": "",
    "text": "Sample \\(x\\) uniformly from \\([a, b]\\).\nSample \\(y\\) uniformly from \\([0, M]\\).\nIf \\(y \\leq p(x)\\), return \\(x\\); otherwise, go back to step 1.\n\n\nExample 11.1 Consider the triangular distribution\n\\[\np(x) = \\begin{cases}\n4x, & \\text{if } 0 \\leq x \\leq 0.5, \\\\\n4(1-x), & \\text{if } 0.5 \\leq x \\leq 1 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\tag{11.2}\\]\nWe can use the accept-reject method to sample from this distribution. The density function is supported over \\([0, 1]\\) and is bounded by \\(M = 2\\).\n\n\n\n\n\n11.1.1 Efficiency\nNote that unlike the methods we have seen so far, the accept-reject method is probabilistic. The method generates uniformly distributed samples in the rectangle of area \\(M(b-a)\\), where \\(M\\) is the bound on \\(p(x)\\) and \\([a, b]\\) is the support of \\(p(x)\\). But because \\(p(x)\\) is a probability distribution, the area under the curve is 1. Thus the efficiency of the accept-reject method is given by\n\\[\\begin{align*}\n\\text{Efficiency} = \\frac{1}{M(b-a)}.\n\\end{align*}\\]\nIf \\(M\\) is large i.e. the probability distribution has a large peak, then the efficiency of the accept-reject method is low.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#accept-reject-method-v2",
    "href": "chapters/sampling/accept_reject.html#accept-reject-method-v2",
    "title": "11  Rejection Sampling",
    "section": "11.2 Accept-Reject Method v2",
    "text": "11.2 Accept-Reject Method v2\nA better version of the accept-reject method is obtained by replacing the enveloping rectangle with a enveloping curve. The closer the enveloping curve is to the distribution, the higher the efficiency of the method.\n\nDefinition 11.1 Majorizing Function: Let \\(p(x)\\) be a probability distribution. A function \\(g(x)\\) is said to majorize \\(p(x)\\) if \\(g(x) \\geq p(x)\\) for all \\(x\\) in the support of \\(p(x)\\).\nIn the accept-reject jargon, we call \\(p(x)\\) the target distribution and \\(g(x)\\) the proposal distribution. Note that a probability distribution can never majorize another probability distribution as the area under the curve is 1. But we can scale the proposal distribution by a constant \\(M\\) such that \\(Mg(x)\\) majorizes \\(p(x)\\).\n\n\nExample 11.2 Consider the triangular distribution given by (Equation 11.2). We saw that we can use the enveloping rectangle with \\(M = 2\\) to sample from this distribution. The Beta distribution \\(\\text{Beta}(2, 2)\\) majorizes the triangular distribution with constant \\(M = 4/3\\).\n\\[\\begin{align}\n\\mathrm{Beta}(2, 2) = 6x(1-x), \\quad x \\in [0, 1].\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\nIn order to run the accept-reject method, we need to sample uniformly from the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis. This can be done using the following theorem:\n\nTheorem 11.2 Majorizing Function: Let \\(g(x)\\) be a probability distribution and let \\(M\\) be a constant. Let \\(X\\) and \\(Y\\) be two random variables such that \\(X \\sim g(x)\\) and \\((Y | X = x)\\sim U(0, Mg(x))\\). Then the joint distribution of \\(X\\) and \\(Y\\) is uniform over the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis.\n\n\nProof. The conditional probability of \\(Y\\) given \\(X\\) is given by\n\\[\\begin{align*}\nf_{Y|X}(y|x) = \\begin{cases}\n\\frac{1}{Mg(x)}, & \\text{if } 0 \\leq y \\leq Mg(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\end{align*}\\]\nHence, the joint distribution of \\(X\\) and \\(Y\\) is given by\n\\[\\begin{align*}\nf_{X,Y}(x,y) = f_{Y|X}(y|x) g(x) = \\begin{cases}\n\\frac{1}{M}, & \\text{if } 0 \\leq y \\leq Mg(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\end{align*}\\]\nHence, the joint distribution is uniform over the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis.\nNote that we are seeing the constant \\(1/M\\) instead of 1 as the area under the curve \\(y = Mg(x)\\) is \\(M\\) and not 1.\n\nThis gives us a way to sample from the majorizing function \\(g(x)\\). We can then use the accept-reject method to sample from the target distribution \\(p(x)\\). The algorithm is as follows:\n\nSample \\(x\\) from \\(g(x)\\).\nSample \\(y\\) uniformly from \\([0, Mg(x)]\\).\nIf \\(y \\leq p(x)\\), return \\(x\\); otherwise, go back to step 1.\n\nSteps 2 and 3, are often rewritten as:\n\nSample \\(u\\) uniformly from \\([0, 1]\\).\nIf \\(u \\leq p(x)/(Mg(x))\\), return \\(x\\); otherwise, go back to step 1.\n\nFurthermore, taking a ratio of two small numbers can lead to numerical instability. We can avoid this by taking the logarithm of the ratio. The third step of the algorithm becomes:\n\nIf \\(\\log(u) \\leq \\log(p(x)) - \\log(Mg(x))\\), return \\(x\\); otherwise, go back to step 1.\n\n\nExample 11.3 Consider the triangular distribution given by (Equation 11.2). We saw that the Beta distribution \\(\\text{Beta}(2, 2)\\) majorizes the triangular distribution. We can use the accept-reject method to sample from the triangular distribution using the Beta distribution as the proposal distribution. This improves the efficiency of the method from \\(1/2\\) to \\(3/4\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#normalizing-constant",
    "href": "chapters/sampling/accept_reject.html#normalizing-constant",
    "title": "11  Rejection Sampling",
    "section": "11.3 Normalizing Constant",
    "text": "11.3 Normalizing Constant\nWe often find ourselves in a situation where we know the probability density function \\(f\\) up to a normalizing constant. For example, the gamma distribution has the density function\n\\[\\begin{align}\np(x) = c x^{\\alpha - 1} e^{-x/\\beta},\n\\end{align}\\]\nwhere \\(c\\) is a normalizing constant. We can compute \\(c\\) by integrating \\(p(x)\\) and setting the integral to 1. But as it turns out we do not need to know \\(c\\) to sample from the distribution. We can use the accept-reject method to sample from the distribution without knowing the normalizing constant. For this we need the following theorem:\n\nTheorem 11.3 Normalizing Constant: Let \\(p(x)\\) be any function with a finite integral \\(c\\). Let \\(X, Y\\) be two random variables having the joint distribution\n\\[\\begin{align}\nf_{X,Y}(x,y) = \\begin{cases}\n\\frac{1}{c}, & \\text{if } 0 \\leq y \\leq p(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\end{align}\\]\nThen the marginal distribution of \\(X\\) is given by \\(p(x)/c\\).\n\n\nProof. The marginal distribution of \\(X\\) is given by\n\\[\\begin{align*}\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) dy = \\int_{0}^{p(x)} \\frac{1}{c} dy = \\frac{p(x)}{c}.\n\\end{align*}\\]\n\\(\\blacksquare\\)\n\nThus sampling from a uniform distribution over the graph of \\(p(x)\\) and then finding the marginal distribution of \\(X\\) gives us a sample from the distribution \\(p(x)/c\\). This is a powerful result as it allows us to sample from a distribution without knowing the normalizing constant.\nIf we majorize \\(cp(x)\\) with a proposal distribution \\(Mg(x)\\), we can use the accept-reject method to sample from the distribution \\(p(x)/c\\) without knowing the normalizing constant. The efficiency in this case will be given by\n\\[\\begin{align*}\n\\text{Efficiency} = \\frac{\\text{area under the graph of } p(x)}{\\text{area under the graph of } Mg(x)} = \\frac{c}{M}.\n\\end{align*}\\]\nNote that there is no loss in efficiency due to the unknown normalizing constant as if we can majorize \\(p(x)\\) with \\(Mg(x)\\), we can majorize the normalized proposal density \\(p(x)/c\\) by \\(Mg(x)/c\\), giving us the same efficiency.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#final-remarks",
    "href": "chapters/sampling/accept_reject.html#final-remarks",
    "title": "11  Rejection Sampling",
    "section": "11.4 Final remarks",
    "text": "11.4 Final remarks\nIf the proposal distribution is not close to the target distribution, then the efficiency of the method is low. In higher dimensions, it is difficult to come up with a good proposal distribution.\nIn higher dimensions, we need more sophisticated methods to sample from the target distribution such as Markov Chain Monte Carlo (MCMC) methods. We’ll see some of these methods such as Metropolis-Hastings and Gibbs sampling in the future.\nAdaptive rejection sampling is another method that tries to find a good proposal distribution by using the samples generated so far. The idea is to use the samples to build a piecewise linear majorizing function. This function is then used as the proposal distribution for the accept-reject method. Because the cdf of a piecewise linear function is a quadratic function, it can be inverted easily. The method works for log-concave densities but has been extended to more general densities. This would be a good topic for a project.\n\n\n\nGilks, W. R., & Wild, P. (1992). Adaptive Rejection Sampling for Gibbs Sampling. Journal of the Royal Statistical Society. Series C (Applied Statistics), 41(2), 337–348. https://doi.org/10.2307/2347565",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html",
    "href": "chapters/sampling/gibbs_2d.html",
    "title": "12  Gibbs Sampling",
    "section": "",
    "text": "12.0.1 Joint Exponential Distribution\nGibbs Sampling is a MCMC algorithm that is used to sample from a joint distribution using conditional distributions. For now, we’ll focus on sampling in 2D, but the algorithm generalizes to higher dimensions.\nThe Gibbs sampling algorithm for sampling from a joint distribution \\(f_{X, Y}(X, Y)\\) is as follows:\nThe algorithm generates a sample path of length \\(N\\) of the Markov Chain as described in Section 12.1. If needed, we can discard the initial samples to ensure that the Markov Chain has converged to the stationary distribution.\nLet \\(X\\) and \\(Y\\) be two random variables with the truncated exponential distribution:\n\\[\nf_{X,Y}(x, y) = c e^{-\\lambda xy} \\text{ for } 0 \\le x \\le D_1, 0 \\le y \\le D_2,\n\\]\nwhere \\(c\\) is the normalization constant. We want to sample from the joint distribution \\(f_{X,Y}\\). One can show that the conditionals are,\n\\[\n\\begin{aligned}\nf_{X|Y}(x | y_0) &= c_{y_0} e^{-\\lambda xy_0} \\text{ for } 0 \\le x \\le D_1 \\\\\nf_{Y|X}(y | x_0) &= c_{x_0} e^{-\\lambda yx_0} \\text{ for } 0 \\le y \\le D_2,\n\\end{aligned}\n\\]\nwhere \\(c_{y_0}\\) and \\(c_{x_0}\\) are the normalization constants. We can easily sample from the two conditionals \\(f_{X|Y}\\) and \\(f_{Y|X}\\) using the inverse method function (even for truncated distributions). The Gibbs algorithm becomes",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#sec-gibbs-markov-chain",
    "href": "chapters/sampling/gibbs_2d.html#sec-gibbs-markov-chain",
    "title": "12  Gibbs Sampling",
    "section": "12.1 Markov Chain",
    "text": "12.1 Markov Chain\nThe Gibbs sampling algorithm generates a Markov Chain whose state space is the product space of the state spaces of the individual variables \\(\\Omega = \\Omega_X \\times \\Omega_Y\\). In the above examples, the state space is \\([0, D_1] \\times [0, D_2]\\) for the exponential distribution and \\(\\mathbb{R}^2\\) for the bivariate normal distribution. The transition matrix of the Markov Chain in discrete case is given by\n\\[\nP \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} \\right) = \\mathbb{P}(X_{i+1} = x' | Y_i = y) \\mathbb{P}(Y_{i+1} = y' | X_i = x').\n\\]\nIn the continuous case, the transition kernel is given by\n\\[\nK \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} \\right)\n= f_{X|Y}(x | y) f_{Y|X}(y | x').\n\\]\n\nTheorem 12.1 The Gibbs sampling algorithm generates a Markov Chain with the transition matrix \\(P\\) as described above. The joint distribution \\(f_{X, Y}\\) is a stationary distribution of the Markov Chain. Hence, if the Markov Chain converges to the stationary distribution, the samples generated by the Gibbs algorithm will be distributed according to \\(f_{X, Y}\\).\n\nWe’ll provide a proof of the above theorem in the next section.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html",
    "href": "chapters/sampling/mh.html",
    "title": "13  Metropolis–Hastings Algorithm",
    "section": "",
    "text": "13.1 Random Walks Metropolis–Hastings Algorithm\nMetropolis–Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a probability distribution. It is a type of a rejection sampling algorithm where we generate a sequence of samples from a target distribution by proposing a new sample and accepting or rejecting it based on a certain criterion. The algorithm is widely used in Bayesian statistics, statistical physics, and machine learning.\nWe’ll start a few examples to understand the algorithm and then discuss the general algorithm.\nConsider a region \\(\\Omega\\) in \\(\\mathbb{R}^n\\). Suppose we want to sample uniformly from this region. One way to do this is through rejection sampling. We envelope the region in simple shape like a cube and sample uniformly from the cube. If the sample lies in the region \\(\\Omega\\), we accept it, otherwise we reject it. The efficiency of this method depends on the ratio of the volume of the cube to the volume of the region \\(\\Omega\\). If the region is highly non-convex, this ratio can be very small and the rejection sampling can be very inefficient.\nAn alternative method is to use random walks. We start at a point \\(x_0\\) in the region \\(\\Omega\\) and take a random step in a random direction. If the new point \\(x_1\\) is in the region \\(\\Omega\\), we accept it, otherwise we stay at the point \\(x_0\\). We repeat this process to generate a sequence of points. This method is more efficient than rejection sampling for highly non-convex regions.\nThe rationale behind this is that if a point is in the region \\(\\Omega\\), then a point near it is also likely to be in the region. We can use this idea to sample from a probability distribution. The random walk Metropolis–Hastings algorithm for generating \\(N\\) samples from a region \\(\\Omega\\) is as follows:\nThis is an example of a Metropolis–Hastings algorithm. The acceptance criterion is that the new point \\(y\\) should be in the region \\(\\Omega\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#sec-random-walks",
    "href": "chapters/sampling/mh.html#sec-random-walks",
    "title": "13  Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Start at a point \\(x_0\\) in the region \\(\\Omega\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a random step \\(\\sigma x\\).\nCompute the new point \\(y\\) near \\(x_i\\).\nIf \\(y\\) is in the region \\(\\Omega\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\n\n13.1.1 Proposal Distribution\nThe missing piece in the above algorithm is the method to generate a new point \\(y\\) near the current point \\(x_i\\). This is done using a proposal distribution. The proposal distribution can be almost of any form, but it should be easy to sample from. The choice of the proposal distribution is crucial for the efficiency of the algorithm. A good proposal distribution should be able to explore the region \\(\\Omega\\) efficiently.\nIn the case of random walks, there are two common choices for the proposal distribution:\n\nGaussian proposal: We sample a new point from a Gaussian distribution centered at the current point \\(x_i\\) with a certain variance.\nUniform proposal: We sample a new point from a uniform distribution in a neighborhood of the current point \\(x_i\\).\n\n\nExample 13.1 Consider the L-shaped region in \\(\\mathbb{R}^2\\) as shown below. This is a non-convex region and rejection sampling would only provide an efficiency of \\(0.1 + 0.1 - 0.01 = 0.18\\). We’ll use the Metropolis–Hastings algorithm to sample from this region with higher efficiency.\n\n\n\n\n\n\n\n\n\nThe images below show scatter plots for the samples generated using the Metropolis–Hastings algorithm with four different choices of the Gaussian proposal distribution with standard deviations \\(1\\), \\(0.5\\), \\(0.2\\), and \\(0.01\\). The plots to the right are the running averages of the \\(x\\) and \\(y\\) coordinates. The scatter plot and the running averages provide a visual representation of the convergence of the samples to the target distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.2 Choosing the Proposal Distribution\nNotice that there is a tradeoff between the acceptance rate and the rate of convergence. In the above example,\n\nThe proposal distribution with \\(\\sigma = 1\\) has the lowest acceptance rate but the samples are well spread out and the underlying markov chain converges quickly.\nThe proposal distribution with \\(\\sigma = 0.01\\) has the highest acceptance rate but the samples are clustered and the underlying markov chain converges slowly. In the above simulation, even after \\(10^5\\) samples (most of which are accepted), the samples are still clustered.\n\nBoth of these are undesirable. We need to choose a proposal distribution that has a good balance between the acceptance rate and the rate of convergence.\nWe see that for \\(\\sigma = 0.5\\), the samples are well spread out and the underlying markov chain converges quickly to the target distribution. This is a good choice for the proposal distribution. However, the acceptance rate is \\(0.07\\) which is lower than the acceptance rate for naive rejection sampling AND the generated samples are correlated. It is better to use rejection sampling than to use this proposal distribution.\nFor \\(\\sigma = 0.2\\), the acceptance rate is \\(0.3\\) which is better than the acceptance rate for naive rejection sampling. The samples are well spread out and the underlying markov chain converges quickly, although not as quickly as for \\(\\sigma = 0.5.\\) For this example, \\(\\sigma = 0.2\\) is a good choice for the proposal distribution. We can fine tune this parameter to get a better acceptance rate, if needed.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#metropolishastings-algorithm",
    "href": "chapters/sampling/mh.html#metropolishastings-algorithm",
    "title": "13  Metropolis–Hastings Algorithm",
    "section": "13.2 Metropolis–Hastings Algorithm",
    "text": "13.2 Metropolis–Hastings Algorithm\nThe general algorithm for the Metropolis–Hastings algorithm is an algorithm for generating a sequence of samples from a probability distribution \\(p(x)\\).\n\n13.2.1 Structure of Metropolis–Hastings Algorithm\nThe above example illustrates the general structure of the Metropolis–Hastings algorithm for sampling from a probability distribution \\(p(x)\\). The algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y\\) close to \\(x_i\\).\nEvaluate the acceptance ratio \\(\\alpha (y | x_i)\\).\nAccept \\(y\\) with probability \\(\\alpha (y | x_i)\\).\n\nIf accepted, set \\(x_{i+1} = y\\).\nElse, set \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\n\n13.2.2 Proposal Distribution\nFor the MH algorithm, we need to specify a proposal distribution for generating “nearby points”. This is a joint distribution \\(q(x, y)\\) for two random variables \\(X\\) and \\(Y\\). However, for running the algorithm we only need the conditional distribution \\(q(y | x)\\). As such, it’s more common to say that the proposal distribution is the conditional distribution \\(q(y | x)\\). We think of \\(x\\) as the current point and \\(y\\) as the proposed point so that the proposal distribution \\(q(y | x)\\) is the distribution of the proposed point \\(y\\) given the current point \\(x\\).\nIn the Example 13.1, the proposal distribution \\(q(y|x)\\) was a Gaussian distribution centered at \\(x\\) with a chosen variance,\n\\[\\begin{align*}\nq(y | x) = \\mathcal{N}(y ; x, \\sigma^2 I).\n\\end{align*}\\]\nThe proposal distribution can be any distribution that is easy to sample from. The choice of the proposal distribution is crucial for the efficiency of the algorithm.\n\n\n13.2.3 Acceptance Criterion\nOnce we generate a proposal \\(y\\), we need to evaluate an acceptance criterion for \\(w\\). The acceptance criterion is based on the ratio of the target distribution \\(p(x)\\) and the proposal distribution \\(q(y | x)\\). We calculate the acceptance ratio as\n\\[\n\\alpha (y | x) = \\min \\left\\{ \\frac{p(y)}{p(x)} \\cdot \\frac{q(x | y)}{q(y | x)}, 1 \\right\\}.\n\\]\nThe complete Metropolis–Hastings algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y\\) from the proposal distribution \\(q(y | x_i)\\).\nCompute the acceptance ratio \\[\n\\alpha(y | x) = \\min\\left\\{ \\frac{p(y)}{p(x)} \\cdot \\frac{q(x | y)}{q(y | x)}, 1 \\right\\}.\n\\]\nAccept \\(y\\) with probability \\(\\alpha(y | x)\\):\n\nIf \\(y\\) is accepted, set \\(x_{i+1} = y\\).\nOtherwise, set \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\nWe can expand the algorithm further as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y \\sim q(y | x_i)\\).\nCompute \\[ \\alpha_1 = p(y) \\cdot q(x_i | y), \\quad \\alpha_2 = p(x_i) \\cdot q(y | x_i). \\]\nIf \\(\\alpha_1 \\geq \\alpha_2\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nGenerate a random number \\(u \\sim \\text{Uniform}(0, 1)\\).\nIf \\(u &lt; \\alpha_1 / \\alpha_2\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#symmetric-proposal-distribution",
    "href": "chapters/sampling/mh.html#symmetric-proposal-distribution",
    "title": "13  Metropolis–Hastings Algorithm",
    "section": "13.3 Symmetric Proposal Distribution",
    "text": "13.3 Symmetric Proposal Distribution\nOften we use a symmetric proposal distribution \\(q(y | x) = q(x | y)\\). In this case, the acceptance ratio simplifies to\n\\[\n\\alpha(y | x) = \\min \\left\\{ \\frac{p(y)}{p(x)}, 1 \\right\\}.\n\\]\nThe Metropolis algorithm is a special case of the Metropolis–Hastings algorithm where the proposal distribution is symmetric. The Metropolis algorithm is widely used in practice. The algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y \\sim q(y | x_i)\\).\nIf \\(p(y) \\geq p(x_i)\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nGenerate a random number \\(u \\sim \\text{Uniform}(0, 1)\\).\nIf \\(u &lt; p(y) / p(x_i)\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\nExample 13.2 If \\(p(x)\\) is a uniform distribution over the region \\(\\Omega\\), then the Metropolis algorithm reduces to the random walk Metropolis–Hastings algorithm. In this case, the acceptance ratio becomes\n\\[\n\\alpha(y | x) = \\begin{cases}\n1, & \\text{if } y \\in \\Omega, \\\\\n0, & \\text{if } y \\notin \\Omega.\n\\end{cases}\n\\]\nThis means that we always accept a proposal \\(y\\) if it is in the region \\(\\Omega\\) and reject it otherwise. This is equivalent to the random walk Metropolis–Hastings algorithm in Section 13.1.\nSeveral other algorithms, such as rejection sampling, random walks on graphs, and Gibbs sampling, can be viewed as special cases of the Metropolis–Hastings algorithm.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#metropolis-markov-chain",
    "href": "chapters/sampling/mh.html#metropolis-markov-chain",
    "title": "13  Metropolis–Hastings Algorithm",
    "section": "13.4 Metropolis Markov Chain",
    "text": "13.4 Metropolis Markov Chain\nIf the target distribution \\(p(x)\\) is defined over the sample space \\(\\Omega\\), then the Metropolis–Hastings algorithm generates a Markov chain with state space \\(\\Omega\\).\nFor simplicity,\n\nwe will analyze the Metropolis algorithm with a symmetric proposal distribution \\(q(y | x) = q(x | y)\\), and\nwe will assume that the target distribution \\(p(x)\\) is defined over a finite sample space \\(\\Omega\\).\n\nThe transition matrix for the Markov chain is given by\n\\[\n\\begin{aligned}\n\\mathbb{P}(X = b | X = a) = \\begin{cases}\nq(b | a) & \\text{if } p(b) \\geq p(a) \\text{ and } a \\neq b, \\\\\n\\frac{p(b)}{p(a)} \\cdot q(b | a) & \\text{if } p(b) &lt; p(a) \\text{ and } a \\neq b, \\\\\nq(a | a) + \\sum \\limits_{b \\in \\Omega, p(b) &lt; p(a)} q(b | a) \\cdot \\left(1 - \\frac{p(b)}{p(a)}\\right) & \\text{if } a = b.\n\\end{cases}\n\\end{aligned}\n\\]\nOne can check that this defines a valid transition matrix by showing that the sum of the transition probabilities for each state is \\(1\\).\n\\[\n\\begin{aligned}\n&\\sum \\limits_{b \\in \\Omega} \\mathbb{P}(X = b | X = a) \\\\\n&= \\sum \\limits_{p(b) \\ge p(a), a \\neq b} q(b | a)\n+ \\sum \\limits_{p(b) &lt; p(a)} \\frac{p(b)}{p(a)} \\cdot q(b | a)\n+ q(a|a)\n+ \\sum \\limits_{p(b) &lt; p(a)} q(b | a) \\cdot \\left(1 - \\frac{p(b)}{p(a)}\\right) \\\\\n&= \\sum \\limits_{b \\in \\Omega} q(b | a) \\\\\n&= 1.\n\\end{aligned}\n\\]\n\n13.4.1 Detailed Balance Equations\n\nTheorem 13.1 The Metropolis algorithm is reversible with respect to the target distribution \\(p(x)\\). This means that the Markov chain defined by the Metropolis algorithm satisfies the detailed balance equations:\n\\[\np(a) \\cdot \\mathbb{P}(X = b | X = a) = p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\]\n\n\nProof. We only need check the detailed balance equations for the case when \\(a \\neq b\\). The case when \\(a = b\\) is trivial. We make two cases:\n\nCase 1: \\(p(a) \\neq p(b)\\).\n\nWithout loss of generality, assume that \\(p(a) &lt; p(b)\\). Then we have\n\\[\n\\begin{aligned}\np(a) \\cdot \\mathbb{P}(X = b | X = a)\n&= p(a) \\cdot q(b | a) \\\\\n&= p(a) \\cdot q(a | b) \\\\\n&= p(b) \\cdot q(a | b) \\dfrac{p(a)}{p(b)} \\\\\n&= p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\end{aligned}\n\\]\n\nCase 2: \\(p(a) = p(b)\\). In this case, we have \\[\n\\begin{aligned}\np(a) \\cdot \\mathbb{P}(X = b | X = a)\n&= p(a) \\cdot q(b | a) \\\\\n&= p(b) \\cdot q(a | b) \\\\\n&= p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\end{aligned}\n\\] Thus, in both cases, we have shown that the detailed balance equations hold. This completes the proof.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html",
    "href": "chapters/applications/SDE.html",
    "title": "14  Stochastic Differential Equations",
    "section": "",
    "text": "14.1 Stochastic Processes\nIn this module, we will look at the Euler-Maruyama method for solving stochastic differential equations. Our focus will be on understanding numberical stability and convergence of the method.\nRecall that the most basic differential equation, which is at the foundation of theory of ordinary differential equations, is the first order ordinary differential equation (ODE) of the form\n\\[\n\\frac{dy}{dt} = \\lambda y\n\\tag{14.1}\\]\nwhere \\(\\lambda\\) is a constant. The solution to this ODE is given by \\[\ny(t) = y(0) e^{\\lambda t}.\n\\tag{14.2}\\]\nWe want to modify the ODE (Equation 14.1) to include noise \\[\n\\frac{dy}{dt} = \\lambda y + \\text{ noise}.\n\\]\nTo make sense of this, we’ll make two changes:\nA stochastic process is a time dependent random variable. More precisely, it is a function of the form\n\\[\nX(t, \\omega): \\mathbb{R} \\times \\Omega \\to \\mathbb{R}\n\\]\nwhere \\(\\Omega\\) is the sample space. For example, if \\(\\Omega\\) is the set of gas molecules in a room, then \\(X(t, i)\\) could be the position of the \\(i\\)-th molecule at time \\(t\\). For at fixed \\(t\\), \\(X(t, \\cdot)\\) is a random variable. For a fixed \\(\\omega\\), \\(X(\\cdot, \\omega)\\) is a function of time. This function, \\(X(\\cdot, \\omega)\\) is called a sample path of the stochastic process.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#stochastic-processes",
    "href": "chapters/applications/SDE.html#stochastic-processes",
    "title": "14  Stochastic Differential Equations",
    "section": "",
    "text": "14.1.1 Wiener Process\nA Wiener process, \\(W(t)\\), also known as Brownian motion, is a stochastic process with the following properties:\n\n\\(W(0) = 0\\).\nFor \\(0 \\leq s &lt; t\\), the increment \\(W(t) - W(s)\\) is normally distributed with mean \\(0\\) and variance \\(t-s\\).\nThe increments are independent.\nThe process is continuous but nowhere differentiable.\n\nWe can derive the properties of the Wiener process by taking the continuous limit of a random walk. Note that for all time \\(t\\), the mean of \\(W(t)\\) is \\(0\\). However the variance grows with time. Imagine a box of gas particles all starting at the origin without any initial velocity or external forces. As there is no external force or initial velocity, the center of mass of the gas particles will remain at the origin. However, the gas particles will spread out over time. The Wiener process models this spreading out of gas particles.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#stochastic-differential-equations",
    "href": "chapters/applications/SDE.html#stochastic-differential-equations",
    "title": "14  Stochastic Differential Equations",
    "section": "14.2 Stochastic Differential Equations",
    "text": "14.2 Stochastic Differential Equations\nA stochastic differential equation (SDE) is an equation of the form\n\\[\ndX(t) = a(X(t), t) dt + b(X(t), t) dW(t)\n\\]\nwhere \\(a\\) and \\(b\\) are functions of \\(X(t)\\) and \\(t\\). The term \\(a(X(t), t) dt\\) is the deterministic part of the equation and \\(b(X(t), t) dW(t)\\) is the stochastic part. The solution to an SDE is a stochastic process \\(X(t)\\). \\(W(t)\\) is the Wiener process.\nWe write the equation in this form because the Wiener process is not differentiable. We interpret the equation as saying\n\\[\nX(t) - X(0) = \\int_0^t a(X(s), s) ds + \\int_0^t b(X(s), s) dW(s).\n\\]\n\n14.2.1 Ito vs Stratonovich Interpretation\nWe can try to define the above integrals in the usual sense using Riemann sums. The first integral can be written as the limit\n\\[\n\\int_0^t a(X(s), s) ds = \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} a(X(t_i), t_i) \\Delta t_i\n\\]\nwhere \\(t_i\\) is some point in the interval \\([t_{i}, t_{i+1}]\\) and \\(\\Delta t = t/n\\). Even though function \\(a(X(s), s)\\) is a stochastic process, it is mathematically still just a function. The integral can be defined in the usual sense.\nHowever, the second integral is more problematic. We can try to define it as\n\\[\n\\int_0^t b(X(s), s) dW(s) = \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} b(X(t_i), t_i) (W(t_{i+1}) - W(t_i))\n\\]\nwhere \\(t_i\\) is some point in the interval \\([t_{i}, t_{i+1}]\\), \\(\\Delta t = t/n\\), and \\(W(t_{i+1}) - W(t_i)\\) is normally distributed with mean \\(0\\) and variance \\(\\Delta t\\). The problem is that this integral does not converge in the usual sense. Where the sum converges depends on which point in the interval \\([t_{i}, t_{i+1}]\\) we choose to evaluate the integrand. This happens because the Wiener process is not differentiable. There are two commonly used interpretations of the integral:\n\nIto Interpretation: In this interpretation, we use the left end point of the interval to evaluate the integrand. This is the most common interpretation.\nStratonovich Interpretation: In this interpretation, we use the midpoint of the interval to evaluate the integrand.\n\nIt is possible to convert between the two interpretations using the Ito formula. We will assume the Ito interpretation in this module.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#geometric-brownian-motion",
    "href": "chapters/applications/SDE.html#geometric-brownian-motion",
    "title": "14  Stochastic Differential Equations",
    "section": "14.3 Geometric Brownian Motion",
    "text": "14.3 Geometric Brownian Motion\nA simple example of a stochastic differential equation is the geometric Brownian motion. This is a model for the evolution of stock prices. The equation is\n\\[\ndX(t) = \\mu X(t) dt + \\sigma X(t) dW(t)\n\\]\nwhere \\(\\mu\\) is the drift and \\(\\sigma\\) is the volatility. This is one of the few SDEs for which we can find an exact solution. The solution to the Ito version of the equation is\n\\[\nX(t) = X(0) e^{(\\mu - \\sigma^2/2)t + \\sigma W(t)}.\n\\tag{14.3}\\]\nThis is a log-normal distribution whose mean is given by \\(X(0) e^{\\mu t}\\) and variance is given by \\(X(0)^2 e^{2\\mu t} (e^{\\sigma^2 t} - 1)\\). Note that when \\(\\sigma = 0\\), this reduces to a standard ODE. Unlike (Equation 14.2), the solution to the SDE has a quadratic term \\(\\sigma^2/2\\) in the exponent. This term appears due to Ito’s lemma, which is the stochastic analog of the chain rule.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#euler-maruyama-method",
    "href": "chapters/applications/SDE.html#euler-maruyama-method",
    "title": "14  Stochastic Differential Equations",
    "section": "14.4 Euler-Maruyama Method",
    "text": "14.4 Euler-Maruyama Method\nMost SDEs do not have analytical solutions. We need to solve them numerically. Similar to ODEs, some simple regularity conditions on the coefficients \\(a\\) and \\(b\\) imply that the SDE has a unique solution. Most common SDEs satisfy these conditions. However, unlike ODEs, the solution to an SDE is a stochastic process. We can’t just evaluate the solution at a few points to get an approximate solution. We need to generate several sample paths of the stochastic process to get an approximate solution.\nThe simplest method for solving SDEs is the Euler-Maruyama method. This is a stochastic analog of the Euler method for ODEs. The Euler-Maruyama method is a recursive method. Given the value of the stochastic process \\(X_n\\) at time \\(t_n\\), we can find the value of the process at time \\(t_{n+1} = t_n + \\Delta t\\) using the formula\n\\[\nX_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n\n\\]\nwhere \\(\\Delta t = t_{n+1} - t_n\\) and \\(\\Delta W_n = W(t_{n+1}) - W(t_n) \\sim \\mathcal{N}(0, \\Delta t)\\). This results in a simple algorithm for solving SDEs:\n\nInitialize \\(X_0\\).\nFor \\(n = 0, 1, 2, \\ldots, N-1\\), do\n\nGenerate a random number \\(\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)\\).\nCompute \\(X_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n\\).\n\nReturn \\(X_0, X_1, \\ldots, X_N\\).",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#convergence-of-the-euler-maruyama-method",
    "href": "chapters/applications/SDE.html#convergence-of-the-euler-maruyama-method",
    "title": "14  Stochastic Differential Equations",
    "section": "14.5 Convergence of the Euler-Maruyama Method",
    "text": "14.5 Convergence of the Euler-Maruyama Method\nThe EM method provides a numerical approximation to the solution of the SDE. We want to understand how good this approximation is.\nFix a time interval \\([0, T]\\). Divide the interval \\([0, T]\\) into \\(N\\) subintervals of length \\(\\Delta t = T/N\\). Let \\(t_i = i \\Delta t\\) so that \\(t_0 = 0\\) and \\(t_N = T\\). Let \\(X(t)\\) be the analytical solution to the SDE at time \\(t\\) with initial condition \\(X(0)\\). We first think of the EM method as generating a sequence of random variables \\(X_0, X_1, \\ldots, X_N\\) defined recursively by\n\\[\\begin{align*}\nX_0 &= X(0), \\\\\nX_{i+1} &= X_i + a(X_i, t_i) \\Delta t + b(X_i, t_i) \\Delta W_i.\n\\end{align*}\\]\nwhere \\(\\Delta W_i \\sim \\mathcal{N}(0, \\Delta t)\\).\nThen we are interested in the question of how good the sequence \\(X_0, X_1, \\ldots, X_N\\) is as an approximation to the solution \\(X(t_0), X(t_1), \\ldots, X(t_N)\\). If the EM method is a good approximation method, we should get\n\\[\nX_i \\to X(t_i) \\text{ as } \\Delta t \\to 0.\n\\]\nHowever, as \\(X_i\\) and \\(X(t_i)\\) are random variables, we need to be more precise about what we mean by convergence. There are two notions of convergence that we are interested in: weak convergence and strong convergence.\n\n14.5.1 Weak Convergence\nWith the setup as above, we say that the EM method converges weakly to the solution over the interval \\([0, T]\\) if\n\\[\n\\mathbb{E}[X_i] \\to \\mathbb{E}[X(t_i)] \\text{ as } \\Delta t \\to 0.\n\\]\nSince we are interested in using the EM method to approximate the solution to the SDE, we need more than just convergence in expectation. We need to know how fast the method converges. We say that the EM method converges weakly to the solution over the interval \\([0, T]\\) with order \\(p\\) if\n\\[\n|\\mathbb{E}[X_i] - \\mathbb{E}[X(t_i)]| \\leq C \\Delta t^p\n\\]\nFor some constant \\(C\\) that does not depend on \\(\\Delta t\\). Note that \\(C\\) is allowed to depend on \\(T\\) and \\(X(0)\\). We often write this as\n\\[\n|\\mathbb{E}[X_i] - \\mathbb{E}[X(t_i)]| = O(\\Delta t^p)\n\\] to emphasize the rate of convergence.\n\n\n14.5.2 Strong Convergence\nStrong convergence is the convergence of the sample paths. We say that the EM method converges strongly to the solution over the interval \\([0, T]\\) if\n\\[\nX_i \\xrightarrow{a.s.} X(t_i) \\text{ as } \\Delta t \\to 0.\n\\]\nRecall that convergence almost surely means that the probability of the event that the sequence \\(X_i\\) does not converge to \\(X(t_i)\\) is \\(0\\). By Markov’s inequality it is enough to say that the expected value of the distance between \\(X_i\\) and \\(X(t_i)\\) goes to \\(0\\).\n\\[\n\\mathbb{E}[|X_i - X(t_i)|] \\to 0 \\text{ as } \\Delta t \\to 0.\n\\]\nWe say that the EM method converges strongly to the solution over the interval \\([0, T]\\) with order \\(p\\) if\n\\[\n\\mathbb{E}[|X_i - X(t_i)|] = O(\\Delta t^p)\n\\]\nIn the homework assignment, you will derive the rates of convergence for the EM method for the geometric Brownian motion empirically. However, it is necessary to derive these rates theoretically as for arbitrary SDEs, it is not possible to write an exact analytical solution. The rates of convergence tell us how close the analytical solution is to the numerical solution.\nAlso, you’ll only estimate \\(p\\) at the end of the interval i.e. \\(t = T\\). This is technically not correct. We should find a \\(p\\) that works for each time step \\(t_i\\). However, this is computationally expensive and the assumption is that error grows with time so that the error at the end of the interval is the largest.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#final-remarks",
    "href": "chapters/applications/SDE.html#final-remarks",
    "title": "14  Stochastic Differential Equations",
    "section": "14.6 Final remarks",
    "text": "14.6 Final remarks\nThe EM method has a slower rate of strong convergence compared to weak convergence. This is because even though the EM method is a natural extension of the Euler method for ODEs, there are second order terms in the Ito formula (chain rule for stochastic processes) that contribute to the error. This is fixed in the Milstein method, which is a second order method for solving SDEs. However the Milstein method is more computationally expensive compared to the EM method and has the same weak rate of convergence as the EM method.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html",
    "href": "chapters/applications/ising.html",
    "title": "15  Ising Model",
    "section": "",
    "text": "15.1 The Physical Ising Model\nThe Ising model is a simple model of ferromagnetism from statistical mechanics. The setup is as follows:\nNext we create a mathematical model to describe the system.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#the-physical-ising-model",
    "href": "chapters/applications/ising.html#the-physical-ising-model",
    "title": "15  Ising Model",
    "section": "",
    "text": "Particles: We have a lattice of \\(N\\) particles, each of which can be have one of two states of magnetization: spin up (\\(X = 1\\)) or spin down (\\(X = -1\\)). For simplicity, we assume the grid is “wrapped” so that the top and bottom edges are connected, and the left and right edges are connected. This means that each site has four neighbors. Such a grid is called a torus.\nCoupling constant: Two adjacent particles have a tendency to have spins aligned. This tendency is quantified by a coupling constant \\(J\\). If two adjacent particles have the same spin, the energy of the system is lowered by \\(-J\\), and if they have opposite spins, the energy is raised by \\(J\\). Any physical system tends to minimize its energy, so the system will tend to have neighboring particles with the same spin.\nExternal magnetic field: For the same of simplicity we’ll assume that there is no external force field.\nTemperature: Temperature adds randomness to the system. At 0K, all the particles have their spins aligned. As the temperature increases, the particles start to flip their spins. The higher the temperature, the more likely it is for neighboring particles to have opposite spins.\nMagnetization: If the system is in a state where most of the particles have the same spin, the system is said to be magnetized.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#the-mathematical-ising-model",
    "href": "chapters/applications/ising.html#the-mathematical-ising-model",
    "title": "15  Ising Model",
    "section": "15.2 The Mathematical Ising Model",
    "text": "15.2 The Mathematical Ising Model\nWe model the system using \\(N^2\\) random variables \\(X_{i, j}\\). Each \\(X_{i, j}\\) is a binary random variable that takes the value \\(1\\) or \\(-1\\). The configuration of the system is given by the \\(N^2\\) length vector \\(\\mathbf{X} = (X_{1, 1}, X_{1, 2}, \\ldots, X_{N, N})\\). There are \\(2^{N^2}\\) possible configurations of the system i.e. the state space has size \\(2^{N^2}\\). We can imagine each configuration as being the vertex of a hypercube in \\(N^2\\) dimensions with vertices \\((\\pm 1, \\pm 1, \\ldots, \\pm 1)\\). We’ll let the variable \\(\\sigma\\) denote a configuration of the system. So \\(\\sigma\\) is a vector of length \\(N^2\\) with entries in \\(\\{-1, 1\\}\\).\nIn order to describe the system mathematically, we need to provide the joint probability distribution of the random variables \\(X_{i, j}\\). This is done using the Hamiltonian of the system and the Boltzmann distribution.\n\n15.2.1 Hamiltonian\nThe Hamiltonian of the system is given by\n\\[\nH(\\sigma) = -J \\sum \\limits_{(i, j) \\text{ and } (i', j') \\text{ are neighbors}} X_{i, j} X_{i', j'}\n\\]\nwhere the sum is over all pairs of neighboring particles. The Hamiltonian is just a measure of the energy of the system. Note that because \\(J\\) is positive, the energy is minimized when neighboring particles have the same spin.\n\n\n15.2.2 Boltzmann Distribution\nThe probability of the system being in a particular configuration \\(\\sigma\\) is given by the Boltzmann distribution:\n\\[\nf(\\sigma) = \\frac{e^{-H(\\sigma)/T}}{Z}\n\\]\nwhere \\(T\\) is the temperature, and \\(Z\\) is the normalization constant called the partition function. This is the joint probability mass function of the random variables \\(X_{i, j}\\). The partition function \\(Z\\) is not easy to compute, but thankfully we do not need it to sample from the joint distribution.\n\n\n15.2.3 Low Temperature\nIf we plugin the partition function explicitly, we can see that the probability distribution becomes\n\\[\nf(\\sigma) = \\frac{e^{-H(\\sigma)/T}}{\\sum \\limits_{\\sigma'}e^{-H(\\sigma')/T}}.\n\\]\nAs \\(T \\to 0\\), the term with the highest value of \\(-H(\\sigma)\\) will dominate the sum in the denominator. But the term with the highest value of \\(-H(\\sigma)\\) is the one that minimizes \\(H(\\sigma)\\). So,\n\\[\n\\lim \\limits_{T \\to \\infty} f(\\sigma) = \\lim \\limits_{T \\to \\infty}  \\frac{e^{-H(\\sigma)/T}}{e^{-H(\\sigma)_{\\min}/T}} = \\lim \\limits_{T \\to \\infty}  e^{(H(\\sigma)_{\\min} - H(\\sigma))/T}.\n\\]\nIf \\(H(\\sigma)_{\\min} = H(\\sigma)\\), then the probability of the system being in that state is \\(1\\) and \\(0\\) otherwise. This means that at low temperatures, the system will be in a state that minimizes the energy. But as \\(T\\) increases, more and more high energy states become probable. So temperature can be thought of as adding variability to the system.\n\n\n15.2.4 Magnetization\nThe magnetization of the system is given by\n\\[\nM(\\sigma) = \\frac{1}{N^2} \\sum \\limits_{i, j} X_{i, j}\n\\]\nIf the system is magnetized, then \\(M(\\sigma)\\) will be close to \\(1\\) or \\(-1\\). Our goal is to find the expected value of the magnetization of the system. To do this we need to sample from the distribution \\(f(\\sigma)\\) and then compute the magnetization of each sample. The average of these magnetizations will be the expected magnetization of the system.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#gibbs-sampling",
    "href": "chapters/applications/ising.html#gibbs-sampling",
    "title": "15  Ising Model",
    "section": "15.3 Gibbs Sampling",
    "text": "15.3 Gibbs Sampling\nThere are two natural ways of sampling from the distribution \\(f(\\sigma)\\): Metropolis-Hastings and Gibbs sampling. In the Metropolis-Hastings approach, we randomly choose a particle and flip its spin. We then accept or reject the new configuration based on the Metropolis-Hastings acceptance probability. In the Gibbs sampling approach, we update the entire grid of particles one at a time. We update each particle by sampling from the conditional distribution of that particle given the rest of the particles. In this notebook, we’ll use Gibbs sampling.\n\n15.3.1 Conditional Distribution\nConsider a single spin \\(X_{i, j}\\). Let \\(\\hat{X}_{i, j}\\) denote the set of spins at all sites except \\((i, j)\\). We want to find the conditional distribution of \\(X_{i, j}\\) given \\(\\hat{X}_{i, j}\\). We’ll assume the following result without proof:\n\\[\n\\begin{aligned}\nP(X_{i, j} = 1 | \\hat{X}_{i, j}) &= \\dfrac{1}{1 + \\exp(-2 h_{i, j} / T)}, \\\\\nP(X_{i, j} = -1 | \\hat{X}_{i, j}) &= 1 - P(X_{i, j} = 1 | \\hat{X}_{i, j}).\n\\end{aligned}\n\\tag{15.1}\\]\nwhere \\(T\\) is the temperature of the system and \\(h_{i, j}\\) is the effective field at site \\((i, j)\\), given by\n\\[\n\\begin{aligned}\nh_{i, j}\n&= J \\sum_{(i', j') \\in \\text{neighbors}(i, j)} X_{i', j'} \\\\\n&= J \\left( X_{i-1, j} + X_{i+1, j} + X_{i, j-1} + X_{i, j+1} \\right)\n\\end{aligned}\n\\]\nThe conditional distribution in Equation 15.1 is a logistic function. When \\(h_{i, j}\\) is positive, the probability of \\(X_{i, j}\\) being \\(1\\) is greater than \\(0.5\\), and when \\(h_{i, j}\\) is negative, the probability of \\(X_{i, j}\\) being \\(1\\) is less than \\(0.5\\). This makes sense because if the effective field is positive, most of the neighbors of \\((i, j)\\) have spin \\(1\\), so it is likely that \\((i, j)\\) will also have spin \\(1\\) and if the effective field is negative, most of the neighbors of \\((i, j)\\) have spin \\(-1\\), so it is likely that \\((i, j)\\) will also have spin \\(-1\\).\n\n\n\n\n\n\n\n\n\n\n\n15.3.2 The Algorithm\n\nInitialize the grid of spins \\(X_i\\) for \\(i = 1\\) to \\(N\\) randomly.\nFor \\(i = 1\\) to \\(N\\)\n\nFor \\(j = 1\\) to \\(N\\)\n\nCompute the effective field \\(h_{i, j}\\).\nGenerate a random number \\(u\\) from a uniform distribution.\nSet \\(X_{i, j}\\) to \\(1\\) if \\(u &lt; \\dfrac{1}{1 + \\exp(-2 h_{i, j} / T)}\\) and \\(-1\\) otherwise.\n\n\nCompute the magnetization of the system.\nRepeat steps 2 and 3.\n\nEquilibrium/Mixing Time:\nEven when the grid size is modest, the dimensions of the state space is quite large, \\(2^{N^2}\\). This means that the Markov chain will take a long time to mix. Practically, this means that the initial configuration might be far from the stationary distribution i.e. the system has not reached thermal equilibrium. In this case it is important to discard the initial samples to not skew the magnetization estimate.\nOne simple way to this is to run the Gibbs sampler until the running average of the magnetization stabilizes. At that point, we can start using the samples to estimate the magnetization.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#phase-transitions",
    "href": "chapters/applications/ising.html#phase-transitions",
    "title": "15  Ising Model",
    "section": "15.4 Phase Transitions",
    "text": "15.4 Phase Transitions\nIt was discovered that the Ising model for a 2D grid exhibits a phenomenon called phase transition. If we slowly increase the temperature, we expect the magnetization to decrease. What is surprising is that the magnetization decreases smoothly until a certain temperature, after which it drops suddenly. This sudden drop is called a phase transition. The temperature at which this happens is called the critical temperature.\nIt is possible to calculate the critical temperature theoretically, in the limit when the size of the lattice \\(\\to \\infty\\). This limiting critical temperature for a square lattice is given by\n\\[\nT_c = \\dfrac{2J}{\\log(1 + \\sqrt{2})}.\n\\]\nOf course, in our simulation, we’ll be using finite sized lattices so we should expect some deviation from this result.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#simulation",
    "href": "chapters/applications/ising.html#simulation",
    "title": "15  Ising Model",
    "section": "15.5 Simulation",
    "text": "15.5 Simulation\nBelow are the results of simulating the Ising model on a \\(20 \\times 20\\) grid for \\(J = 1\\). The predicted critical temperature is \\(T_c = 2.269\\).\nWe can see that at \\(T=1.5\\), the system is magnetized and the magnetization is more or less constant and close to 1 or -1 over various runs. At \\(T=2.5\\), the system is not magnetized and the average magnetization is close to 0. It fluctuates wildly over various runs.\nIf we gradually increase the temperature from \\(T=1\\) to \\(T=3\\), we can see that the magnetization decreases smoothly until \\(T=2\\) after which it drops suddenly. This is the phase transition. Below the graph are the snapshots of the grid at different temperatures. One can see that at low temperatures, the grid is mostly monochromatic, but at high temperatures, the grid is more chaotic.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#final-remarks",
    "href": "chapters/applications/ising.html#final-remarks",
    "title": "15  Ising Model",
    "section": "15.6 Final Remarks",
    "text": "15.6 Final Remarks\nNot all graphs show a phase transition. The Ising model in 1D has no phase transition at any temperature. It was conjectured that the Ising model in 2D has no phase transition either, but this was proven wrong when an analytical solution was found by Lars Onsager in 1944. The Ising model in 3D also has a phase transition, but the critical temperature is not known exactly. Most critical temperatures are only known for limiting cases when the size of the lattice goes to infinity. For finite cases, Monte Carlo simulations are the only way to estimate the critical temperature.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/mmn.html",
    "href": "chapters/applications/mmn.html",
    "title": "16  M/M/n Queue",
    "section": "",
    "text": "The M/M/n queue is a discrete stochastic process that models a queue with n servers. There are three parameters:\n\n\\(n\\): the number of servers\n\\(\\lambda\\): the arrival rate of customers\n\\(\\mu\\): the service rate of each server\n\nThe “M” in M/M/n stands for “memoryless” or Markovian. The M/M/n queue models the following process: - customers arrive at the queue according to a Poisson process with rate \\(\\lambda\\). - Each customer is served by one of the n servers, and the service times are exponentially distributed with rate \\(\\mu\\). - If all servers are busy, the customer waits in line until a server becomes available.\nWe are interested in testing the performance of the queue as function of the three parameters. There are various performance measures we can compute, including: - The average number of customers in the waiting line - The average time a customer spends waiting in line - The usage of the servers - The probability that a customer has to wait in line\nThere are two approaches we can take to simulate an M/M/n queue\n\nProcess driven\nEvent driven\n\n\n\nSimulated 1000 customers\nAverage wait time     : 0.4640\nAverage sojourn time  : 1.1262\nServer utilization    : 0.6667 (theoretical)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>M/M/n Queue</span>"
    ]
  },
  {
    "objectID": "chapters/applications/bootstrap.html",
    "href": "chapters/applications/bootstrap.html",
    "title": "17  Bootstrap",
    "section": "",
    "text": "Sample Mean: 1.967\nTrue CI: [1.5979144  2.34577445]\nBootstrap CI: [1.62388954 2.35938801]\nNormal-Theory CI: [1.60463905 2.32850834]\n\n\n\n\n\n\n\n\n\n\n\nTrue CI: [1.5979144  2.34577445]\nBootstrap CI: [1.62388954 2.35938801]\nNormal-Theory CI: [1.60463905 2.32850834]\nNaive CI: [-1.65277273  5.58592012]\n\n\n\n\n\n\n\n\n\n\n\nSample Mean: 1.619\nTrue CI:          [0.96432733 3.4209794 ]\nBootstrap CI:     [0.98059327 2.28526628]\nNormal-Theory CI: [0.92412973 2.31300113]",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "23  References",
    "section": "",
    "text": "Rubinstein, R. Y., and D. P. Kroese. 2017. Simulation and the Monte\nCarlo Method. 3rd ed. USA: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html",
    "href": "appendices/markov_chains.html",
    "title": "Appendix B — Markov Chains",
    "section": "",
    "text": "B.1 Definitions\nIn this module, we will learn about Markov Chains. A Markov Chain is a discrete-time stochastic process that satisfies the Markov property. The Markov property states that the future state of the process depends only on the current state and not on the sequence of events that preceded it. In other words, the future is conditionally independent of the past given the present.\nThe transition matrix of a Markov chain is a square matrix whose \\((i, j)\\)-th entry is the probability of transitioning from state \\(i\\) to state \\(j\\) in one time step.\nThroughout this notebook, we’ll let \\(X_0, X_1, X_2, \\ldots\\) be a Markov chain with state space \\(\\Omega = \\{1, 2, \\ldots, N\\}\\) and transition matrix \\(P\\).\nWe can represent a Markov chain by a directed graph called a state diagram. Each state is represented by a node, and the transition probabilities are represented by directed edges between the nodes. The transition matrix can be derived from the state diagram by assigning the transition probabilities to the corresponding entries of the matrix.\nThe probability distribution of the Markov chain at time \\(n\\) is a row vector \\(\\pi_n\\) whose \\(i^{th}\\) entry is \\(\\mathbb{P}(X_n = i)\\) for each \\(i \\in \\Omega\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#definitions",
    "href": "appendices/markov_chains.html#definitions",
    "title": "Appendix B — Markov Chains",
    "section": "",
    "text": "Definition B.1 Markov Chain: A Markov chain is a sequence of random variables \\(X_0, X_1, X_2, \\ldots\\) satisfying the following properties:\n\nState Space: The random variables \\(X_i\\) take values in a finite set \\(\\Omega\\) called the state space.\nMarkov Property: For all \\(n \\geq 0\\) and all states \\(i_0, i_1, \\ldots, i_{n+1} \\in \\Omega\\),\n\n\\[\\begin{align*}\nP(X_{n+1} = i_{n+1} \\mid X_0 = i_0, X_1 = i_1, \\ldots, X_n = i_n) = P(X_{n+1} = i_{n+1} \\mid X_n = i_n).\n\\end{align*}\\]\n\nTime Homogeneity: The transition probabilities \\(P(X_{n+1} = j \\mid X_n = i)\\) do not depend on \\(n\\).\n\n\n\n\nDefinition B.2 Transition Matrix: Let \\(X_1, X_2, X_3, \\ldots\\) be a Markov chain with state space \\(\\Omega = \\{1, 2, \\ldots, N\\}\\). The transition matrix \\(P\\) of the Markov chain is an \\(N \\times N\\) matrix whose \\((i, j)\\)-th entry is given by\n\\[\\begin{align*}\nP(i, j) = P(X_{n+1} = j \\mid X_n = i).\n\\end{align*}\\]\n\n\n\n\n\nTheorem B.1 Let \\(\\pi_n\\) be the probability distribution of the chain at time \\(n\\). Then,\n\\[\\begin{align*}\n\\pi_{n+1} = \\pi_n P.\n\\end{align*}\\]\nAnd hence,\n\\[\\begin{align*}\n\\pi_n = \\pi_0 P^n.\n\\end{align*}\\]\n\n\nProof. \\[\\begin{align*}\n\\pi_{n+1}(j) &= \\mathbb{P}(X_{n+1} = j) \\\\\n&= \\sum_{i \\in S} \\mathbb{P}(X_{n+1} = j \\mid X_n = i) \\mathbb{P}(X_n = i) \\\\\n&= \\sum_{i \\in S} \\pi_n(i) P(i, j) \\\\\n&= \\pi_n P(j).\n\\end{align*}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#stationary-distribution",
    "href": "appendices/markov_chains.html#stationary-distribution",
    "title": "Appendix B — Markov Chains",
    "section": "B.2 Stationary Distribution",
    "text": "B.2 Stationary Distribution\n\nDefinition B.3 A probability distribution \\(\\pi\\) is called a stationary distribution of a Markov chain with transition matrix \\(P\\) if \\(\\pi = \\pi P\\).\n\n\\(P\\) is guaranteed to have an eigenvalue of 1 because it’s row sum is 1 i.e. \n\\[\n  P \\vec{1} = \\vec{1},\n\\]\nwhere \\(\\vec{1}\\) is the vector of all ones. Since, there is a right eigenvector corresponding to the eigenvalue 1, there will be a left eigenvector as well. The left eigenvector is a stationary distribution of the Markov chain.\nIt is not hard to see that every eigenvalue of \\(P\\) is less than or equal to 1 in magnitude. Suppose \\(\\vec{v}\\) is a left eigenvector of \\(P\\) corresponding to an eigenvalue \\(\\lambda\\). Let \\(v_I\\) be the largest component of \\(\\vec{v}\\) in magnitude. Then, we have\n\\[\\begin{align*}\n\\lambda \\vec{v} &= \\vec{v} P \\\\\n\\implies\n\\lambda v_I &= \\sum_{j} v_j P(j, I) \\\\\n&\\leq \\sum_{j} |v_j| P(j, I) \\\\\n&\\leq \\sum_{j} |v_I| P(j, I) \\\\\n&= |v_I|\n\\end{align*}\\]\nThus, \\(|\\lambda| \\leq 1\\). In particular, this means that the Frobenius norm of \\(P\\) is less than or equal to 1 and for all vectors \\(\\vec{v}\\), we have\n\\[\n\\| \\vec{v} \\|_2 \\leq \\| \\vec{v} P \\|_2.\n\\]\nWe are particularly interested in the case when there is exactly one eigenvector with eigenvalue of magnitude 1 which would then be the stationary distribution of the Markov chain.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#fundamental-theorem",
    "href": "appendices/markov_chains.html#fundamental-theorem",
    "title": "Appendix B — Markov Chains",
    "section": "B.3 Fundamental Theorem",
    "text": "B.3 Fundamental Theorem\n\nDefinition B.4 We say that a Markov Chain is irreducible if for every pair of states \\(i, j \\in \\Omega\\), there exists an integer \\(n\\) such that \\(P^n(i, j) &gt; 0\\) i.e. it is possible to go from any state to any other state in a finite number of steps.\n\n\nDefinition B.5 We say that a state \\(i\\) is aperiodic if the greatest common divisor of the set \\(\\{n \\geq 1 : P^n(i, i) &gt; 0\\}\\) is 1. A Markov chain is called aperiodic if all its states are aperiodic.\n\nNote that sometimes we add a preliminary requirement that the set \\(\\{n \\geq 1 : P^n(i, i) &gt; 0\\}\\) is non-empty. This condition is called positive recurrence. We’ll assume this as a part of the definition of aperiodicity.\n\nDefinition B.6 A Markov chain is called ergodic if it is irreducible and aperiodic.\n\n\nTheorem B.2 Fundamental Theorem of Markov Chains: If a Markov chain is ergodic, then it has a unique stationary distribution \\(\\Pi\\). Moreover, in this case, for any initial distribution \\(\\pi_0\\), the distribution of the chain converges to \\(\\Pi\\) as \\(n \\to \\infty\\) i.e. \n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\pi_0 P^n = \\Pi,\n\\end{align*}\\]\nfor any initial distribution \\(\\pi_0\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#random-walk-on-graphs",
    "href": "appendices/markov_chains.html#random-walk-on-graphs",
    "title": "Appendix B — Markov Chains",
    "section": "B.4 Random Walk on Graphs",
    "text": "B.4 Random Walk on Graphs\nOur main example of a Markov chain is the random walk on a graph. Let \\(G = (V, E)\\) be a graph with vertex set \\(V\\) and edge set \\(E\\). Suppose you want to move from one vertex to another by following the edges of the graph. At each vertex, you choose an edge uniformly at random and move to the adjacent vertex. This process is called a random walk on the graph.\nThe random walk on \\(G\\) is a Markov chain with state space \\(\\Omega = V\\) and transition probabilities given by\n\\[\\begin{align*}\nP(i, j) =\n\\begin{cases}\n\\frac{1}{\\text{deg}(i)} & \\text{if } (i, j) \\in E, \\\\\n0 & \\text{otherwise},\n\\end{cases}\n\\end{align*}\\]\nwhere \\(\\text{deg}(i)\\) is the degree of vertex \\(i\\) i.e. the number of edges incident to \\(i\\).\n\nExample B.1 Consider a graph \\(G\\) with 4 vertices as shown below. The transition matrix of the random walk on \\(G\\) is given by\n\\[\\begin{align*}\n  P =\n  \\begin{pmatrix}\n    0 & 1 & 0 & 0 \\\\\n    1/3 & 0 & 1/3 & 1/3 \\\\\n    0 & 1/2 & 0 & 1/2 \\\\\n    0 & 1/2 & 1/2 & 0\n  \\end{pmatrix}.\n\\end{align*}\\]\nSuppose we start at vertex \\(A\\). This means that the initial distribution is \\(\\pi_0 = [1, 0, 0, 0]\\). The \\(n^{th}\\) distribution \\(\\pi_n\\) can be obtained by multiplying \\(\\pi_0\\) with the transition matrix \\(P^n\\), as shown below.\n\n\n\n\n\n\n\n\n\n\nTransition matrix of the Markov chain:\n[[0.         1.         0.         0.        ]\n [0.33333333 0.         0.33333333 0.33333333]\n [0.         0.5        0.         0.5       ]\n [0.         0.5        0.5        0.        ]]\n\n \nExample of evolution of a Markov chain:\nState at time 0 : [1, 0, 0, 0]\nState at time 1 : [0. 1. 0. 0.]\nState at time 2 : [0.333 0.    0.333 0.333]\nState at time 3 : [0.    0.667 0.167 0.167]\nState at time 4 : [0.222 0.167 0.306 0.306]\nState at time 5 : [0.056 0.528 0.208 0.208]\nState at time 6 : [0.176 0.264 0.28  0.28 ]\nState at time 7 : [0.088 0.456 0.228 0.228]\nState at time 8 : [0.152 0.316 0.266 0.266]\nState at time 9 : [0.105 0.418 0.238 0.238]\nState at time 10 : [0.139 0.344 0.259 0.259]\n\n\nIn HW, you’ll prove the following theorem about ergodicity of random walks on graphs.\n\nTheorem B.3 A random walk on a graph is\n\nIrreducible if and only if the graph is connected.\nAperiodic if and only if the graph is not bipartite.\n\nIf these conditions hold, then the stationary distribution of the random walk is given by\n\\[\\begin{align*}\n\\Pi(i) = \\frac{\\text{deg}(i)}{2|E|},\n\\end{align*}\\]\nwhere \\(\\text{deg}(i)\\) is the degree of vertex \\(i\\) and \\(|E|\\) is the number of edges in the graph.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#mixing-time",
    "href": "appendices/markov_chains.html#mixing-time",
    "title": "Appendix B — Markov Chains",
    "section": "B.5 Mixing Time",
    "text": "B.5 Mixing Time\nAssume that the Markov chain is ergodic, and hence has a stationary distribution \\(\\Pi\\). We know that any initial distribution \\(\\pi_0\\) converges to \\(\\Pi\\) as \\(n \\to \\infty\\). The mixing time measures the rate of this convergence.\n\nDefinition B.7 The total variation distance between two probability distributions \\(\\mu\\) and \\(\\nu\\) on a finite state space \\(\\Omega\\) is defined as\n\\[\\begin{align*}\n\\| \\mu - \\nu \\|_{\\text{TV}}\n&= \\frac{1}{2} \\sum_{i \\in \\Omega} |\\mu(i) - \\nu(i)| \\\\\n&= \\frac{1}{2} \\| \\mu - \\nu \\|_{L^1}.\n\\end{align*}\\]\n\nOne can show that\n\\[\\begin{align*}\n\\| \\mu - \\nu \\|_{\\text{TV}} = \\mathrm{sup} \\{ |\\mu(A) - \\nu(A)| : A \\subseteq \\Omega \\},\n\\end{align*}\\]\ni.e. \\(\\|\\mu - \\nu \\|_{\\text{TV}}\\) is the maximum difference in the probability of any event under the two distributions.\nWe use the total variation distance to measure the distance between the distribution of the Markov chain at time \\(n\\) and the stationary distribution. The mixing time of the Markov chain is defined as the smallest \\(N\\) such that for the stationary distribution \\(\\Pi\\) and any initial distributions \\(\\pi_0\\), we have\n\\[\\begin{align*}\n\\| \\pi_0 P^n - \\Pi \\|_{\\text{TV}} \\leq \\frac{1}{4},\n\\end{align*}\\]\nfor all \\(n \\geq N\\). The constant \\(\\frac{1}{4}\\) is arbitrary and can be replaced by any other constant in \\((0, 1)\\). This will only change the value of the mixing time by a constant factor and not the order of magnitude.\nWhen using Markov chains for sampling, we want the mixing time to be as small as possible. This ensures that the distribution of the chain is close to the stationary distribution after a small number of steps. We think of the time before the chain mixes as a transient phase - the chain has not yet reached equilibrium. This is a burn-in period where we discard the samples. The bigger the mixing time, the more the number of wasted samples in the burn-in phase.\nExample. Consider Example B.1 again. If we start at the vertex \\(A\\), by step 4 we have already reached the distribution \\(\\pi_4 = [0.222, 0.167, 0.306, 0.306]\\). The stationary distribution is \\(\\Pi = [1/8, 4/8, 2/8, 2/8]\\). The total variation distance between \\(\\pi_4\\) and \\(\\Pi\\) is \\(\\| \\pi_4 - \\Pi \\|_{\\text{TV}} = 0.21\\). This is less than \\(1/4\\), and hence the mixing time is 4.\nThis only computes the mixing time for the initial distribution \\(\\pi_0 = [1, 0, 0, 0]\\). In general, we need to compute the mixing time for all possible initial distributions. The mixing time is the maximum of these mixing times.\nIn practice, we either provide a theoretical bound on the mixing time or “visually” inspect the convergence of the chain to the stationary distribution. Running a simulation to compute the mixing time is computationally expensive and not commonly done.\n\n\n\n\n\n\n\n\n\n\nB.5.1 Connection to Spectral Theory\nContinuing the example from above, the matrix \\(I_4 - P\\) is called the normalized Laplacian matrix of the graph \\(G\\).\n\\[\\begin{align*}\n\\mathcal{L} = I_4 - P =\n  \\begin{pmatrix}\n    1 & -1 & 0 & 0 \\\\\n    -1/3 & 1 & -1/3 & -1/3 \\\\\n    0 & -1/2 & 1 & -1/2 \\\\\n    0 & -1/2 & -1/2 & 0\n  \\end{pmatrix}\n\\end{align*}\\]\nOne can show that 0 is an eigenvalue of \\(\\mathcal{L}\\) and all eigenvalues are non-negative. The second smallest eigenvalue of \\(\\mathcal{L}\\) is called the spectral gap of the graph (which could be 0).\nSpectral graph theory, in particular Cheeger inequalities, prove that there is an inverse relationship between the spectral gap of the graph and the mixing time of the random walk on the graph. The smaller the spectral gap, the larger the mixing time. You’ll explore this connection in the homework.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#sampling-from-a-markov-chain",
    "href": "appendices/markov_chains.html#sampling-from-a-markov-chain",
    "title": "Appendix B — Markov Chains",
    "section": "B.6 Sampling from a Markov Chain",
    "text": "B.6 Sampling from a Markov Chain\nThe algorithm to generate sample paths of length \\(n\\) of a Markov Chain is simple. Suppose \\(P\\) is the transition matrix of the Markov Chain with mixing time \\(t\\). The algorithm is as follows:\n\nStart at some initial state \\(X_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\)\n\nGenerate \\(X_{i+1} \\sim P(X_i , \\cdot )\\).\n\nDiscard the first \\(T\\) samples and return \\(X_{T+1}, X_{T+2}, \\ldots, X_N\\).\n\nWe can interpret this algorithm as generating \\(N-T\\) samples from the stationary distribution of the Markov Chain. The number of samples discarded is called the burn-in period.\nOne big issue with this algorithm is that the samples are highly correlated. If independence is important, selecting every 40th sample may be beneficial. However, this leads to a lot of wasted samples and it might not get rid of all the correlation. In practice, it is better to generate a large number of samples than to “thin” the samples.\n\nExample B.2 In the example below we generate a uniform distribution over \\([0, 14]\\) by generating a random walk over a cycle of length 15.\nThe autocorrelation plot below shows the correlation between samples at different lags i.e. \\(X_i\\) and \\(X_{i+k}\\) for different values of \\(k\\). We can see that the correlation decreases as \\(k\\) increases and stabilizes around \\(k = 40\\).\nWe can either discard the first 40 samples or select every 40-th sample to reduce the correlation. If independence is important, then we should select every 40-th sample. However, this leads to a lot of wasted samples and it might not get rid of all the correlation. This method is not preferred in practice. In practice, it is better to generate a large number of samples than to “thin” the samples.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#reversible-markov-chains",
    "href": "appendices/markov_chains.html#reversible-markov-chains",
    "title": "Appendix B — Markov Chains",
    "section": "B.7 Reversible Markov Chains",
    "text": "B.7 Reversible Markov Chains\nA Markov Chain is reversible with respect to a distribution \\(\\pi\\) if the following holds:\n\\[\n\\pi_i P(i, j) = \\pi_j P(j, i) \\quad \\text{for all } i, j.\n\\tag{B.1}\\]\nThis is saying that the probability of transitioning from \\(x\\) to \\(y\\) is the same as the probability of transitioning from \\(y\\) to \\(x\\). Equation B.1 is known as the detailed balance equation.\n\nTheorem B.4 (Detailed Balance Equation). If a Markov Chain is reversible with respect to a distribution \\(\\pi\\), then \\(\\pi\\) is a stationary distribution of the Markov Chain.\n\n\nProof. Suppose the Markov Chain is reversible with respect to \\(\\pi\\). Then,\n\\[\n\\begin{aligned}\n(\\pi P)_i &= \\sum_j \\pi_j P(j, i) \\\\\n&= \\sum_j \\pi_i P(i, j) \\\\\n&= \\pi_i \\sum_j P(i, j) \\\\\n&= \\pi_i.\n\\end{aligned}\n\\]\nThus, \\(\\pi\\) is the stationary distribution of the Markov Chain.\n\nEquation B.1 is a sufficient but not necessary condition for \\(\\pi\\) to be the stationary distribution of the Markov Chain. You can have a Markov Chain with a stationary distribution that is not reversible.\nNote that we did not use any properties of the Markov Chain in the proof of the theorem, except that the row sum of the transition matrix is \\(1\\). A better way to phrase this theorem would be to say that “if the row sum of the transition matrix is \\(1\\) and the detailed balance equation holds, then \\(\\pi\\) is an eigenvector of the transition matrix with eigenvalue \\(1\\).”\n\nExample B.3 Random Walks on Graphs. Consider a graph \\(G = (V, E)\\) with vertices \\(V\\) and edges \\(E\\). Let \\(P(i, j) = 1/d(i)\\) if \\((i, j) \\in E\\) and \\(0\\) otherwise, where \\(d(i)\\) is the degree of vertex \\(i\\). Then, the stationary distribution of the Markov Chain is \\(\\pi_i = d(i)/2|E|\\), where \\(|E|\\) is the number of edges in the graph.\nThe Markov Chain is reversible with respect to \\(\\pi\\). Consider two vertices \\(i\\) and \\(j\\). If \\((i, j) \\in E\\), then\n\\[\n\\begin{aligned}\n\\pi_i P(i, j) &= \\frac{d(i)}{2|E|} \\cdot \\frac{1}{d(i)} \\\\\n&= \\frac{1}{2|E|} \\\\\n&= \\frac{d(j)}{2|E|} \\cdot \\frac{1}{d(j)} \\\\\n&= \\pi_j P(j, i).\n\\end{aligned}\n\\]\nIf \\((i, j) \\notin E\\), then \\(\\pi_i P(i, j) = \\pi_j P(j, i) = 0\\).\n\nMany Markov chains encountered in practice are reversible with respect to some distribution. It is much easier to check the detailed balance equation than to compute the stationary distribution directly. Moreover, reversible markov chains can be analyzed using spectral methods and we can find good bounds on their mixing time.\n\nB.7.1 Reversibility and Symmetry\n\nTheorem B.5 If a Markov Chain is reversible with respect to a distribution \\(\\pi\\), then the matrix\n\\[\nQ = \\text{diag}(\\sqrt{\\pi}) \\: P \\: \\text{diag}(\\sqrt{\\pi^{-1}})\n\\]\nis symmetric. In particular, \\(P\\) is similar to the symmetric matrix \\(Q\\) and hence has real eigenvalues.\n\n\nProof. This is because\n\\[\n\\begin{aligned}\nQ(i, j)\n&= \\sqrt{\\pi_i} P(i, j) \\sqrt{\\pi_j^{-1}} \\\\\n&= \\sqrt{\\pi_i} \\dfrac{\\pi_j P(j, i)}{\\pi_i} \\sqrt{\\pi_j^{-1}} \\\\\n&= \\sqrt{\\pi_j} P(j, i) \\sqrt{\\pi_i^{-1}} \\\\\n&= Q(j, i).\n\\end{aligned}\n\\]\nSimilarly, \\(Q(j, i) = \\pi_j P(j, i)\\). As the Markov Chain is reversible with respect to \\(\\pi\\), we get \\(Q(i, j) = Q(j, i)\\). Thus, \\(Q\\) is symmetric.\n\nNow suppose \\(\\textbf{v}\\) is a left eigenvector of \\(Q\\) with eigenvalue \\(\\lambda\\). Then,\n\\[\n\\begin{aligned}\n\\textbf{v} Q &= \\lambda \\textbf{v} \\\\\n\\textbf{v} \\text{diag}(\\sqrt{\\pi}) \\: P \\: \\text{diag}(\\sqrt{\\pi^{-1}}) &= \\lambda \\textbf{v} \\\\\n\\implies \\textbf{v} \\text{diag}(\\sqrt{\\pi}) \\: P &= \\lambda \\textbf{v} \\: \\text{diag}(\\sqrt{\\pi}).\n\\end{aligned}\n\\]\nHence, \\(\\text{diag}(\\sqrt{\\pi}) \\textbf{v}\\) will be an eigenvector of \\(P\\) with the same eigenvalue. As \\(Q\\) is symmetric, it has real eigenvalues and orthogonal eigenvectors. It is easier to do spectral analysis of \\(Q\\) and use that to deduce properties of \\(P\\). For example, we can find the eigenvector corresponding to the largest eigenvalue of \\(Q\\) by solving the optimization problem\n\\[\n\\mathbf{v} = \\underset{\\mathbf{x} \\neq 0}{\\text{arg max}} \\dfrac{\\mathbf{x}^T Q \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}.\n\\]\nMultiplying the above vector \\(\\mathbf{v}\\) by \\(\\text{diag}(\\sqrt{\\pi})\\) gives us the stationary distribution for \\(P\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#transition-kernels",
    "href": "appendices/markov_chains.html#transition-kernels",
    "title": "Appendix B — Markov Chains",
    "section": "B.8 Transition Kernels",
    "text": "B.8 Transition Kernels\nThe sample spaces we encounter in MCMC methods are not discrete but continuous. Instead of a transition matrix, we use a transition kernel \\(K(x, y)\\) that gives the “probability of transitioning from state \\(x\\) to state \\(y\\)”. However, because the sample space is continuous, the probability of being in a particular state is zero. Instead, we use the density of the distribution at that point. The transition kernel satisfies the equation:\n\\[\\begin{align*}\n\\mathbb{P}(X_{n+1} \\in A \\mid X_n = x) = \\int_{A} K(x, y) \\, dy.\n\\end{align*}\\]\nAll the properties of Markov Chains that we discussed earlier can be extended to transition kernels. The fundamental theorem of Markov Chains becomes\n\nTheorem B.6 (Fundamental Theorem of Markov Chains for Kernels). If a Markov Chain with transition kernel \\(K\\) is\n\nIrreducible: For all \\(x, y\\), there exists \\(n\\) such that \\(K^n(x, y) &gt; 0\\).\nPositive recurrent: The expected return time to a state is finite.\nAperiodic: For all \\(x\\), \\(\\gcd\\{n : K^n(x, x) &gt; 0\\} = 1\\).\n\nThen, the Markov Chain has a unique stationary distribution \\(\\Pi\\) and for all initial distributions \\(\\pi_0\\),\n\\[\\begin{align*}\n\\int \\pi_0(x) K^n(x, y) \\, dx \\xrightarrow[TV]{} \\Pi(y) \\quad \\text{as } n \\to \\infty.\n\\end{align*}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#analyzing-convergence-of-markov-chains",
    "href": "appendices/markov_chains.html#analyzing-convergence-of-markov-chains",
    "title": "Appendix B — Markov Chains",
    "section": "B.9 Analyzing Convergence of Markov Chains",
    "text": "B.9 Analyzing Convergence of Markov Chains\nIn practice we need to decide how many samples to burn and how many samples to generate. We use various heuristics to decide this.\n\nB.9.1 Trace Plots\nA trace plot is simply a scatter plot of the samples generated by the Markov Chain. It is useful to see if the Markov Chain has converged. If the Markov Chain has converged, the trace plot should look like a cloud of points. If the Markov Chain has not converged, the trace plot will show a trend.\nBelow are the trace plots for Example B.2. We can see that the chain does not look uniform even after a 1000 samples but after 5000 samples it is starting to look uniform.\n\n\n\n\n\n\n\n\n\n\n\nB.9.2 Running Average\nThe running average is the average of the first \\(n\\) samples. It is useful to see if the Markov Chain has converged. If the Markov Chain has converged, the running average should stabilize around the true mean. If the Markov Chain has not converged, the running average will show a trend.\nBelow is the running average plot for Example B.2. We can see that the running average is stabilizing around the true mean around 3000 samples.\nRunning averages can be deceptive and show stability even when the Markov Chain has not converged. It is better to use multiple diagnostics to check for convergence. They are better at telling when the Markov Chain has not converged than when it has converged.\n\n\n\n\n\n\n\n\n\n\n\nB.9.3 Autocorrelation Function\nOne method for finding the burn-in period is to use the autocorrelation function. The autocorrelation function of a sequence of numbers \\(x = (x_0, x_1, \\dots, x_n)\\) at lag \\(k\\) is defined as\n\\[\n\\text{ACF}(k) = \\text{Corr}(x[k: \\:], x[\\: :-k])\n\\]\nwhere by \\(x[k:]\\) we mean the subsequence \\(x_k, x_{k+1}, \\dots, x_n\\) and by \\(x[:-k]\\) we mean the subsequence \\(x_0, x_1, \\dots, x_{n-k}\\). It is the correlation between the sequence \\(x\\) and the same sequence shifted by \\(k\\). One way to choose a burn-in period is to pick a threshold \\(\\epsilon\\) and find the smallest \\(T\\) such that \\(\\text{ACF}(T) &lt; \\epsilon\\). Then to be conservative choose a burn-in period of \\(2T\\) or \\(3T\\).\nBelow is the autocorrelation plot for Example B.2. We can see that the correlation decreases as \\(k\\) increases and drops below \\(0.1\\) around \\(17\\). So a burn-in period of \\(40\\) is a good conservative choice.\nThis is just one way to choose the burn-in period. There are many other methods and the choice depends on the application. We will stick to this method for the rest of the course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/hidden_markov.html",
    "href": "appendices/hidden_markov.html",
    "title": "Appendix C — Hidden Markov Chains",
    "section": "",
    "text": "D Particle Filter with Resampling\n\nInputs:\n\nObservations \\(\\{y_1, y_2, \\dots, y_T\\}\\)\n\nNumber of particles \\(N\\)\n\nTransition model \\(p(x_t \\mid x_{t-1})\\)\n\nEmission model \\(p(y_t \\mid x_t)\\)\n\nInitial distribution \\(p(x_1)\\)\n\nInitialization: For \\(i = 1\\) to \\(N\\)\n\nSet \\(x_0^{(i)} \\leftarrow x_0\\)\n\nSet weight \\(w_1^{(i)} \\leftarrow 1/N\\)\n\nFor \\(t = 1\\) to \\(T\\)\n\nFor \\(i = 1\\) to \\(N\\)\n\nSample \\(x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)})\\)\n\nCompute weight \\(w_t^{(i)} \\leftarrow w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})\\)\n\n\nNormalize weights \\(w_t^{(i)} \\leftarrow \\frac{w_t^{(i)}}{\\sum_{j=1}^N w_t^{(j)}}\\)\n\nReturn: Particles \\(\\{x_t^{(i)}\\}\\) and weights \\(w_t^{(i)}\\) for all \\(t\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hidden Markov Chains</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html",
    "href": "chapters/estimation/index.html",
    "title": "Monte Carlo Estimation",
    "section": "",
    "text": "Types of Estimation Problems\nOne of the most fundamental applications of Monte Carlo methods is estimation — using random sampling to approximate unknown quantities. This approach is remarkably versatile, applying equally well to inherently random phenomena and deterministic mathematical problems.\nMonte Carlo estimation addresses two distinct categories of problems:\nStochastic quantities: Values that are naturally random or uncertain\nDeterministic quantities: Fixed mathematical values that are difficult to compute directly",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html",
    "href": "chapters/estimation/integrals.html",
    "title": "4  Estimating Integrals",
    "section": "",
    "text": "5 Mathematical Foundation\nMonte Carlo integration transforms the problem of computing definite integrals into a sampling problem, making it particularly powerful for high-dimensional integration where traditional methods fail.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#confidence-intervals",
    "href": "chapters/estimation/index.html#confidence-intervals",
    "title": "Estimation Techniques",
    "section": "",
    "text": "Relative Confidence Intervals\nIt is common practice in simulation to use and report the relative widths of the confidence interval, defined as \\[\n\\text{Relative Width} = \\frac{\\text{Width of CI}}{\\hat{\\ell}} = 2z_{1-\\alpha/2} \\frac{S}{\\hat{\\ell}\\sqrt{N}}.\n\\]\nThe relative width of the confidence interval is a measure of the precision of the estimate.",
    "crumbs": [
      "Estimation Techniques"
    ]
  },
  {
    "objectID": "chapters/intro.html#what-are-monte-carlo-methods",
    "href": "chapters/intro.html#what-are-monte-carlo-methods",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "3.1 What are Monte Carlo Methods?",
    "text": "3.1 What are Monte Carlo Methods?\nAt their core, Monte Carlo methods rely on repeated random sampling to obtain numerical results for problems that might be difficult or impossible to solve analytically. By generating large numbers of random samples and analyzing their statistical properties, we can approximate solutions with quantifiable uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#key-application-areas",
    "href": "chapters/intro.html#key-application-areas",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.2 Key Application Areas",
    "text": "2.2 Key Application Areas\nMonte Carlo methods excel in three primary problem domains:\n\n\n\n\n\n\nCore Applications\n\n\n\n\nNumerical integration and estimation — Computing integrals, expected values, and other mathematical quantities\nPhysical and mathematical system simulation — Modeling complex systems with inherent randomness\nOptimization — Finding optimal solutions in high-dimensional or complex parameter spaces",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#the-monte-carlo-workflow",
    "href": "chapters/intro.html#the-monte-carlo-workflow",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.3 The Monte Carlo Workflow",
    "text": "2.3 The Monte Carlo Workflow\nA typical Monte Carlo analysis follows this systematic approach:\n\nModel formulation: Develop a mathematical representation of the real-world system, explicitly incorporating relevant random variables\nDistribution specification: Determine the probability distributions governing the random variables, using available data, theoretical knowledge, or expert judgment\nSimulation: Generate representative samples from the specified distributions and execute computational experiments\nValidation and refinement: Compare simulation results with empirical observations to assess model adequacy and guide iterative improvements",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#why-monte-carlo-methods-matter",
    "href": "chapters/intro.html#why-monte-carlo-methods-matter",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.4 Why Monte Carlo Methods Matter",
    "text": "2.4 Why Monte Carlo Methods Matter\nThese methods are particularly valuable when dealing with:\n\nHigh-dimensional problems where traditional numerical methods become computationally prohibitive\nSystems with complex, nonlinear relationships\nProblems involving uncertainty quantification\nSituations where analytical solutions are intractable\n\nThroughout these notes, we’ll explore how Monte Carlo methods provide both theoretical insights and practical solutions across diverse fields, from finance and physics to machine learning and engineering.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html",
    "href": "appendices/probability.html",
    "title": "Appendix A — Review of Probability Theory",
    "section": "",
    "text": "A.1 Random Variables and Probability Theory\nThis appendix reviews core probability and estimation concepts needed for Monte Carlo methods, including probability spaces, random variables, unbiased estimation, and the Law of Large Numbers and Central Limit Theorem.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#confidence-intervals",
    "href": "appendices/probability.html#confidence-intervals",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.3 Confidence Intervals",
    "text": "A.3 Confidence Intervals\n\nA.3.1 Construction\nLet \\(S_N^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(g(X_i) - \\hat{\\theta}_N)^2\\) be the sample variance. By the CLT: \\[\\frac{\\hat{\\theta}_N - \\theta}{S_N/\\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1)\\]\nAn asymptotic \\((1-\\alpha)\\)-level confidence interval is: \\[\\left[ \\hat{\\theta}_N - z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}, \\quad \\hat{\\theta}_N + z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}} \\right]\\]\nwhere \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\) and \\(\\Phi\\) is the standard normal CDF.\n\n\nA.3.2 Width Analysis\nAbsolute width: \\[\\text{Width} = 2z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}\\]\nRelative width: \\[\\text{RW}_N = \\frac{2z_{1-\\alpha/2} S_N}{|\\hat{\\theta}_N|\\sqrt{N}}\\]\n\n\nA.3.3 Convergence Rate\nThe convergence rate of an estimator describes how quickly the estimation error decreases as sample size increases. For the sample mean estimator, the standard error decreases as \\(O(N^{-1/2})\\), meaning:\n\\[\\text{SE}(\\hat{\\theta}_N) = \\frac{\\sigma}{\\sqrt{N}} = O(N^{-1/2})\\]\nThis rate is independent of the dimension of the integration domain, which is the key advantage of Monte Carlo methods over deterministic quadrature rules in high-dimensional problems.\n\n\n\n\n\n\nMonte Carlo Convergence Properties\n\n\n\n\nSquare Root Law: To halve the confidence interval width, need four times as many samples\nDimension Independence: For independent samples, the standard error decreases as \\(O(N^{-1/2})\\), independent of problem dimension. This is the fundamental advantage over deterministic methods.\nMCMC Caveat: When independent sampling is impossible and MCMC is used, high-dimensional problems can suffer from poor mixing and high autocorrelation, effectively reducing the number of independent samples.\nScale Invariance: Relative width provides scale-invariant precision measure",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#the-monte-carlo-estimation-strategy",
    "href": "chapters/estimation/index.html#the-monte-carlo-estimation-strategy",
    "title": "Monte Carlo Estimation",
    "section": "The Monte Carlo Estimation Strategy",
    "text": "The Monte Carlo Estimation Strategy\nThe key insight behind Monte Carlo estimation is transforming any estimation problem into a probabilistic framework, even when the original problem contains no randomness.\n\n\n\n\n\n\nCore Monte Carlo Principle\n\n\n\nTo estimate any quantity θ, we:\n\nConstruct a random variable \\(X\\) such that \\(\\mathbb{E}[X] = \\theta\\)\nGenerate independent samples \\(x_1, x_2, \\ldots, x_n\\) from the distribution of \\(X\\)\n\nEstimate θ using the sample average: \\[\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\n\n\n\nFor more details, see Estimation Theory.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#why-this-works",
    "href": "chapters/estimation/index.html#why-this-works",
    "title": "Monte Carlo Estimation",
    "section": "Why This Works",
    "text": "Why This Works\nThis approach leverages the Law of Large Numbers: as the sample size \\(n\\) increases, our estimate \\(\\hat{\\theta}\\) converges to the true expected value \\(\\theta\\). The beauty lies in converting complex analytical problems into straightforward sampling and averaging procedures.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#whats-next",
    "href": "chapters/estimation/index.html#whats-next",
    "title": "Monte Carlo Estimation",
    "section": "What’s Next",
    "text": "What’s Next\nIn the following sections, we’ll see:\n\nConcrete examples demonstrating this estimation framework\nHow to construct appropriate random variables \\(X\\) for different problems\n\nMethods for assessing and improving estimation accuracy\nAdvanced sampling techniques for enhanced efficiency\n\nThe examples will illustrate how this simple yet powerful principle enables us to tackle problems that would otherwise require sophisticated analytical techniques or be computationally intractable.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#types-of-estimation-problems",
    "href": "chapters/estimation/index.html#types-of-estimation-problems",
    "title": "Monte Carlo Estimation",
    "section": "",
    "text": "Future stock prices\nWeather predictions\n\nSystem reliability measures\nRisk assessments\n\n\n\nThe value of \\(\\pi\\)\nComplex integrals\nSolutions to differential equations\nHigh-dimensional optimization problems",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "appendices/probability.html#random-variables",
    "href": "appendices/probability.html#random-variables",
    "title": "Appendix A — Review of Probability Theory",
    "section": "",
    "text": "\\(\\Omega\\) is the sample space (set of all possible outcomes)\n\\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\) (collection of measurable events)\n\\(P: \\mathcal{F} \\to [0,1]\\) is a probability measure satisfying Kolmogorov’s axioms\n\n\n\n\n\n\n\nThe mean (or expected value) is given by: \\[\\mathbb{E}[X] = \\begin{cases}\n\\sum_{x} x \\cdot P(X = x) & \\text{if } X \\text{ is discrete} \\\\\n\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases}\\]\nThe variance measures the spread around the mean: \\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#estimation-theory",
    "href": "appendices/probability.html#estimation-theory",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.2 Estimation Theory",
    "text": "A.2 Estimation Theory\n\nA.2.1 Problem Setup\nConsider i.i.d. observations \\(X_1, \\ldots, X_N\\) with common distribution \\(F\\). We seek to estimate \\(\\theta = \\mathbb{E}[g(X)]\\) for some measurable function \\(g: \\mathbb{R} \\to \\mathbb{R}\\).\nThe sample mean estimator is: \\[\\hat{\\theta}_N = \\frac{1}{N} \\sum_{i=1}^{N} g(X_i) \\tag{A.1}\\]\n\n\nA.2.2 Properties of the Sample Mean Estimator\n\n\n\n\n\n\nFundamental Properties\n\n\n\nUnbiasedness: \\(\\mathbb{E}[\\hat{\\theta}_N] = \\theta\\)\nVariance: \\(\\text{Var}(\\hat{\\theta}_N) = \\frac{\\sigma^2}{N}\\) where \\(\\sigma^2 = \\text{Var}(g(X))\\)\nStandard Error: \\(\\text{SE}(\\hat{\\theta}_N) = \\sigma/\\sqrt{N}\\)\n\n\n\n\nA.2.3 Asymptotic Theory\n\n\n\n\n\n\nFundamental Limit Theorems\n\n\n\nStrong Law of Large Numbers: If \\(\\mathbb{E}[|g(X)|] &lt; \\infty\\), then \\[\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta \\quad \\text{as } N \\to \\infty\\]\nCentral Limit Theorem: If \\(\\sigma^2 = \\text{Var}(g(X)) &lt; \\infty\\), then \\[\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\nA.2.4 Bias and Mean Squared Error\nFor any estimator \\(\\hat{\\theta}\\) of parameter \\(\\theta\\):\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})\\)\n\nThe sample mean estimator is unbiased with \\(\\text{MSE}(\\hat{\\theta}_N) = \\text{Var}(\\hat{\\theta}_N) = \\sigma^2/N\\).\n\n\nA.2.5 Consistency\nAn estimator sequence \\(\\{\\hat{\\theta}_N\\}\\) is:\n\nConsistent if \\(\\hat{\\theta}_N \\xrightarrow{P} \\theta\\) as \\(N \\to \\infty\\)\nStrongly consistent if \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\) as \\(N \\to \\infty\\)\nAsymptotically normal if \\(\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)\\) for some \\(\\tau^2 &gt; 0\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#random-variables-and-distribution-theory",
    "href": "appendices/probability.html#random-variables-and-distribution-theory",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.4 Random Variables and Distribution Theory",
    "text": "A.4 Random Variables and Distribution Theory\n\nA.4.1 Probability Spaces and Random Variables\n\n\nA.4.2 Distribution Functions\n\n\nA.4.3 Moments and Characteristic Functions\nFor a random variable \\(X\\) with distribution \\(F_X\\):\nMoments (when they exist): - Mean: \\(\\mu = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\, dF_X(x)\\) - \\(k\\)-th moment: \\(\\mathbb{E}[X^k] = \\int_{-\\infty}^{\\infty} x^k \\, dF_X(x)\\) - Variance: \\(\\sigma^2 = \\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}[X^2] - \\mu^2\\)\nCharacteristic function: \\[\\varphi_X(t) = \\mathbb{E}[e^{itX}] = \\int_{-\\infty}^{\\infty} e^{itx} \\, dF_X(x)\\]\nThe characteristic function uniquely determines the distribution and is particularly useful for proving limit theorems.\n\n\nA.4.4 Asymptotic Properties\n\n\n\n\n\n\nFundamental Limit Theorems\n\n\n\nStrong Law of Large Numbers (SLLN): If \\(\\mathbb{E}[|g(X)|] &lt; \\infty\\), then \\[\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta \\quad \\text{as } N \\to \\infty\\]\nCentral Limit Theorem (CLT): If \\(\\sigma^2 = \\text{Var}(g(X)) &lt; \\infty\\), then \\[\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\nA.4.5 Bias and Mean Squared Error\nFor any estimator \\(\\hat{\\theta}\\) of parameter \\(\\theta\\):\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})\\)\n\nThe sample mean estimator (Equation A.1) is unbiased: \\(\\mathbb{E}[\\hat{\\theta}_N] = \\theta\\), with variance \\(\\text{Var}(\\hat{\\theta}_N) = \\sigma^2/N\\).\n\n\nA.4.6 Consistency\nAn estimator sequence \\(\\{\\hat{\\theta}_N\\}\\) is:\n\nConsistent if \\(\\hat{\\theta}_N \\xrightarrow{P} \\theta\\) as \\(N \\to \\infty\\)\nStrongly consistent if \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\) as \\(N \\to \\infty\\)\nAsymptotically normal if \\(\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)\\) for some \\(\\tau^2 &gt; 0\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#estimation-theory-1",
    "href": "appendices/probability.html#estimation-theory-1",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.5 Estimation Theory",
    "text": "A.5 Estimation Theory\n\nA.5.1 Statistical Model and Parameter Estimation\n\n\nA.5.2 Asymptotic Properties\n\n\n\n\n\n\nFundamental Limit Theorems\n\n\n\nStrong Law of Large Numbers (SLLN): If \\(\\mathbb{E}[|g(X)|] &lt; \\infty\\), then \\[\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta \\quad \\text{as } N \\to \\infty\\]\nCentral Limit Theorem (CLT): If \\(\\sigma^2 = \\text{Var}(g(X)) &lt; \\infty\\), then \\[\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\nA.5.3 Bias and Mean Squared Error\nFor any estimator \\(\\hat{\\theta}\\) of parameter \\(\\theta\\):\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})\\)\n\nThe sample mean estimator (Equation A.1) is unbiased: \\(\\mathbb{E}[\\hat{\\theta}_N] = \\theta\\), with variance \\(\\text{Var}(\\hat{\\theta}_N) = \\sigma^2/N\\).\n\n\nA.5.4 Consistency\nAn estimator sequence \\(\\{\\hat{\\theta}_N\\}\\) is:\n\nConsistent if \\(\\hat{\\theta}_N \\xrightarrow{P} \\theta\\) as \\(N \\to \\infty\\)\nStrongly consistent if \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\) as \\(N \\to \\infty\\)\nAsymptotically normal if \\(\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)\\) for some \\(\\tau^2 &gt; 0\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#confidence-intervals-and-hypothesis-testing",
    "href": "appendices/probability.html#confidence-intervals-and-hypothesis-testing",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.4 Confidence Intervals and Hypothesis Testing",
    "text": "A.4 Confidence Intervals and Hypothesis Testing\n\nA.4.1 Asymptotic Confidence Intervals\nBy the CLT, for large \\(N\\): \\[\\frac{\\hat{\\theta}_N - \\theta}{S_N/\\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1)\\]\nwhere \\(S_N^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(g(X_i) - \\hat{\\theta}_N)^2\\) is the sample variance.\nAn asymptotic \\((1-\\alpha)\\)-level confidence interval is: \\[\\left[ \\hat{\\theta}_N - z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}, \\quad \\hat{\\theta}_N + z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}} \\right]\\]\nwhere \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\) and \\(\\Phi\\) is the standard normal CDF.\n\n\nA.4.2 Relative Error Analysis\nDefine the coefficient of variation: \\(\\text{CV} = \\sigma/|\\theta|\\)\nThe relative width of the confidence interval is: \\[\\text{RW}_N = \\frac{2z_{1-\\alpha/2} S_N}{|\\hat{\\theta}_N|\\sqrt{N}}\\]\n\n\n\n\n\n\nMonte Carlo Convergence Rate\n\n\n\nThe relative error decreases as \\(O(N^{-1/2})\\), independent of dimension. This is the fundamental advantage of Monte Carlo methods over deterministic quadrature in high-dimensional problems.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#random-variables-and-probability-theory",
    "href": "appendices/probability.html#random-variables-and-probability-theory",
    "title": "Appendix A — Review of Probability Theory",
    "section": "",
    "text": "A.1.1 Probability Spaces\nA probability space is a triple \\((\\Omega, \\mathcal{F}, P)\\) where:\n\n\\(\\Omega\\) is the sample space (set of all possible outcomes)\n\\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\) (collection of measurable events)\n\\(P: \\mathcal{F} \\to [0,1]\\) is a probability measure satisfying Kolmogorov’s axioms\n\n\n\nA.1.2 Random Variables and Distributions\nA random variable is a measurable function \\(X: \\Omega \\to \\mathbb{R}\\) such that \\(\\{X \\leq x\\} \\in \\mathcal{F}\\) for all \\(x \\in \\mathbb{R}\\).\nThe cumulative distribution function (CDF) of \\(X\\) is: \\[F_X(x) = P(X \\leq x) = P(\\{\\omega \\in \\Omega : X(\\omega) \\leq x\\})\\]\nContinuous random variables have a probability density function (PDF) \\(f_X(x)\\) such that: \\[F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt \\quad \\text{and} \\quad f_X(x) = \\frac{dF_X(x)}{dx}\\]\nDiscrete random variables with support \\(\\{x_1, x_2, \\ldots\\}\\) satisfy: \\[P(X = x_i) = p_i \\quad \\text{where} \\quad \\sum_{i} p_i = 1\\]\n\n\nA.1.3 Moments\nFor a random variable \\(X\\):\nExpected value: \\[\\mathbb{E}[X] = \\begin{cases}\n\\sum_{i} x_i \\cdot P(X = x_i) & \\text{if } X \\text{ is discrete} \\\\\n\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases}\\]\nVariance: \\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#fundamental-limit-theorems-1",
    "href": "appendices/probability.html#fundamental-limit-theorems-1",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.4 Fundamental Limit Theorems",
    "text": "A.4 Fundamental Limit Theorems\nThe theoretical foundation of Monte Carlo methods rests on two cornerstone results from probability theory: the Law of Large Numbers and the Central Limit Theorem.\n\nA.4.1 Law of Large Numbers (LLN)\nThe Law of Large Numbers justifies why sample averages converge to expected values.\n\n\n\n\n\n\nWeak Law of Large Numbers (WLLN)\n\n\n\nLet \\(X_1, X_2, \\ldots\\) be i.i.d. random variables with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Then: \\[\\frac{1}{N}\\sum_{i=1}^{N} X_i \\xrightarrow{P} \\mu \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\n\n\n\n\nStrong Law of Large Numbers (SLLN)\n\n\n\nUnder the same conditions, we have the stronger result: \\[\\frac{1}{N}\\sum_{i=1}^{N} X_i \\xrightarrow{a.s.} \\mu \\quad \\text{as } N \\to \\infty\\]\n\n\nConvergence notation:\n\n\\(\\xrightarrow{P}\\): convergence in probability\n\\(\\xrightarrow{a.s.}\\): almost sure convergence (stronger than convergence in probability)\n\nMonte Carlo implication: For our estimator \\(\\hat{\\theta}_N = \\frac{1}{N}\\sum_{i=1}^{N} g(X_i)\\), the SLLN guarantees that \\(\\hat{\\theta}_N \\to \\theta\\) almost surely, providing the consistency of our estimator.\n\n\nA.4.2 Central Limit Theorem (CLT)\nWhile the LLN tells us that sample averages converge to the true mean, the CLT describes the rate and distribution of this convergence.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nLet \\(X_1, X_2, \\ldots\\) be i.i.d. random variables with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(0 &lt; \\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Then: \\[\\frac{\\sqrt{N}(\\bar{X}_N - \\mu)}{\\sigma} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } N \\to \\infty\\]\nwhere \\(\\bar{X}_N = \\frac{1}{N}\\sum_{i=1}^{N} X_i\\) and \\(\\xrightarrow{d}\\) denotes convergence in distribution.\n\n\nEquivalent formulations: \\[\\sqrt{N}(\\bar{X}_N - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\\] \\[\\bar{X}_N \\xrightarrow{d} \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{N}\\right) \\quad \\text{(approximately, for large } N\\text{)}\\]\n\n\nA.4.3 Monte Carlo Applications\nFor our estimator \\(\\hat{\\theta}_N = \\frac{1}{N}\\sum_{i=1}^{N} g(X_i)\\) where \\(\\theta = \\mathbb{E}[g(X)]\\):\n\nConsistency (from SLLN): \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\)\nAsymptotic normality (from CLT): \\[\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\\] where \\(\\sigma^2 = \\text{Var}(g(X))\\)\nStandard error: \\(\\text{SE}(\\hat{\\theta}_N) = \\sigma/\\sqrt{N}\\)\nConfidence intervals: For large \\(N\\), \\[P\\left(\\theta \\in \\left[\\hat{\\theta}_N \\pm z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}\\right]\\right) \\approx 1-\\alpha\\]\n\n\n\n\n\n\n\nConditions for CLT/LLN\n\n\n\nBoth theorems require:\n\nIndependence: Samples must be independent (or satisfy weaker mixing conditions)\nIdentical distribution: Samples from the same distribution\nFinite moments: Finite mean for LLN, finite variance for CLT\n\nWhen using MCMC, the independence assumption is violated, requiring more sophisticated analysis of the effective sample size.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#sec-estimation-theory",
    "href": "appendices/probability.html#sec-estimation-theory",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.2 Estimation Theory",
    "text": "A.2 Estimation Theory\n\nA.2.1 Problem Setup\nConsider i.i.d. observations \\(X_1, \\ldots, X_N\\) with common distribution \\(F\\). We seek to estimate \\(\\theta = \\mathbb{E}[g(X)]\\) for some measurable function \\(g: \\mathbb{R} \\to \\mathbb{R}\\).\nThe sample mean estimator is: \\[\\hat{\\theta}_N = \\frac{1}{N} \\sum_{i=1}^{N} g(X_i) \\tag{A.1}\\]\n\n\nA.2.2 Properties of the Sample Mean Estimator\n\n\n\n\n\n\nFundamental Properties\n\n\n\nUnbiasedness: \\(\\mathbb{E}[\\hat{\\theta}_N] = \\theta\\)\nVariance: \\(\\text{Var}(\\hat{\\theta}_N) = \\frac{\\sigma^2}{N}\\) where \\(\\sigma^2 = \\text{Var}(g(X))\\)\nStandard Error: \\(\\text{SE}(\\hat{\\theta}_N) = \\sigma/\\sqrt{N}\\)\n\n\n\n\nA.2.3 Asymptotic Theory\n\n\n\n\n\n\nFundamental Limit Theorems\n\n\n\nStrong Law of Large Numbers: If \\(\\mathbb{E}[|g(X)|] &lt; \\infty\\), then \\[\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta \\quad \\text{as } N \\to \\infty\\]\nCentral Limit Theorem: If \\(\\sigma^2 = \\text{Var}(g(X)) &lt; \\infty\\), then \\[\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\nA.2.4 Bias and Mean Squared Error\nFor any estimator \\(\\hat{\\theta}\\) of parameter \\(\\theta\\):\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})\\)\n\nThe sample mean estimator is unbiased with \\(\\text{MSE}(\\hat{\\theta}_N) = \\text{Var}(\\hat{\\theta}_N) = \\sigma^2/N\\).\n\n\nA.2.5 Consistency\nAn estimator sequence \\(\\{\\hat{\\theta}_N\\}\\) is:\n\nConsistent if \\(\\hat{\\theta}_N \\xrightarrow{P} \\theta\\) as \\(N \\to \\infty\\)\nStrongly consistent if \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\) as \\(N \\to \\infty\\)\nAsymptotically normal if \\(\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)\\) for some \\(\\tau^2 &gt; 0\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#monte-carlo-formulation",
    "href": "chapters/estimation/estimating_pi.html#monte-carlo-formulation",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "3.2 Monte Carlo Formulation",
    "text": "3.2 Monte Carlo Formulation\nTo apply our estimation framework, we formulate this as an expectation problem.\nSetup: Let \\((X, Y)\\) be uniformly distributed on \\([-1, 1] \\times [-1, 1]\\). Define the indicator random variable:\n\\[I(X, Y) = \\begin{cases}\n1 & \\text{if } X^2 + Y^2 \\leq 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nKey observation: \\[\\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \\leq 1) = \\frac{\\text{Area of unit circle}}{\\text{Area of square}} = \\frac{\\pi}{4}\\]\nMonte Carlo estimator: Given i.i.d. samples \\((X_1, Y_1), \\ldots, (X_N, Y_N)\\): \\[\\hat{\\pi} = 4 \\cdot \\frac{1}{N}\\sum_{i=1}^{N} I(X_i, Y_i) = \\frac{4}{N} \\sum_{i=1}^{N} I_i\\]\nwhere \\(I_i = I(X_i, Y_i)\\).",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#theoretical-analysis",
    "href": "chapters/estimation/estimating_pi.html#theoretical-analysis",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "3.3 Theoretical Analysis",
    "text": "3.3 Theoretical Analysis\nUnbiasedness: \\(\\mathbb{E}[\\hat{\\pi}] = 4 \\mathbb{E}[I] = 4 \\cdot \\frac{\\pi}{4} = \\pi\\)\nVariance: Since \\(I\\) is Bernoulli with \\(p = \\pi/4\\):\n\n\\(\\text{Var}(I) = p(1-p) = \\frac{\\pi}{4}\\left(1 - \\frac{\\pi}{4}\\right) = \\frac{\\pi(4-\\pi)}{16}\\)\n\\(\\text{Var}(\\hat{\\pi}) = 16 \\cdot \\frac{\\text{Var}(I)}{N} = \\frac{\\pi(4-\\pi)}{N}\\)\n\nStandard error: \\(\\text{SE}(\\hat{\\pi}) = \\sqrt{\\frac{\\pi(4-\\pi)}{N}} \\approx \\frac{1.64}{\\sqrt{N}}\\)\nAsymptotic distribution: By the CLT: \\[\\sqrt{N}(\\hat{\\pi} - \\pi) \\xrightarrow{d} \\mathcal{N}(0, \\pi(4-\\pi))\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#the-method",
    "href": "chapters/estimation/estimating_pi.html#the-method",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "",
    "text": "Circle area: \\(A_{\\text{circle}} = \\pi r^2 = \\pi\\) (for \\(r = 1\\))\nSquare area: \\(A_{\\text{square}} = (2r)^2 = 4\\)\nArea ratio: \\(\\frac{A_{\\text{circle}}}{A_{\\text{square}}} = \\frac{\\pi}{4}\\)",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#problem-setup",
    "href": "chapters/estimation/estimating_pi.html#problem-setup",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "4.1 Problem Setup",
    "text": "4.1 Problem Setup\nConsider a unit circle inscribed in a square with side length 2, both centered at the origin:\n\n\n\n\n\n\nKey Geometric Relationships\n\n\n\n\nCircle: radius \\(r = 1\\), area \\(A_{\\text{circle}} = \\pi r^2 = \\pi\\)\nSquare: side length \\(2r = 2\\), area \\(A_{\\text{square}} = (2r)^2 = 4\\)\nArea ratio: \\(\\frac{A_{\\text{circle}}}{A_{\\text{square}}} = \\frac{\\pi}{4}\\)",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#the-monte-carlo-insight",
    "href": "chapters/estimation/estimating_pi.html#the-monte-carlo-insight",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "4.2 The Monte Carlo Insight",
    "text": "4.2 The Monte Carlo Insight\nKey insight: If we randomly sample points uniformly within the square, the probability that any point falls inside the inscribed circle equals the ratio of their areas: \\(\\pi/4\\).\nThis geometric probability provides our pathway to estimating \\(\\pi\\).\n\n\n\n\n\n\n\n\nFigure 4.1: Estimating π using Monte Carlo method with different sample sizes\n\n\n\n\n\nMonte Carlo π Estimation Results\n==================================================\nSample Size Points Inside π Estimate Absolute Error Standard Error\n        100            80     3.2000         0.0584         0.1600\n     10,000         7,763     3.1052         0.0364         0.0167",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#random-variable-definition",
    "href": "chapters/estimation/estimating_pi.html#random-variable-definition",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "5.1 Random Variable Definition",
    "text": "5.1 Random Variable Definition\nTo apply Monte Carlo estimation, we formulate this as an expectation problem:\nSetup: Let \\((X, Y)\\) be uniformly distributed on \\([-1, 1] \\times [-1, 1]\\).\nDefine the indicator random variable: \\[I(X, Y) = \\mathbf{1}_{\\{X^2 + Y^2 \\leq 1\\}} = \\begin{cases}\n1 & \\text{if } X^2 + Y^2 \\leq 1 \\text{ (inside circle)} \\\\\n0 & \\text{if } X^2 + Y^2 &gt; 1 \\text{ (outside circle)}\n\\end{cases}\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#the-expectation",
    "href": "chapters/estimation/estimating_pi.html#the-expectation",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "5.2 The Expectation",
    "text": "5.2 The Expectation\nThe expected value of our indicator function gives us the desired probability:\n\\[\\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \\leq 1) = \\frac{\\text{Area of unit circle}}{\\text{Area of square}} = \\frac{\\pi}{4}\\]\nTherefore: \\(\\pi = 4\\mathbb{E}[I(X, Y)]\\)",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#monte-carlo-estimator",
    "href": "chapters/estimation/estimating_pi.html#monte-carlo-estimator",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "5.3 Monte Carlo Estimator",
    "text": "5.3 Monte Carlo Estimator\nGiven \\(N\\) independent samples \\((X_1, Y_1), \\ldots, (X_N, Y_N)\\), our estimator is:\n\\[\\hat{\\pi}_N = 4 \\cdot \\frac{1}{N}\\sum_{i=1}^{N} I(X_i, Y_i) = \\frac{4 \\cdot \\text{(number of points inside circle)}}{N}\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#unbiasedness",
    "href": "chapters/estimation/estimating_pi.html#unbiasedness",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "6.1 Unbiasedness",
    "text": "6.1 Unbiasedness\nOur estimator is unbiased: \\[\\mathbb{E}[\\hat{\\pi}_N] = 4\\mathbb{E}\\left[\\frac{1}{N}\\sum_{i=1}^{N} I_i\\right] = 4\\mathbb{E}[I] = 4 \\cdot \\frac{\\pi}{4} = \\pi\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#variance-analysis",
    "href": "chapters/estimation/estimating_pi.html#variance-analysis",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "6.2 Variance Analysis",
    "text": "6.2 Variance Analysis\nSince \\(I \\sim \\text{Bernoulli}(p)\\) with \\(p = \\pi/4\\):\n\nVariance of indicator: \\(\\text{Var}(I) = p(1-p) = \\frac{\\pi}{4}\\left(1 - \\frac{\\pi}{4}\\right) = \\frac{\\pi(4-\\pi)}{16}\\)\nVariance of estimator: \\(\\text{Var}(\\hat{\\pi}_N) = 16 \\cdot \\frac{\\text{Var}(I)}{N} = \\frac{\\pi(4-\\pi)}{N}\\)\nStandard error: \\(\\text{SE}(\\hat{\\pi}_N) = \\sqrt{\\frac{\\pi(4-\\pi)}{N}} \\approx \\frac{1.64}{\\sqrt{N}}\\)\n\n\n\n\n\n\n\nConvergence Rate\n\n\n\nThe standard error decreases as \\(O(1/\\sqrt{N})\\), which is the typical Monte Carlo convergence rate. To gain one decimal place of accuracy, we need approximately 100 times more samples.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#asymptotic-distribution",
    "href": "chapters/estimation/estimating_pi.html#asymptotic-distribution",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "6.3 Asymptotic Distribution",
    "text": "6.3 Asymptotic Distribution\nBy the Central Limit Theorem: \\[\\sqrt{N}(\\hat{\\pi}_N - \\pi) \\xrightarrow{d} \\mathcal{N}(0, \\pi(4-\\pi))\\]\nThis gives us approximate confidence intervals for large \\(N\\): \\[\\hat{\\pi}_N \\pm z_{\\alpha/2} \\sqrt{\\frac{\\pi(4-\\pi)}{N}}\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html",
    "href": "chapters/variance_reduction.html",
    "title": "12  Variance Reduction",
    "section": "",
    "text": "12.0.1 Markov Chain Monte Carlo (MCMC)\nWhen the random variables \\(X_i\\) are not independent, the variance of the sum is given by\n\\[\n\\text{Var}(\\hat{\\ell}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\text{Var}(f(X_i)) + \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1, j \\neq i}^{N} \\text{Cov}(f(X_i), f(X_j)).\n\\]\nThere are \\(n^2\\) terms in the covariance sum, and in general, we cannot guarantee that the covariance is small. So the above confidence interval is not valid. In the case of ergodic Markov chains, the central limit theorem for Markov chains allows us to estimate the CI as \\[\n\\left[ \\hat{\\ell} -  z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}}, \\hat{\\ell} + z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}} \\right],\n\\] where \\(N_{eff}\\) is the effective sample size. We will not discuss the details of effective sample size here.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#importance-sampling",
    "href": "chapters/variance_reduction.html#importance-sampling",
    "title": "12  Variance Reduction",
    "section": "12.1 Importance Sampling",
    "text": "12.1 Importance Sampling\nImportance sampling is a method to reduce the variance of an estimator by changing the distribution from which we sample. The idea is to reduce the number of low probability events that contribute to the variance of the estimator.\nFor example, when estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\), if \\(\\ell\\) is small, then most of the samples will be in the region \\(X \\leq \\gamma\\), which contributes little to the estimate. However, we cannot just sample from the tail of the distribution as this would provide us no information about the rest of the distribution. Importance sampling allows us to sample from tail but then “fix” the estimate by weighting the samples appropriately.\n\nDefinition: Importance Sampling. Let \\(X\\) be a random variable with probability density function (pdf) \\(p(x)\\), and let \\(q(x)\\) be a proposal pdf such that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(p(x)\\). The importance sampling estimator of \\(\\ell = E[f(X)]\\) is given by \\[\n\\hat{\\ell} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) \\frac{p(X_i)}{q(X_i)},\n\\] where \\(X_1, X_2, \\ldots, X_N\\) are i.i.d. samples from the distribution with pdf \\(q(x)\\).\nFor clarity, we’ll denote the estimator in ?eq-crude-estimator as \\(\\hat{\\ell}_{crude}\\) and the importance sampling estimator as \\(\\hat{\\ell}_{IS}\\).\n\nSuppose \\(N = 1\\) so that the estimator is given by \\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}.\n\\] Note that here \\(X \\sim q(x)\\) and NOT \\(p(x)\\). We can check that this estimator is unbiased: \\[\n\\begin{aligned}\nE_q[\\hat{\\ell}_{IS}] &= E_q\\left[f(X) \\frac{p(X)}{q(X)}\\right] \\\\\n&= \\int f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\n&= \\int f(x) p(x) dx \\\\\n&= E[f(X)] \\\\\n&= \\ell.\n\\end{aligned}\n\\]\nHowever, \\[\n\\begin{aligned}\n\\text{Var}(\\hat{\\ell}_{IS})\n&\\neq \\text{Var}(\\hat{\\ell}_{crude}).\n\\end{aligned}\n\\]\nThis allows us to reduce the variance of the estimator by choosing \\(q(x)\\) appropriately.\nSuppose \\(f(X)\\) is a non-negative function. Then if we choose\n\\[\nq(x) \\propto p(x) f(x),\n\\]\nthen the importance sampling estimator for \\(N = 1\\)\n\\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}\n\\]\nis a constant and has zero variance! When \\(H\\) is not non-negative, we can show that\n\\[\nq(x) \\propto p(x) |f(x)|\n\\]\nminimizes the variance of the estimator \\(\\hat{\\ell}_{IS}\\).\nHowever, note that our goal is to estimate \\(\\ell = E[f(X)]\\), which means that we do not know \\(f(x)\\) in advance. So we cannot choose this \\(q(x)\\) in advance. Even if we could, we might not be able to sample from \\(q(x)\\) easily. In practice, we choose \\(q(x)\\) to be a distribution that is easy to sample from and that is “close” to \\(p(x)\\) in some sense.\n\nExample: Importance Sampling for Rare Events. Consider the problem of estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\) for a random variable \\(X\\) with standard normal distribution. We can use importance sampling to estimate this probability by choosing a proposal distribution \\(q(x)\\) that is concentrated in the tail region. One such distribution is the exponential distribution with parameter \\(\\lambda\\), which has pdf \\[\nq(x) = \\lambda e^{-\\lambda (x-2)}, \\quad x \\ge 2.\n\\] The plots below show the running averages of the crude and importance sampling estimators for \\(N = 2000\\) samples. The importance sampling estimator is much more stable and converges to the true value of \\(\\ell\\) much faster than the crude estimator.\n\n\n\n\n\n\n\n\n\n\nVariance of Importance Sampling Estimate: 0.00000\nRelative Error of Importance Sampling Estimate: 0.01247\nVariance of Crude Monte Carlo Estimate: 0.00001\nRelative Error of Crude Monte Carlo Estimate: 0.09231\n\n\n\n12.1.1 Remarks\n\nThe optimal choice of the proposal distribution \\(q(x)\\) is not always easy to find. Even if we can find it, we may not be able to sample from it easily. For importance sampling algorithm, we need to be able to sample from \\(q(x)\\). Often, we use a distribution that is easy to sample from and that is “close” to \\(|f(x)|p(x)\\) in some sense.\nUnlike rejection sampling and MCMC methods, for importance sampling we need to know the normalizing constant of the proposal distribution \\(q(x)\\) in order to compute the weights. This means that we are fairly limited in the choice of \\(q(x)\\). Some common choices are the exponential distribution, the normal distribution, and the uniform distribution, and a mixture of these distributions.\nIn order for the estimator to be well-defined, we need to ensure that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(f(x) p(x)\\). As in the case of rare-event estimation, the support of \\(f(x) p(x)\\) may be very small compared to the support of \\(p(x)\\). We only need \\(q(x)\\) to be positive in this smaller region.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#antithetic-and-control-random-variates",
    "href": "chapters/variance_reduction.html#antithetic-and-control-random-variates",
    "title": "12  Variance Reduction",
    "section": "12.2 Antithetic and Control Random Variates",
    "text": "12.2 Antithetic and Control Random Variates\nIn this section, we will discuss two methods of variance reduction that are based on the idea of using correlated random variables: antithetic variables and control variates. Recall that the variance of a sum of two random variables is given by\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y) + 2\\text{cov}(X, Y).\n\\]\nOftentimes, having correlated random variables in undesirable as it reduces to an increase in variance and a decrease in the effective sample size. However, in some cases, we can use this correlation to our advantage.\n\n12.2.1 Antithetic Variates\nAntithetic variates are pairs of random variables that are negatively correlated. If we can find an estimator that uses sums to two antithetic random variables, we can reduce its variance.\nConsider the example of estimating the integral from ?sec-estimating-integrals. Suppose \\(f(x)\\) is a monotonic function over \\([a, b]\\). Then you’ll show on the homework that if \\(X \\sim U(a, b)\\) then\n\\[\n\\text{cov}(f(X), f(a + b - X)) \\le 0.\n\\]\nWe can see intuitively why this is happening - if \\(f(x)\\) is increasing the \\(f(b - x)\\) is decreasing and vice versa, and hence the two are negatively correlated. In this case, we can reduce the variance of the crude estimator by instead using\n\\[\n\\hat{\\ell}_{anti} = \\dfrac{(b - a)}{N} \\sum \\limits_{i = 1}^{2N} \\left(f(X) + f(a + b - X)\\right)\n\\]\nNote that if \\(X \\sim U(a, b)\\) then so is \\(b - X\\) and so \\(\\hat{\\ell}_{anti}\\) is an unbiased estimator.\n\nTheorem 12.1 \\(\\text{var}(\\hat{\\ell}_{anti}) \\le \\text{var}(\\hat{\\ell}_{crude})\\).\n\n\nExample 12.1 Example: Antithetic Variates. Consider the problem of estimating the integral \\(\\ell = \\int_0^1 (1 + x^2)^{-1} dx\\) using antithetic variates. The plots below show the running averages of the crude and antithetic variate estimators for \\(N = 100\\) samples. The antithetic variate estimator achieves a \\(50x\\) reduction in variance compared to the crude estimator.\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Monte Carlo Estimate: 0.770064, Variance: 1.180981e-04\nAntithetic Variates Estimate: 0.787292, Variance: 1.917273e-06\nVariance Reduction Factor: 61.60x\n\n\n\n\n12.2.2 Control Variates\nControl random variables are examples of random variables that are positively correlated. In this case, the difference\n\\[\n\\text{var}(X - Y) = \\text{var}(X) + \\text{var}(Y) - 2 \\text{cov}(X, Y)\n\\]\nwill have lower variance. Let \\(X \\sim p(x)\\). Suppose we want to estimate \\(\\ell = \\mathbb{E}[f(x)]\\) for some function \\(f(x)\\). We can use a control variate \\(Y = h(X)\\) for some function \\(h(x)\\) such that\n\n\\(\\mathbb{E}[h(X)]\\) is known, say \\(\\mathbb{E}[h(X)] = h_0\\).\n\\(h(x)\\) is strongly positively correlated with \\(f(x)\\), i.e., \\(\\text{cov}(f(X), h(X)) \\gg 0\\).\n\nThen we can use the control variate estimator\n\\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (h(X_i) - h_0)\\right]\n\\]\nwhere \\(\\beta\\) is a constant. It is easy to see that \\(\\hat{\\ell}_\\text{CV}\\) is an unbiased estimator of \\(\\ell\\). The variance of the control variate estimator is given by\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\ell}_\\text{CV})\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta^2 \\text{var}(h(X)) - 2 \\beta \\text{cov}(f(X), h(X))\\right] \\\\\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta \\text{var}(h(X)) \\left[ \\beta - 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\right]\\right]\n\\end{aligned}\n\\]\nBy choosing \\(\\beta &lt; 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\), we can reduce the variance of the control variate estimator. In practice, it is not easy to calculate the covariance between \\(f(X)\\) and \\(h(X)\\), so we\n\nPick a control variate \\(h(X)\\) that is strongly correlated with \\(f(X)\\), and whose expectation is known.\nExperiment with different values of \\(\\beta\\) to find the one that minimizes the variance of the control variate estimator.\n\n\nExample 12.2 In the example below, we estimate the integral of \\(x e^{-x}\\) over the interval \\([0, 1]\\) using control function \\(g(x) = x\\). We know that \\(\\mathbb{E}[g(X)] = \\frac{1}{2}\\) so the estimator is given by \\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (g(X_i) - \\frac{1}{2})\\right]\n\\] where \\(\\beta\\) is a constant and \\(X_i \\sim \\text{Uniform}(0, 1)\\) are i.i.d. We choose \\(\\beta \\approx 0.35\\) which minimizes the variance of the control variate estimator. This results in an 8-fold reduction in variance compared to the naive estimator.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#problem-setup",
    "href": "chapters/estimation/integrals.html#problem-setup",
    "title": "4  Estimating Integrals",
    "section": "5.1 Problem Setup",
    "text": "5.1 Problem Setup\nWe want to estimate the integral: \\[\\ell = \\int_a^b f(x) \\, dx\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#the-monte-carlo-estimator",
    "href": "chapters/estimation/integrals.html#the-monte-carlo-estimator",
    "title": "4  Estimating Integrals",
    "section": "5.2 The Monte Carlo Estimator",
    "text": "5.2 The Monte Carlo Estimator\nKey insight: Rewrite the integral as an expectation. If \\(X \\sim \\text{Uniform}(a, b)\\):\n\\[\\int_a^b f(x) \\, dx = (b-a) \\mathbb{E}[f(X)]\\]\nEstimator: Given \\(N\\) samples \\(X_1, \\ldots, X_N \\sim \\text{Uniform}(a, b)\\):\n\\[\\hat{\\ell}_N = \\frac{b - a}{N} \\sum_{i=1}^{N} f(X_i)\\]\n\n\n\n\n\n\nGeometric Interpretation\n\n\n\nWe’re approximating the area under \\(f(x)\\) by averaging function values at random points and scaling by interval width.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#unbiasedness",
    "href": "chapters/estimation/integrals.html#unbiasedness",
    "title": "4  Estimating Integrals",
    "section": "6.1 Unbiasedness",
    "text": "6.1 Unbiasedness\n\\[\\begin{aligned}\n\\mathbb{E}[\\hat{\\ell}_N] &= \\frac{b - a}{N} \\sum_{i=1}^{N} \\mathbb{E}[f(X_i)] \\\\\n&= \\frac{b - a}{N} \\sum_{i=1}^{N} \\int_a^b f(x) \\frac{1}{b - a} \\, dx \\\\\n&= \\frac{1}{N} \\sum_{i=1}^{N} \\ell = \\ell\n\\end{aligned}\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#variance",
    "href": "chapters/estimation/integrals.html#variance",
    "title": "4  Estimating Integrals",
    "section": "6.2 Variance",
    "text": "6.2 Variance\n\\[\\text{Var}(\\hat{\\ell}_N) = \\frac{(b - a)^2}{N} \\cdot \\sigma^2_f\\]\nwhere \\(\\sigma^2_f = \\text{Var}(f(X))\\) is the variance of \\(f(X)\\) under uniform distribution.\n\n\n\n\n\n\nKey Properties\n\n\n\n\nStandard Error: \\(\\text{SE} = \\frac{(b-a)\\sigma_f}{\\sqrt{N}}\\)\nConvergence Rate: \\(O(1/\\sqrt{N})\\) regardless of dimension\nCLT: \\(\\sqrt{N}(\\hat{\\ell}_N - \\ell) \\xrightarrow{d} \\mathcal{N}(0, (b-a)^2\\sigma^2_f)\\)",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#example-polynomial-integration",
    "href": "chapters/estimation/integrals.html#example-polynomial-integration",
    "title": "4  Estimating Integrals",
    "section": "6.3 Example: Polynomial Integration",
    "text": "6.3 Example: Polynomial Integration\nMonte Carlo integration’s performance can vary significantly depending on the characteristics of the function being integrated. While the method’s convergence rate is theoretically independent of function complexity, practical performance depends heavily on the function’s smoothness and oscillatory behavior.\nFunctions with high-frequency oscillations present particular challenges for Monte Carlo methods because random sampling may miss important features or fail to adequately capture rapid variations. This example compares Monte Carlo integration performance on two trigonometric functions with dramatically different oscillation frequencies, illustrating how function characteristics affect integration accuracy and convergence behavior.\n\n6.3.1 Example: Low vs. High Frequency Functions\nThe following example integrates two functions over the interval \\([0, 2\\pi]\\):\n\nSmooth decay: \\(f_1(x) = e^{-x/3}\\) — a smooth, slowly varying function\nOscillatory: \\(f_2(x) = \\sin(50x)\\) — a rapidly oscillating function with many periods\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Results (1000 samples, 50 runs each):\nFunction     True Value   Mean Est     Std Dev      Error       \n-----------------------------------------------------------------\nSmooth       2.630566     2.631755     0.046838     0.001189    \nOscillatory  0.000000     0.001093     0.136233     0.001093    \n\nVariance ratio (Osc/Smooth): 2.9x higher\n\n\n\n\n6.3.2 Explanation of Results\nThe comparison reveals several important insights about Monte Carlo integration performance:\n\nFunction Visualization (Top Panels): The stark contrast between the smooth \\(\\sin(2x)\\) and rapidly oscillating \\(\\sin(100x)\\) immediately suggests why integration difficulty varies. The high-frequency function has many more sign changes and local extrema that random sampling must capture.\nConvergence Behavior (Bottom Left): Both functions eventually converge to their true values (shown as dashed horizontal lines), but the high-frequency function exhibits much more erratic convergence with larger fluctuations around the true value.\nError Analysis (Bottom Right): The log-log error plot reveals that while both functions follow the expected \\(O(1/\\sqrt{n})\\) convergence rate, the high-frequency function consistently maintains higher absolute errors across all sample sizes.\nStatistical Summary: The numerical results quantify the performance difference:\n\nThe low-frequency function achieves good accuracy with relatively few samples\nThe high-frequency function requires significantly more samples to achieve comparable accuracy\nStandard errors are notably larger for the oscillatory function, indicating higher variance in estimates\n\n\n\n\n6.3.3 Key Takeaways\n\nSmooth functions are well-suited for Monte Carlo integration, converging quickly with relatively few samples\nHighly oscillatory functions present challenges, requiring larger sample sizes and exhibiting higher variance\nFor oscillatory integrals, consider alternative approaches such as:\n\nStratified sampling to ensure adequate coverage of oscillation periods\nImportance sampling with distributions that account for function behavior\nSpecialized quadrature methods designed for oscillatory integrals\nTransformation techniques to reduce oscillations\n\n\nThis example demonstrates why understanding your function’s characteristics is crucial for effective Monte Carlo integration.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#performance-analysis",
    "href": "chapters/estimation/integrals.html#performance-analysis",
    "title": "4  Estimating Integrals",
    "section": "7.2 Performance Analysis",
    "text": "7.2 Performance Analysis\n\n\n\nTable 7.1: Monte Carlo integration performance comparison\n\n\n\n     Function Difficulty      N Mean Est. True Value      Bias Std Error    RMSE\nPolynomial x²       Easy  1,000   0.33260    0.33333 -7.38e-04   0.00695 0.00699\nPolynomial x²       Easy 10,000   0.33328    0.33333 -5.23e-05   0.00295 0.00295\nPolynomial x²       Easy 50,000   0.33343    0.33333  9.36e-05   0.00136 0.00137\nTrigonometric       Easy  1,000   2.00187    2.00000  1.87e-03   0.03425 0.03430\nTrigonometric       Easy 10,000   1.99934    2.00000 -6.63e-04   0.00950 0.00952\nTrigonometric       Easy 50,000   1.99999    2.00000 -1.29e-05   0.00445 0.00445\n     Gaussian     Medium  1,000   1.77181    3.53605 -1.76e+00   0.03760 1.76463\n     Gaussian     Medium 10,000   1.76085    3.53605 -1.78e+00   0.01594 1.77527\n     Gaussian     Medium 50,000   1.76559    3.53605 -1.77e+00   0.00604 1.77047\n  Oscillatory       Hard  1,000   0.09445    0.09232  2.13e-03   0.02449 0.02458\n  Oscillatory       Hard 10,000   0.09099    0.09232 -1.33e-03   0.00646 0.00659\n  Oscillatory       Hard 50,000   0.09144    0.09232 -8.81e-04   0.00313 0.00325",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#multidimensional-integration",
    "href": "chapters/estimation/integrals.html#multidimensional-integration",
    "title": "4  Estimating Integrals",
    "section": "6.4 Multidimensional Integration",
    "text": "6.4 Multidimensional Integration\n\n6.4.1 Introduction\nOne of Monte Carlo integration’s greatest advantages becomes apparent in higher dimensions. While traditional numerical integration methods (like the trapezoidal rule or Simpson’s rule) suffer from the “curse of dimensionality”—requiring exponentially more grid points as dimensions increase—Monte Carlo methods maintain their \\(O(1/\\sqrt{n})\\) convergence rate regardless of dimension.\nIn \\(d\\) dimensions, a grid-based method with \\(m\\) points per dimension requires \\(m^d\\) total evaluations. For even modest accuracy in high dimensions, this becomes computationally prohibitive. Monte Carlo integration, however, uses the same number of random samples regardless of dimension, making it the method of choice for high-dimensional problems in physics, finance, and machine learning.\n\n\n6.4.2 Example: Gaussian-like Function in Multiple Dimensions\nThe following example demonstrates Monte Carlo integration of the function \\(f(\\mathbf{x}) = e^{-\\|\\mathbf{x}\\|^2}\\) over the unit hypercube \\([0,1]^d\\) for dimensions \\(d = 1, 2, 3, 4, 5\\).\n\n\n\n\n\n\n\n\nFigure 6.1: Monte Carlo advantage in higher dimensions\n\n\n\n\n\n\nMultidimensional Integration Results From 5000 samples:\n Dimension  Estimate  Std Error  Rel Error\n         1  0.746787   0.002663   0.003566\n         2  0.558120   0.002912   0.005218\n         3  0.416152   0.002850   0.006847\n         4  0.311272   0.002507   0.008055\n         5  0.232565   0.001850   0.007955\n\n\n\n\n6.4.3 Explanation of Results\nThe example reveals several key insights about multidimensional integration:\n\nDecreasing Integral Values: As dimension increases, the integral value decreases rapidly. This occurs because the function \\(e^{-\\|\\mathbf{x}\\|^2}\\) decays exponentially with distance from the origin, and in higher dimensions, most of the unit hypercube’s volume lies far from the origin.\nStable Relative Error: Despite the changing integral magnitudes, the relative error remains relatively stable across dimensions. This demonstrates Monte Carlo’s dimension-independent convergence behavior—a crucial advantage over grid-based methods.\nComputational Efficiency: Each dimension uses exactly the same number of function evaluations (5,000), showcasing how Monte Carlo avoids the exponential scaling that plagues deterministic methods.\n\nThis behavior makes Monte Carlo integration indispensable for high-dimensional problems in computational physics, Bayesian inference, and financial modeling, where traditional quadrature rules become computationally infeasible.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#practical-considerations",
    "href": "chapters/estimation/integrals.html#practical-considerations",
    "title": "4  Estimating Integrals",
    "section": "7.1 Practical Considerations",
    "text": "7.1 Practical Considerations\n\nSample size: Need large \\(N\\) for high precision\nFunction smoothness: Smoother functions → lower variance → better estimates\nVariance reduction: Techniques like antithetic variables can improve efficiency\nError estimation: Use sample variance to estimate uncertainty",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  }
]