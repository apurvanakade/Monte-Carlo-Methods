[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monte Carlo Methods Lecture Notes",
    "section": "",
    "text": "Preface\n⚠️ Note: These notes are very much under construction and subject to significant changes.\nThese are the lecture notes for Monte Carlo Methods (EN.553.433). The notes are loosely based on course materials by Dr. Jim Spall and the textbook by Rubinstein & Kroese (Rubinstein and Kroese 2017).\nThese notes represent only half of the course content. The remaining material consists of Jupyter notebooks assigned as homework, which are not published online.\nThanks to Kyle Beaty for help with cleaning these notes.\n\n\n\n\nRubinstein, R. Y., and D. P. Kroese. 2017. Simulation and the Monte Carlo Method. 3rd ed. USA: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "",
    "text": "2.1 What are Monte Carlo Methods?\nMonte Carlo methods are a powerful class of computational algorithms that harness the power of random sampling to solve complex numerical problems. Named after the famous Monte Carlo Casino in Monaco, these methods transform deterministic problems into probabilistic ones, allowing us to approximate solutions through statistical simulation.\nAt their core, Monte Carlo methods rely on repeated random sampling to obtain numerical results for problems that might be difficult or impossible to solve analytically. By generating large numbers of random samples and analyzing their statistical properties, we can approximate solutions with quantifiable uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#key-application-areas",
    "href": "chapters/intro.html#key-application-areas",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.2 Key Application Areas",
    "text": "2.2 Key Application Areas\nMonte Carlo methods excel in three primary problem domains:\n\n\n\n\n\n\nCore Applications\n\n\n\n\nNumerical integration and estimation — Computing integrals, expected values, and other mathematical quantities\nPhysical and mathematical system simulation — Modeling complex systems with inherent randomness\nOptimization — Finding optimal solutions in high-dimensional or complex parameter spaces",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#the-monte-carlo-workflow",
    "href": "chapters/intro.html#the-monte-carlo-workflow",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.3 The Monte Carlo Workflow",
    "text": "2.3 The Monte Carlo Workflow\nA typical Monte Carlo analysis follows this systematic approach:\n\nModel formulation: Develop a mathematical representation of the real-world system, explicitly incorporating relevant random variables\nDistribution specification: Determine the probability distributions governing the random variables, using available data, theoretical knowledge, or expert judgment\nSimulation: Generate representative samples from the specified distributions and execute computational experiments\nValidation and refinement: Compare simulation results with empirical observations to assess model adequacy and guide iterative improvements",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#why-monte-carlo-methods-matter",
    "href": "chapters/intro.html#why-monte-carlo-methods-matter",
    "title": "2  Introduction to Monte Carlo Methods",
    "section": "2.4 Why Monte Carlo Methods Matter",
    "text": "2.4 Why Monte Carlo Methods Matter\nThese methods are particularly valuable when dealing with:\n\nHigh-dimensional problems where traditional numerical methods become computationally prohibitive\nSystems with complex, nonlinear relationships\nProblems involving uncertainty quantification\nSituations where analytical solutions are intractable\n\nThroughout these notes, we’ll explore how Monte Carlo methods provide both theoretical insights and practical solutions across diverse fields, from finance and physics to machine learning and engineering.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html",
    "href": "chapters/estimation/index.html",
    "title": "Monte Carlo Estimation",
    "section": "",
    "text": "Types of Estimation Problems\nOne of the most fundamental applications of Monte Carlo methods is estimation — using random sampling to approximate unknown quantities. This approach is remarkably versatile, applying equally well to inherently random phenomena and deterministic mathematical problems.\nMonte Carlo estimation addresses two distinct categories of problems:\nStochastic quantities: Values that are naturally random or uncertain\nDeterministic quantities: Fixed mathematical values that are difficult to compute directly",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#types-of-estimation-problems",
    "href": "chapters/estimation/index.html#types-of-estimation-problems",
    "title": "Monte Carlo Estimation",
    "section": "",
    "text": "Future stock prices\nWeather predictions\n\nSystem reliability measures\nRisk assessments\n\n\n\nThe value of \\(\\pi\\)\nComplex integrals\nSolutions to differential equations\nHigh-dimensional optimization problems",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#the-monte-carlo-estimation-strategy",
    "href": "chapters/estimation/index.html#the-monte-carlo-estimation-strategy",
    "title": "Monte Carlo Estimation",
    "section": "The Monte Carlo Estimation Strategy",
    "text": "The Monte Carlo Estimation Strategy\nThe key insight behind Monte Carlo estimation is transforming any estimation problem into a probabilistic framework, even when the original problem contains no randomness.\n\n\n\n\n\n\nCore Monte Carlo Principle\n\n\n\nTo estimate any quantity θ, we:\n\nConstruct a random variable \\(X\\) such that \\(\\mathbb{E}[X] = \\theta\\)\nGenerate independent samples \\(x_1, x_2, \\ldots, x_n\\) from the distribution of \\(X\\)\n\nEstimate θ using the sample average: \\[\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\n\n\n\nIn fact, we do not even need \\(\\mathbb{E}[X] = \\theta\\). It suffices that \\(\\lim_{n \\to \\infty} \\hat{\\theta} = \\theta\\) with high probability. For details, see Statistical Estimation and the Sample Mean.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#why-this-works",
    "href": "chapters/estimation/index.html#why-this-works",
    "title": "Monte Carlo Estimation",
    "section": "Why This Works",
    "text": "Why This Works\nThis approach leverages the Law of Large Numbers: as the sample size \\(n\\) increases, our estimate \\(\\hat{\\theta}\\) converges to the true expected value \\(\\theta\\). The beauty lies in converting complex analytical problems into straightforward sampling and averaging procedures.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/index.html#whats-next",
    "href": "chapters/estimation/index.html#whats-next",
    "title": "Monte Carlo Estimation",
    "section": "What’s Next",
    "text": "What’s Next\nIn the following sections, we’ll see:\n\nConcrete examples demonstrating this estimation framework\nHow to construct appropriate random variables \\(X\\) for different problems\n\nMethods for assessing and improving estimation accuracy\nAdvanced sampling techniques for enhanced efficiency\n\nThe examples will illustrate how this simple yet powerful principle enables us to tackle problems that would otherwise require sophisticated analytical techniques or be computationally intractable.",
    "crumbs": [
      "Monte Carlo Estimation"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html",
    "href": "chapters/estimation/estimating_pi.html",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "",
    "text": "3.1 Statistical Properties\nWe begin with a classic Monte Carlo application: estimating the value of \\(\\pi\\) using geometric probability. This example illustrates the core principles of Monte Carlo simulation while providing an intuitive geometric interpretation.\nConsider a unit circle inscribed in a square with side length 2, both centered at the origin. The circle has radius \\(r = 1\\) and area \\(A_{\\text{circle}} = \\pi\\), while the square has area \\(A_{\\text{square}} = 4\\). The ratio of these areas is \\(\\pi/4\\).\nIf we randomly sample points uniformly within the square, the probability that any point falls inside the inscribed circle equals this area ratio. This geometric probability provides our pathway to estimating \\(\\pi\\).\nTo formulate this as a Monte Carlo problem, let \\((X, Y)\\) be uniformly distributed on \\([-1, 1] \\times [-1, 1]\\) and define the indicator random variable: \\[I(X, Y) = \\mathbf{1}_{\\{X^2 + Y^2 \\leq 1\\}} = \\begin{cases}\n1 & \\text{if } X^2 + Y^2 \\leq 1 \\text{ (inside circle)} \\\\\n0 & \\text{if } X^2 + Y^2 &gt; 1 \\text{ (outside circle)}\n\\end{cases}\\]\nThe expected value of this indicator function gives us the desired probability: \\[\\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \\leq 1) = \\frac{\\text{Area of unit circle}}{\\text{Area of square}} = \\frac{\\pi}{4}\\]\nTherefore \\(\\pi = 4\\mathbb{E}[I(X, Y)]\\), and given \\(N\\) independent samples \\((X_1, Y_1), \\ldots, (X_N, Y_N)\\), our Monte Carlo estimator is: \\[\\hat{\\pi}_N = 4 \\cdot \\frac{1}{N}\\sum_{i=1}^{N} I(X_i, Y_i) = \\frac{4 \\cdot \\text{(number of points inside circle)}}{N}\\]\nThis estimator possesses several important statistical properties. First, it is unbiased: \\[\\mathbb{E}[\\hat{\\pi}_N] = 4\\mathbb{E}\\left[\\frac{1}{N}\\sum_{i=1}^{N} I_i\\right] = 4\\mathbb{E}[I] = 4 \\cdot \\frac{\\pi}{4} = \\pi\\]\nSince \\(I \\sim \\text{Bernoulli}(\\pi/4)\\), we can compute the variance. The indicator has variance \\(\\text{Var}(I) = \\frac{\\pi}{4}(1 - \\frac{\\pi}{4}) = \\frac{\\pi(4-\\pi)}{16}\\), which gives our estimator variance: \\[\\text{Var}(\\hat{\\pi}_N) = 16 \\cdot \\frac{\\text{Var}(I)}{N} = \\frac{\\pi(4-\\pi)}{N}\\]\nThe standard error is therefore \\(\\text{SE}(\\hat{\\pi}_N) = \\sqrt{\\frac{\\pi(4-\\pi)}{N}} \\approx \\frac{1.64}{\\sqrt{N}}\\).\nBy the Central Limit Theorem, for large \\(N\\) we have the asymptotic distribution: \\[\\sqrt{N}(\\hat{\\pi}_N - \\pi) \\xrightarrow{d} \\mathcal{N}(0, \\pi(4-\\pi))\\]\nThis provides approximate confidence intervals: \\[\\hat{\\pi}_N \\pm z_{\\alpha/2} \\sqrt{\\frac{\\pi(4-\\pi)}{N}}\\]",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/estimating_pi.html#statistical-properties",
    "href": "chapters/estimation/estimating_pi.html#statistical-properties",
    "title": "3  Estimating \\(\\pi\\)",
    "section": "",
    "text": "Convergence Rate\n\n\n\nThe standard error decreases as \\(O(1/\\sqrt{N})\\), which is the typical Monte Carlo convergence rate. To gain one decimal place of accuracy, we need approximately 100 times more samples.\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Principle\n\n\n\nThis example demonstrates the fundamental Monte Carlo approach: reformulate a deterministic problem (computing \\(\\pi\\)) as the expectation of a random variable, then estimate that expectation using sample averages.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating $\\pi$</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html",
    "href": "chapters/estimation/integrals.html",
    "title": "4  Estimating Integrals",
    "section": "",
    "text": "4.1 Statistical Properties\nMonte Carlo integration transforms the problem of computing definite integrals into a sampling problem, making it particularly powerful for high-dimensional integration where traditional methods fail.\nConsider the problem of estimating the integral \\(\\ell = \\int_a^b f(x) \\, dx\\). The key insight is to rewrite this integral as an expectation. If \\(X \\sim \\text{Uniform}(a, b)\\), then: \\[\\int_a^b f(x) \\, dx = (b-a) \\mathbb{E}[f(X)]\\]\nThis reformulation allows us to estimate the integral using sample averages. Given \\(N\\) independent samples \\(X_1, \\ldots, X_N \\sim \\text{Uniform}(a, b)\\), our Monte Carlo estimator is: \\[\\hat{\\ell}_N = \\frac{b - a}{N} \\sum_{i=1}^{N} f(X_i)\\]\nThe Monte Carlo integration estimator possesses several important statistical properties. First, it is unbiased: \\[\\begin{aligned}\n\\mathbb{E}[\\hat{\\ell}_N] &= \\frac{b - a}{N} \\sum_{i=1}^{N} \\mathbb{E}[f(X_i)] = \\frac{b - a}{N} \\sum_{i=1}^{N} \\int_a^b f(x) \\frac{1}{b - a} \\, dx = \\ell\n\\end{aligned}\\]\nThe variance of our estimator is: \\[\\text{Var}(\\hat{\\ell}_N) = \\frac{(b - a)^2}{N} \\cdot \\sigma^2_f\\] where \\(\\sigma^2_f = \\text{Var}(f(X))\\) is the variance of \\(f(X)\\) under the uniform distribution.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#statistical-properties",
    "href": "chapters/estimation/integrals.html#statistical-properties",
    "title": "4  Estimating Integrals",
    "section": "",
    "text": "Key Properties\n\n\n\n\nStandard Error: \\(\\text{SE} = \\frac{(b-a)\\sigma_f}{\\sqrt{N}}\\)\nConvergence Rate: \\(O(1/\\sqrt{N})\\) regardless of dimension\nCLT: \\(\\sqrt{N}(\\hat{\\ell}_N - \\ell) \\xrightarrow{d} \\mathcal{N}(0, (b-a)^2\\sigma^2_f)\\)",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#function-characteristics-and-performance",
    "href": "chapters/estimation/integrals.html#function-characteristics-and-performance",
    "title": "4  Estimating Integrals",
    "section": "4.2 Function Characteristics and Performance",
    "text": "4.2 Function Characteristics and Performance\nMonte Carlo integration’s performance depends heavily on the function’s characteristics. While the convergence rate is theoretically independent of function complexity, practical performance varies significantly with function smoothness and oscillatory behavior.\nFunctions with high-frequency oscillations present particular challenges for Monte Carlo methods because random sampling may miss important features or fail to adequately capture rapid variations. This example compares Monte Carlo integration performance on two trigonometric functions with dramatically different oscillation frequencies, illustrating how function characteristics affect integration accuracy and convergence behavior.\n\n4.2.1 Example: Low vs. High Frequency Functions\nThe following example integrates two functions over the interval \\([0, \\pi]\\):\n\nSmooth decay: \\(f_1(x) = \\sin(x)\\) — a smooth, slowly varying function\nOscillatory: \\(f_2(x) = 25\\sin(25x)\\) — a rapidly oscillating function with many periods\n\n\n\n\n\n\n\n\n\n\n      Function  True Value  Mean Est   Error  Std Dev  Std Ratio\n0       Smooth         2.0    2.0008  0.0008   0.0086          -\n1  Oscillatory         2.0    1.9919  0.0081   0.6073  71.006457\n\n\nThe contrast between the smooth \\(\\sin(x)\\) and rapidly oscillating \\(25\\sin(25x)\\) demonstrates why integration difficulty varies. The high-frequency function has many more sign changes and local extrema that random sampling must capture.\nThe log-log error plot reveals that while both functions follow the expected \\(O(1/\\sqrt{n})\\) convergence rate, the high-frequency function consistently maintains higher absolute errors across all sample sizes. The variance ratio indicates that the oscillatory function’s variance is around 70 times higher than the smooth function’s variance, illustrating how oscillations increase uncertainty in Monte Carlo estimates.\n\n\n4.2.2 Remarks\n\nSmooth functions are well-suited for Monte Carlo integration, converging quickly with relatively few samples.\nHighly oscillatory functions present challenges, requiring larger sample sizes and exhibiting higher variance.\nFor oscillatory integrals, consider alternative approaches such as:\n\nStratified sampling to ensure adequate coverage of oscillation periods\nImportance sampling with distributions that account for function behavior\nTransformation techniques to reduce oscillations.\n\n\nThis example demonstrates why understanding your function’s characteristics is crucial for effective Monte Carlo integration.\n\n\n4.2.3 Variance Bounds Using Derivatives\nFor smooth functions, we can bound the variance using derivative information.\n\n\n\n\n\n\nTheorem (Variance Bound via Derivatives)\n\n\n\nIf \\(g\\) is differentiable on \\([a,b]\\) with derivative \\(g'\\), then: \\[\\text{Var}(g(X)) \\leq (b-a)^2 \\cdot \\max_{x \\in [a,b]} |g'(x)|^2\\]\n\n\n\nProof. Let \\(M = \\max_{x \\in [a,b]} |g'(x)|\\) and \\(\\mu = \\mathbb{E}[g(X)]\\).\nSince \\(g\\) is continuous on the compact interval \\([a,b]\\), it attains its minimum and maximum values. Let \\(g_{\\min} = \\min_{x \\in [a,b]} g(x)\\) and \\(g_{\\max} = \\max_{x \\in [a,b]} g(x)\\).\nBy the mean value theorem, for any two points in \\([a,b]\\): \\[g_{\\max} - g_{\\min} \\leq M(b - a)\\]\nSince \\(\\mu = \\mathbb{E}[g(X)]\\) lies between \\(g_{\\min}\\) and \\(g_{\\max}\\), we have for any \\(x \\in [a,b]\\): \\[|g(x) - \\mu| \\leq g_{\\max} - g_{\\min} \\leq M(b - a)\\]\nSquaring both sides and taking expectations: \\[\\text{Var}(g(X)) = \\mathbb{E}[(g(X) - \\mu)^2] \\leq [M(b - a)]^2\\]\nThis gives us the desired bound.\n\n\nNote that this is a very weak bound and not useful in practice. There are no universal techniques that give us good bounds on \\(\\text{Var}(g(X))\\). In practice, we rely on the heuristic that oscillatory functions are high variance.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#multidimensional-integration",
    "href": "chapters/estimation/integrals.html#multidimensional-integration",
    "title": "4  Estimating Integrals",
    "section": "4.3 Multidimensional Integration",
    "text": "4.3 Multidimensional Integration\nOne of Monte Carlo integration’s greatest advantages becomes apparent in higher dimensions. While traditional numerical integration methods (like the trapezoidal rule or Simpson’s rule) suffer from the “curse of dimensionality”—requiring exponentially more grid points as dimensions increase—Monte Carlo methods maintain their \\(O(1/\\sqrt{n})\\) convergence rate regardless of dimension.\nIn \\(d\\) dimensions, a grid-based method with \\(m\\) points per dimension requires \\(m^d\\) total evaluations. For even modest accuracy in high dimensions, this becomes computationally prohibitive. Monte Carlo integration, however, uses the same number of random samples regardless of dimension, making it the method of choice for high-dimensional problems in physics, finance, and machine learning.\n\n4.3.1 Example: Gaussian-like Function in Multiple Dimensions\nThe following example demonstrates Monte Carlo integration of the function \\(f(\\mathbf{x}) = e^{-\\|\\mathbf{x}\\|^2}\\) over the unit hypercube \\([0,1]^d\\) for dimensions \\(d = 1, 2, 3, 4, 5\\).\n\n\n\n\n\n\n\n\nFigure 4.1: Monte Carlo advantage in higher dimensions\n\n\n\n\n\nThe example reveals several important properties of multidimensional integration:\n\nDimension-Independent Convergence: The relative error plots demonstrate Monte Carlo’s key advantage - convergence behavior remains consistent across all dimensions, avoiding the curse of dimensionality that plagues grid-based methods.\nTheoretical Validation: The standard deviation follows the expected \\(1/\\sqrt{n}\\) scaling (shown by the dashed reference line) regardless of dimension, confirming Monte Carlo’s theoretical foundation.\nConstant Computational Cost: Unlike deterministic methods requiring \\(n^d\\) evaluations, Monte Carlo uses the same number of function evaluations per sample regardless of dimension.\n\nThis dimensional robustness makes Monte Carlo indispensable for high-dimensional applications in physics, statistics, and finance where traditional methods become computationally intractable.\n\n\n\n\n\n\nWhen to Use Monte Carlo Integration\n\n\n\nAdvantages:\n\nHigh dimensions: Performance doesn’t degrade with dimension\nComplex domains: Easy to handle irregular integration regions\nRough functions: Works with discontinuous or highly oscillatory functions\n\nDisadvantages:\n\nSlow convergence: \\(O(1/\\sqrt{N})\\) rate\nRandom error: Introduces stochastic uncertainty\nLow dimensions: Often outperformed by deterministic methods\n\nPractical Considerations:\n\nSample size: Need large \\(N\\) for high precision\nFunction smoothness: Smoother functions → lower variance → better estimates\nVariance reduction: Techniques like antithetic variables can improve efficiency\nError estimation: Use sample variance to estimate uncertainty",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/estimation/integrals.html#conclusion",
    "href": "chapters/estimation/integrals.html#conclusion",
    "title": "4  Estimating Integrals",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nMonte Carlo integration transforms integration into a sampling problem, making it invaluable for high-dimensional problems where traditional quadrature fails. While it converges slowly, its dimension-independent performance and ability to handle complex functions make it essential for modern computational statistics and machine learning applications.",
    "crumbs": [
      "Monte Carlo Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimating Integrals</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/index.html",
    "href": "chapters/sampling/index.html",
    "title": "Sampling Techniques",
    "section": "",
    "text": "In this chapter, we will discuss how to sample from a probability distribution. Sampling from a probability distribution is a fundamental problem in statistics and machine learning. It is used in various applications like Monte Carlo methods, Bayesian inference, and reinforcement learning.\nTo understand what it means to sample from a probability distribution, let’s consider a simple example. Let \\(X\\) be a discrete random variable that takes values \\(1, 2, \\ldots, n\\) with probabilities \\(p_1, p_2, \\ldots, p_n\\). To sample from this distribution, we want to generate a random variable \\(X\\) such that \\(\\mathbb{P}(X = i) = p_i\\) for all \\(i = 1, 2, \\ldots, n\\) i.e. we want to select a random integer \\(i\\) with probability \\(p_i\\). If we generate enough samples \\(x_1, x_2, \\ldots, x_N\\) from this distribution, then the fraction of samples that are equal to \\(i\\) will be approximately equal to \\(p_i\\) for large \\(N\\).\nMore formally, let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and let \\(X: \\Omega \\to \\mathcal{X}\\) be a random variable with distribution \\(P_X\\). Sampling from the distribution of \\(X\\) means generating independent realizations \\(x_1, x_2, \\ldots, x_n\\) such that each \\(x_i\\) has the same distribution as \\(X\\). By the strong law of large numbers, we have\n\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{j=1}^{n} f(x_j) = \\mathbb{E}[f(X)]\\]\nalmost surely for any measurable function \\(f\\) such that \\(\\mathbb{E}[|f(X)|] &lt; \\infty\\).\nThe fundamental challenge in sampling is transforming uniform random numbers, which are readily available from pseudorandom number generators, into samples from arbitrary probability distributions. Most computational environments provide uniform random number generators that produce sequences \\(U_1, U_2, \\ldots\\) where each \\(U_i\\) is approximately uniformly distributed on \\([0,1]\\) and the sequence has good statistical properties.\nIn the following chapters, we will examine several sampling techniques in detail, providing both theoretical foundations and practical algorithms for implementation.",
    "crumbs": [
      "Sampling Techniques"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html",
    "href": "chapters/sampling/rng.html",
    "title": "5  Random Number Generators",
    "section": "",
    "text": "5.1 Pseudo-random number generators\nIn the problem of estimating the value of \\(\\pi\\) using a Monte Carlo method, we encountered three fundamental principles that underpin all Monte Carlo simulations:\nThese three principles form the foundation of Monte Carlo methodology and will resurface throughout our exploration of more sophisticated simulation techniques. We begin our deeper investigation with the first principle: the generation and properties of random numbers.\nA random number generator is a function that produces a sequence of numbers that meet certain statistical requirements for randomness. True random number generators are based on physical processes that are fundamentally random, such as radioactive decay or thermal noise. Such systems are useful in cryptography and other applications where true randomness is important for security.\nBelow is an example of a true random number generator based on lava lamps called the “wall of entropy”. The lava lamps are used to generate random bits, which are then combined to generate random keys for encryption.\nThere are less exotic ways to generate random numbers. Computer chips have a hardware random number generator that uses thermal noise to generate random bits.\nHowever, true random number generators are slow and expensive. These are not useful for simulations as they are not reproducible. Instead, we use pseudo-random number generators (PRNGs) to generate random numbers for simulations.\nWe test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. A good PRNG should produce numbers that are indistinguishable from true random numbers. It is said to fool the statistical tests of randomness.\nPRNGs need to satisfy several statistical requirements to be useful in simulations. The most important requirements are:\nFor example:",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html#pseudo-random-number-generators",
    "href": "chapters/sampling/rng.html#pseudo-random-number-generators",
    "title": "5  Random Number Generators",
    "section": "",
    "text": "Reproducibility in Simulations\n\n\n\nA pseudo-random number generator (PRNG) is a random number generator that produces a sequence of numbers that are not truly random, but are generated by a deterministic algorithm. The sequence of numbers produced by a PRNG is completely determined by the seed: if you know the seed, you can predict the entire sequence of numbers.\nThe reason for using PRNGs in simulations is to be able to reproduce the results. If you run a simulation with a given seed, you should get the same results every time. This is important for testing, debugging, and for sharing results with others.\n\n\n\n\n\nUniformity: The numbers generated should be uniformly distributed between 0 and 1.\nIndependence: The numbers generated should be independent of each other. Knowing one number should not give you any information about the next number.\nSpeed: The PRNG should be fast. Generating random numbers is a common operation in simulations, so the PRNG should be as fast as possible.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese are all difficult requirements to satisfy simultaneously. In practice, most PRNGs are imperfect. You have to choose a PRNG that is appropriate for your application. You have to decide which statistical tests of randomness are most important for your application, and choose a PRNG that satisfies those tests.\n\n\n\n\nLinear congruential generators are simple and fast, but they have some statistical problems, as you’ll see in the exercises. These are good enough if you only need a few random numbers. These were the first PRNGs to be widely used and have stayed popular for a long time because of their simplicity.\nThe Mersenne Twister is a widely-used PRNG that is fast and has good statistical properties. It is the default PRNG in many programming languages, including Python. However, it is not suitable for cryptographic applications.\nCryptographically secure PRNGs are designed to be secure against cryptographic attacks. They are slower than other PRNGs, but they are necessary for applications where security is important.\n\n\n5.1.1 Cycle length\nIn practice, PRNGs generate a random integer between 0 and \\(M\\) for some large number \\(M\\), and then divide by \\(M\\) to get a random number between 0 and 1. The period or cycle length of a PRNG is the number of random numbers it can generate before it starts repeating itself. A good PRNG should have a long cycle length.\n\n\n\n\n\n\nCycle Length ≠ Quality\n\n\n\nRelying solely on the cycle length to determine the quality of a PRNG is a mistake. A PRNG can have a long cycle length and still have poor statistical properties. For example, a PRNG that generates the sequence\n\\[1, 2, 3, 4, 5, \\ldots, M-1, M, 1, 2, 3, 4, 5,\\]\nhas a cycle length of \\(M\\), but it is a terrible PRNG!\n\n\n\n\n5.1.2 Linear Congruential Generator\nA linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized integers using a simple recurrence relation. The generator is defined by the recurrence relation:\n\\[\\begin{equation}\nX_{n+1} = (aX_n + c) \\mod m\n\\end{equation}\\]\nwhere: - \\(X_n\\) is the sequence of pseudo-randomized numbers - \\(a\\) is the multiplier - \\(c\\) is the increment - \\(m\\) is the modulus\nThe above equation generates a sequence of integers between 0 and \\(m-1\\). To generate a sequence of random numbers between 0 and 1, we divide the sequence by \\(m\\):\n\\[\\begin{equation}\n  r_n = \\frac{X_n}{m}\n\\end{equation}\\]\nLCGs are simple to implement and are computationally efficient. However, as you’ll see in the homework, they have some statistical problems. The modulus \\(m\\) is the largest integer that the generator can produce. The modulus \\(m\\) is usually a power of 2, which makes the modulo operation fast.\n\n\n\n\n\n\nHull-Dobell Theorem\n\n\n\nThe Hull-Dobell Theorem states that an LCG will have a full period for all seed values if and only if:\n\n\\(c\\) and \\(m\\) are relatively prime,\n\\(a - 1\\) is divisible by all prime factors of \\(m\\),\n\\(a - 1\\) is a multiple of 4 if \\(m\\) is a multiple of 4.\n\nIn the special case when \\(m\\) is a power of 2, the Hull-Dobell Theorem simplifies to:\n\n\\(c\\) is odd,\n\\(a\\) is congruent to 1 modulo 4.\n\nIt is in fact enough to take \\(c = 1\\). Thus when the modulus is a power of 2, a full period LCG will be of the form:\n\\[\\begin{equation}\nX_{n+1} = (aX_n + 1) \\mod 2^b,\n\\end{equation}\\]\nwith \\(a \\equiv 1 \\mod 4\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/rng.html#statistical-test-of-randomness",
    "href": "chapters/sampling/rng.html#statistical-test-of-randomness",
    "title": "5  Random Number Generators",
    "section": "5.2 Statistical test of randomness",
    "text": "5.2 Statistical test of randomness\nAs mentioned earlier, we test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. The more tests a PRNG passes, the better it is. There exist many “test suites” that are used to evaluate the quality of PRNGs such as the Diehard tests, the TestU01 suite, and the NIST Statistical Test Suite.\nWe’ll look at the following three simple statistical tests of randomness from the NIST Statistical Test Suite:\n\nChi-square test: The chi-square test checks whether the observed frequency of the sequence is consistent with the expected frequency. If the sequence is truly random, then the observed frequency should be consistent with the expected frequency.\nMono-bit test: The mono-bit test checks whether the number of 0s and 1s in the sequence is approximately equal. If the sequence is truly random, then the number of 0s and 1s should be roughly equal.\nRuns test: The runs test checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. If the sequence is truly random, then the number of runs of 0s and 1s should be consistent with a random sequence.\n\n\n\n\n\n\n\nHypothesis Testing Framework\n\n\n\nFor each of these tests, we’ll set up the null hypothesis and the alternative hypothesis as:\n\n\\(H_0\\): The sequence is random.\n\\(H_1\\): The sequence is not random.\n\nWe’ll use the p-value to determine whether to reject the null hypothesis. If the p-value is less than the significance level \\(\\alpha\\), we reject the null hypothesis. If the p-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\n\n\n5.2.1 Chi-square test\nThe chi-square test is a one-sided statistical test that measures how well a sample of data matches a theoretical distribution. The chi-square test is used to test whether the observed data is consistent with the expected data. The test statistic is given by:\n\\[\\begin{equation}\n\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\\end{equation}\\]\nwhere: - \\(O_i\\) is the observed frequency of the \\(i\\)-th bin - \\(E_i\\) is the expected frequency of the \\(i\\)-th bin - \\(k\\) is the number of bins\nIn our case of testing the randomness of a sequence of random numbers, we divide the interval \\([0, 1]\\) into \\(k\\) bins and count the number of random numbers that fall into each bin. The expected frequency of each bin is \\(n/k\\), where \\(n\\) is the total number of random numbers.\nThe chi-square test is used to test the null hypothesis that the observed data is consistent with the expected data. If the chi-square test statistic is large, then the null hypothesis is rejected.\nThe critical value of the chi-square test statistic depends on the number of degrees of freedom. The degrees of freedom is given by \\(k-1\\). In Python, you can use the scipy.stats.chi2.ppf function to perform the chi-square test.\n\n\n\n\n\n\nChoosing the Number of Bins\n\n\n\nThe chi-square test is sensitive to the number of bins \\(k\\). If \\(k\\) is too small, then the test may not be sensitive enough to detect deviations from the expected distribution. If \\(k\\) is too large, then the test may be too sensitive and may detect deviations that are not significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Mono-bit test\nThe mono-bit test is a two-sided statistical test that checks whether the number of 0s and 1s in the sequence is approximately equal. In a uniform binary sequence, roughly half the bits are 0s and half the bits are 1s.\nLet \\(X_i\\) be the \\(i\\)-th bit in the sequence. Then \\(X_i\\) is a Bernoulli random variable with probability \\(p = 0.5\\). Because the random variables \\(X_i\\) are independent and identically distributed, by the central limit theorem, their average\n\\[\\begin{equation}\n  \\bar{X} = \\dfrac{X_1 + X_2 + \\cdots + X_k}{k}\n\\end{equation}\\]\napproaches a normal distribution with mean \\(0.5\\) and variance \\(1/(4k)\\) as \\(k\\) approaches infinity. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by\n\\[\\begin{equation}\n  Z = \\dfrac{\\bar{X} - 0.5}{\\sqrt{1/(4k)}}\n\\end{equation}\\]\n\n\n\n\n\n\nTwo-Sided Test\n\n\n\nNote that because this is a two-sided test, we reject the null hypothesis if \\(Z &gt; z_{\\alpha/2}\\) or \\(Z &lt; -z_{\\alpha/2}\\) where \\(z_{\\alpha/2}\\) is the critical value of the z-test statistic at the significance level \\(\\alpha/2\\).\nIn Python, you can use the scipy.stats.norm.ppf function to compute the critical values for the z-test.\n\n\n\n\nTo find the critical value, we find $x$ such that $P(-x &lt; X &lt; x) = 0.95$ for a standard normal distribution. In the figure below, this is the area between the green dashed lines.\nIf your z-score is outside of the critical values, you can reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Runs test\nThe Wald-Wolfowitz runs test is a two-sided statistical test that checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. In a random sequence, the number of runs of 0s and 1s should be consistent with a random sequence.\n\n\n\n\n\n\nExample\n\n\n\nIn the sequence 0011100110, there are 5 runs: 00, 111, 00, 11, 0. The runs test checks whether the number of runs of 0s and 1s is consistent with a random sequence.\n\n\nConsider a binary sequence of length \\(k\\). Define a random variable\n\\[\\begin{equation}\n  Z_i = \\begin{cases}\n  1 & \\text{if the $(i+1)^{\\text{st}}$ bit is different from the $i^{\\text{th}}$ bit} \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\\]\nOne can check that the number of runs in the sequence is given by the random variable\n\\[\\begin{equation}\n  R = 1 + \\sum_{i=1}^{k-1} Z_i\n\\end{equation}\\]\nNote that the random variables \\(Z_i\\) are independent. Hence,\n\\[\\begin{align}\n  \\text{E}[R] &= 1 + \\sum_{i=1}^{k-1} \\text{E}[Z_i] \\\\\n  \\text{Var}[R] &= \\sum_{i=1}^{k-1} \\text{Var}[Z_i]\n\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe calculation of this expectation and variance is left as an exercise.\n\n\nWe assume that for large \\(k\\) the number of runs is approximately normally distributed. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by\n\\[\\begin{equation}\n  Z = \\dfrac{R - \\text{E}[R]}{\\sqrt{\\text{Var}[R]}}\n\\end{equation}\\]\nWe reject the null hypothesis if \\(Z &gt; z_{\\alpha/2}\\) or \\(Z &lt; -z_{\\alpha/2}\\) where \\(z_{\\alpha/2}\\) is the critical value of the z-test statistic at the significance level \\(\\alpha/2\\). Note that here we use \\(\\alpha/2\\) instead of \\(\\alpha\\) because this is a two-sided test.\n\n\n5.2.4 Kolmogorov-Smirnov test\nThe Kolmogorov-Smirnov (KS) test is a non-parametric statistical test that checks whether a sample follows a specified probability distribution by comparing the empirical distribution function with the theoretical cumulative distribution function. For RNG testing, we typically test whether the generated numbers follow a uniform distribution on [0,1].\nGiven a sample \\(U_1, U_2, \\ldots, U_n\\), the empirical distribution function (EDF) is defined as:\n\\[\\begin{equation}\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}_{U_i \\leq x}\n\\end{equation}\\]\nwhere \\(\\mathbf{1}_{U_i \\leq x}\\) is the indicator function that equals 1 if \\(U_i \\leq x\\) and 0 otherwise. The EDF represents the proportion of sample values that are less than or equal to \\(x\\), creating a step function that jumps by \\(1/n\\) at each observed data point.\n\n\n\n\n\n\nExample: Constructing the Empirical CDF\n\n\n\nConsider the sequence 0.23, 0.67, 0.12, 0.89, 0.45. First, we order the values: 0.12, 0.23, 0.45, 0.67, 0.89.\nThe empirical CDF is a step function that jumps by 0.2 at each point:\n\n\\(F_5(x) = 0.2\\) for \\(0.12 \\leq x &lt; 0.23\\)\n\\(F_5(x) = 0.4\\) for \\(0.23 \\leq x &lt; 0.45\\)\n\n\\(F_5(x) = 0.6\\) for \\(0.45 \\leq x &lt; 0.67\\)\nAnd so on…\n\nFor a uniform distribution on [0,1], the theoretical CDF is \\(F(x) = x\\).\n\n\nThe KS test statistic measures the maximum absolute difference between the empirical and theoretical distribution functions:\n\\[\\begin{equation}\nD_n = \\sup_{x} |F_n(x) - F(x)|\n\\end{equation}\\]\nwhere \\(F(x)\\) is the theoretical CDF under the null hypothesis. For computational purposes, we don’t need to check every possible value of x x. Since the empirical distribution function is a step function that only changes at the observed data points, it suffices to check the maximum difference at each of the sample values. Let \\(U_{(1)} \\leq U_{(2)} \\leq \\cdots \\leq U_{(n)}\\) be the order statistics. It is enough to compare the values:\n\\[\\begin{equation}\n\\frac{i}{n} - F(U_{(i)}), \\text{ and } F(U_{(i)}) - \\frac{i-1}{n}\n\\end{equation}\\]\nThe first term \\(\\frac{i}{n} - F(U_{(i)})\\) measures the “positive deviation” (empirical CDF above theoretical), while the second term \\(F(U_{(i)}) - \\frac{i-1}{n}\\) measures the “negative deviation” (empirical CDF below theoretical).\nUnder the null hypothesis that the sample follows the specified continuous distribution \\(F(x)\\), the test statistic \\(D_n\\) follows the Kolmogorov distribution asymptotically. This remarkable result shows that as \\(n \\to \\infty\\), the distribution of \\(\\sqrt{n}D_n\\) converges to a distribution that is independent of the specific form of \\(F(x)\\), making the KS test truly distribution-free.\n\n\n\n\n\n\nTheorem: Kolmogorov’s Result\n\n\n\nUnder \\(H_0\\), as \\(n \\to \\infty\\), the distribution of \\(\\sqrt{n}D_n\\) converges to the Kolmogorov distribution with CDF: \\[K(z) = 1 - 2\\sum_{j=1}^{\\infty} (-1)^{j-1} e^{-2j^2z^2}\\]\n\n\n\n\n\n\n\n\n\n\n\nFor hypothesis testing, we reject the null hypothesis \\(H_0\\) (that the sample follows the specified distribution) if \\(D_n &gt; D_{\\alpha}\\) where \\(D_{\\alpha}\\) is the critical value at significance level \\(\\alpha\\). For large \\(n\\), the critical value can be approximated as:\n\\[\\begin{equation}\nD_{\\alpha} \\approx \\sqrt{-\\frac{1}{2n} \\ln\\left(\\frac{\\alpha}{2}\\right)}\n\\end{equation}\\]\nCommon approximations include \\(D_{0.05} \\approx \\frac{1.36}{\\sqrt{n}}\\) for 5% significance level and \\(D_{0.01} \\approx \\frac{1.63}{\\sqrt{n}}\\) for 1% significance level.\nThe KS test assumes the theoretical distribution is completely specified (no parameters estimated from data) and works best for detecting differences in location and scale. For small samples (\\(n &lt; 30\\)), exact critical values should be used instead of asymptotic approximations.\n\n\n5.2.5 Spectral test\nThe previous tests fail to detect some of the problems with generating vectors using LCGs. One test for detecting these problems is the spectral test. The spectral test uses the Fast Fourier Transform (FFT) to analyze the spectral properties of the sequence. This test is beyond the scope of this class but you can see some examples in the homework.\n\n\n\n\n\n\nTip\n\n\n\nThis would be a good topic for a project!\n\n\n\n\n5.2.6 Final remarks\nNote that by definition, a PRNG is not “random” in an absolute sense. The PRNG used in Python, the Mersenne Twister, is sufficiently random for most applications involving simulations. However, it is not suitable for cryptographic applications as it is possible to predict the entire sequence of numbers if you know a limited set of numbers.\n\n\n\n\n\n\nCryptographic Security\n\n\n\nFor cryptographic applications, you should use a cryptographically secure PRNG whose future numbers cannot be predicted easily from past numbers.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Number Generators</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/discrete.html",
    "href": "chapters/sampling/discrete.html",
    "title": "6  Discrete Distributions",
    "section": "",
    "text": "6.1 Discrete Case\nOnce we have a reliable source of random numbers from a uniform distribution \\(U(0,1)\\), we can use these to generate samples from virtually any probability distribution. This process, known as random variate generation, is fundamental to Monte Carlo simulations since most applications require sampling from specific distributions rather than just uniform random numbers.\nThe key insight is that we can transform uniform random numbers into samples from any target distribution using various mathematical techniques. We’ll explore several methods, starting with discrete distributions and then moving to continuous ones.\nFor discrete random variables, we can partition the unit interval \\([0,1]\\) according to the probabilities of each outcome and use this to transform uniform random numbers into samples from our target distribution.\nLet’s consider a simple example where \\(X\\) is a discrete random variable that takes values \\(1, 2, 3\\) with probabilities \\(0.2, 0.3, 0.5\\) respectively. To sample from this distribution, we can use the following algorithm:",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/discrete.html#discrete-case",
    "href": "chapters/sampling/discrete.html#discrete-case",
    "title": "6  Discrete Distributions",
    "section": "",
    "text": "Generate a random number \\(u\\) from the uniform distribution \\(U(0, 1)\\).\nIf \\(u \\leq 0.2\\), set \\(X = 1\\).\nIf \\(0.2 &lt; u \\leq 0.5\\), set \\(X = 2\\).\nIf \\(0.5 &lt; u \\leq 1\\), set \\(X = 3\\).\nReturn \\(X\\).\n\n\n\n\n\n\n\nGeneral Algorithm for Discrete Distributions\n\n\n\nFor a discrete random variable \\(X\\) that takes values \\(x_1, x_2, \\ldots, x_n\\) with probabilities \\(p_1, p_2, \\ldots, p_n\\):\n\nCompute the cumulative probabilities: \\(F_1 = p_1\\), \\(F_2 = p_1 + p_2\\), …, \\(F_n = p_1 + p_2 + \\cdots + p_n = 1\\)\nGenerate a random number \\(u\\) from \\(U(0, 1)\\)\nFind the smallest index \\(i\\) such that \\(u \\leq F_i\\)\nReturn \\(X = x_i\\)\n\nThis algorithm can be implemented efficiently using binary search when \\(n\\) is large, reducing the time complexity from \\(O(n)\\) to \\(O(\\log n)\\) per sample. This approach is an example of the inverse transform method that we’ll study in detail next.\n\n\n\n6.1.1 Binomial Distribution\nLet \\(X\\) be a random variable with binomial distribution with parameters \\(n\\) and \\(p\\). The probability mass function of \\(X\\) is given by\n\\[\\begin{align*}\n\\mathrm{Binomial}(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\ldots, n.\n\\end{align*}\\]\nWhile we could treat the binomial distribution as a general discrete distribution and use the inverse transform method above, there’s a more natural approach that exploits the underlying structure of the binomial distribution.\n\n\n\n\n\n\nExploiting Distribution Structure\n\n\n\nSince a binomial random variable represents the sum of \\(n\\) independent Bernoulli trials, we can generate it by actually performing these trials computationally.\n\n\nIf \\(Y_1, Y_2, \\ldots, Y_n\\) are independent random variables with Bernoulli distribution with parameter \\(p\\), then the random variable \\(X = Y_1 + Y_2 + \\ldots + Y_n\\) has binomial distribution with parameters \\(n\\) and \\(p\\). This gives us a simple algorithm to sample from the binomial distribution:\n\n\n\n\n\n\nAlgorithm for Binomial Distribution\n\n\n\n\nGenerate \\(n\\) random numbers \\(u_1, u_2, \\ldots, u_n\\) from the uniform distribution \\(U(0, 1)\\).\nSet \\(Y_i = 1\\) if \\(u_i \\leq p\\) and \\(Y_i = 0\\) otherwise for \\(i = 1, 2, \\ldots, n\\).\nCompute \\(X = Y_1 + Y_2 + \\ldots + Y_n\\).\nReturn \\(X\\).\n\n\n\nThis method is particularly intuitive and works well when \\(n\\) is not too large. For large \\(n\\), more efficient algorithms exist that avoid generating \\(n\\) uniform random numbers for each binomial sample.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html",
    "href": "chapters/sampling/inverse_transform.html",
    "title": "7  Inverse Transform Sampling",
    "section": "",
    "text": "7.1 Exponential Distribution\nFor continuous random variables, we can use a powerful and elegant method called inverse transform sampling that provides a systematic way to generate samples from any distribution whose cumulative distribution function (CDF) we can compute and invert.\nConsider a continuous random variable \\(X\\) with probability density function \\(f(x)\\). Let \\(U\\) be a random variable with uniform distribution \\(U(0, 1)\\).\nLet \\(X\\) be a random variable with exponential distribution with rate parameter \\(\\lambda &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0.\n\\end{align*}\\]\nThe cumulative distribution function of \\(X\\) is given by\n\\[\\begin{align*}\nF(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0.\n\\end{align*}\\]\nTo find the inverse of the cumulative distribution function, we solve \\(u = F(x) = 1 - e^{-\\lambda x}\\) for \\(x\\):\n\\[\\begin{align*}\nu &= 1 - e^{-\\lambda x} \\\\\n\\Rightarrow e^{-\\lambda x} &= 1 - u \\\\\n\\Rightarrow -\\lambda x &= \\log(1 - u) \\\\\n\\Rightarrow x &= -\\frac{1}{\\lambda} \\log(1 - u)\n\\end{align*}\\]\nTherefore, the inverse of the cumulative distribution function is given by\n\\[\\begin{align*}\nF^{-1}(u) = -\\frac{1}{\\lambda} \\log(1 - u).\n\\end{align*}\\]",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#exponential-distribution",
    "href": "chapters/sampling/inverse_transform.html#exponential-distribution",
    "title": "7  Inverse Transform Sampling",
    "section": "",
    "text": "Sampling Algorithm\n\n\n\nTo generate an exponential random variable with rate \\(\\lambda\\):\n\nGenerate \\(u \\sim U(0,1)\\).\nReturn \\(x = -\\frac{1}{\\lambda} \\log(1 - u)\\).\n\nNote that since \\(1-u\\) is also uniformly distributed on \\((0,1)\\) when \\(u \\sim U(0,1)\\), we can simplify this to \\(x = -\\frac{1}{\\lambda} \\log(u)\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#weibull-distribution",
    "href": "chapters/sampling/inverse_transform.html#weibull-distribution",
    "title": "7  Inverse Transform Sampling",
    "section": "7.2 Weibull Distribution",
    "text": "7.2 Weibull Distribution\nThe Weibull distribution is a generalization of the exponential distribution. Let \\(X\\) be a random variable with Weibull distribution with shape parameter \\(k &gt; 0\\) and scale parameter \\(\\lambda &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} e^{-(x/\\lambda)^k}, \\quad x \\geq 0.\n\\end{align*}\\]\nThe cumulative distribution function is:\n\\[\\begin{align*}\nF(x) = 1 - e^{-(x/\\lambda)^k}\n\\end{align*}\\]\nSolving for the inverse: \\[\\begin{align*}\nu &= 1 - e^{-(x/\\lambda)^k} \\\\\n\\Rightarrow e^{-(x/\\lambda)^k} &= 1 - u \\\\\n\\Rightarrow -(x/\\lambda)^k &= \\log(1 - u) \\\\\n\\Rightarrow x &= \\lambda[-\\log(1 - u)]^{1/k}\n\\end{align*}\\]\n\n\n\n\n\n\nSampling Algorithm\n\n\n\nTo generate a Weibull random variable:\n\nGenerate \\(u \\sim U(0,1)\\).\nReturn \\(x = \\lambda[-\\log(1 - u)]^{1/k}\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#triangular-distribution",
    "href": "chapters/sampling/inverse_transform.html#triangular-distribution",
    "title": "7  Inverse Transform Sampling",
    "section": "7.3 Triangular Distribution",
    "text": "7.3 Triangular Distribution\nLet \\(X\\) be a random variable with triangular distribution supported over the interval \\([a,b]\\) with mode at \\(c \\in [a,b]\\). The triangular distribution has a piecewise linear probability density function that increases linearly from \\(a\\) to \\(c\\) and then decreases linearly from \\(c\\) to \\(b\\).\n\n\n\n\n\n\nTip\n\n\n\nThe triangular distribution provides a good example of inverse transform sampling with a piecewise CDF. The inverse function must be computed separately for the two pieces of the distribution, making this a more complex but instructive application of the method.\n\n\nThe inverse transform sampling method can be used to sample from the triangular distribution, though the derivation involves handling the piecewise nature of both the CDF and its inverse.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#normal-distribution",
    "href": "chapters/sampling/inverse_transform.html#normal-distribution",
    "title": "7  Inverse Transform Sampling",
    "section": "7.4 Normal Distribution",
    "text": "7.4 Normal Distribution\nThe standard normal distribution with mean \\(0\\) and variance \\(1\\) has the probability density function\n\\[\\begin{align*}\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}.\n\\end{align*}\\]\nIt is not possible to sample from the normal distribution using the inverse transform sampling method because the cumulative distribution function of the normal distribution does not have a closed-form inverse. However, there are other methods to sample from the normal distribution. One such method is the Box-Muller transform.\nThe Box-Muller transform is based on the following idea. Consider a 2D random variable \\((X, Y)\\) with standard normal distribution. The joint probability density function of \\((X, Y)\\) is given by\n\\[\\begin{align*}\nf(x, y) = \\frac{1}{2\\pi} e^{-(x^2 + y^2)/2}, \\quad -\\infty &lt; x, y &lt; \\infty.\n\\end{align*}\\]\nLet \\(R = \\sqrt{X^2 + Y^2}\\) and \\(\\Theta = \\arctan(Y/X)\\) be the polar coordinates of \\((X, Y)\\). Then notice that \\(\\Theta\\) is uniformly distributed in \\([0, 2\\pi]\\). We can calculate the CDF of \\(R\\) as follows:\n\\[\\begin{align*}\n\\mathbb{P}(R \\leq r) &= \\mathbb{P}(X^2 + Y^2 \\leq r^2) \\\\\n&= \\int_{x^2 + y^2 \\leq r^2} f(x, y) \\, dx \\, dy \\\\\n&= \\int_{x^2 + y^2 \\leq r^2} \\frac{1}{2\\pi} e^{-(x^2 + y^2)/2} \\, dx \\, dy \\\\\n&= \\int_{0}^{2\\pi} \\int_{0}^{r} \\frac{1}{2\\pi} e^{-\\rho^2/2} \\rho \\, d\\rho \\, d\\theta \\\\\n&= 1 - e^{-r^2/2}.\n\\end{align*}\\]\nWe can invert this to get the inverse CDF of \\(R\\):\n\\[\\begin{align*}\nF_R^{-1}(u) = \\sqrt{-2 \\log(1 - u)}.\n\\end{align*}\\]\nWe can summarize the above discussion in the following theorem.\n\nTheorem 7.2 (Box-Muller Transform): Let \\(U_1, U_2\\) be independent random variables with uniform distribution \\(U(0, 1)\\). Let \\(R = \\sqrt{-2 \\log U_1}\\) and \\(\\Theta = 2\\pi U_2\\). Then, the random variables \\(X = R \\cos(\\Theta)\\) and \\(Y = R \\sin(\\Theta)\\) are independent and have standard normal distribution.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we are using \\(U_1\\) instead of \\(1-U_1\\) in the formula for \\(R\\). This is because \\(1-U_1\\) is also uniformly distributed in \\([0, 1]\\).\n\n\nThis gives us the following algorithm to sample from the normal distribution:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nGenerate two random numbers \\(u_1, u_2\\) from the uniform distribution \\(U(0, 1)\\).\nCompute \\(R = \\sqrt{-2 \\log u_1}\\) and \\(\\Theta = 2\\pi u_2\\).\nCompute \\(X = R \\cos(\\Theta)\\) and \\(Y = R \\sin(\\Theta)\\).\nReturn \\(X\\) (or \\(Y\\)).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that if \\(X\\) is a standard normal random variable, then \\(Z = \\sigma X + \\mu\\) is a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus, to sample from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we can use the above algorithm to sample from the standard normal distribution and then transform the result using \\(Z = \\sigma X + \\mu\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/inverse_transform.html#poisson-distribution",
    "href": "chapters/sampling/inverse_transform.html#poisson-distribution",
    "title": "7  Inverse Transform Sampling",
    "section": "7.5 Poisson Distribution",
    "text": "7.5 Poisson Distribution\nThe Poisson distribution with parameter \\(\\lambda &gt; 0\\) is a discrete distribution that models the number of events occurring in a fixed interval of time or space, where \\(\\lambda\\) is the average rate of events. In unit time \\(T\\), the expected number of events is \\(\\lambda T\\). The probability mass function of the Poisson distribution is given by\n\\[\\begin{align*}\n\\mathrm{Pois}(n) = \\frac{e^{-\\lambda} \\lambda^n}{n!}, \\quad n \\in \\mathbb{N}.\n\\end{align*}\\]\nThe PMF of the Poisson distribution measures the probability of observing \\(n\\) events in time \\(T\\).\n\n7.5.1 Relation between Poisson and Binomial Distribution\nThe Poisson distribution can be approximated by the binomial distribution when the number of trials \\(n\\) is large and the probability of success \\(p\\) is small. Let \\(X\\) be a random variable with binomial distribution with parameters \\(n\\) and \\(p\\). As \\(n \\to \\infty\\) and \\(p \\to 0\\) such that \\(\\lambda = np\\) remains constant, the PMF of \\(X\\) converges to the PMF of the Poisson distribution with parameter \\(\\lambda\\). This gives us a simple algorithm to sample from the Poisson distribution:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSet \\(X = 0\\).\nChoose \\(n\\) to be a large integer (something like \\(n &gt; 10\\lambda\\)).\nSet \\(p = \\lambda/n\\).\nGenerate \\(X\\) according to the binomial distribution with parameters \\(n\\) and \\(p\\).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a fast method to sample from the binomial approximation to the Poisson distribution and is good when \\(\\lambda\\) is small. For large \\(\\lambda\\), the method described below is more efficient as the number of trials \\(n\\) required for the binomial distribution to approximate the Poisson distribution becomes very large.\n\n\n\n\n7.5.2 Relation between Poisson and Exponential Distribution\nWe exploit the relation between the Poisson and exponential distributions to sample from the Poisson distribution. When events occur at a constant rate \\(\\lambda\\), the time between events follows an exponential distribution with rate parameter \\(\\lambda\\). More precisely,\n\nTheorem 7.3 (Inter-arrival Times):\nLet \\(X_1, X_2, \\ldots\\) be independent random variables with exponential distribution with rate parameter \\(\\lambda\\). Define\n\\[\\begin{align*}\nN = \\max \\left\\{ n : X_1 + X_2 + \\dots + X_n \\le 1 \\right\\}.\n\\end{align*}\\]\nThen, \\(N\\) has Poisson distribution with parameter \\(\\lambda\\).\n\n\nProof. The key insight is that \\(N\\) counts the number of events (exponential inter-arrival times) that occur within a unit time interval. Let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) be the time of the \\(n\\)-th event. Then:\n\\[\\begin{align*}\n\\mathbb{P}(N = n) &= \\mathbb{P}(S_n \\leq 1 &lt; S_{n+1}) \\\\\n&= \\mathbb{P}(S_n \\leq 1) - \\mathbb{P}(S_{n+1} \\leq 1)\n\\end{align*}\\]\nSince the sum of \\(n\\) independent exponential random variables with rate \\(\\lambda\\) follows a gamma distribution with shape parameter \\(n\\) and rate parameter \\(\\lambda\\), we have \\(S_n \\sim \\text{Gamma}(n, \\lambda)\\) with PDF:\n\\[\\begin{align*}\nf_{S_n}(t) = \\frac{\\lambda^n t^{n-1} e^{-\\lambda t}}{(n-1)!}\n\\end{align*}\\]\nTherefore: \\[\\begin{align*}\n\\mathbb{P}(S_n \\leq 1) = \\int_0^1 \\frac{\\lambda^n t^{n-1} e^{-\\lambda t}}{(n-1)!} dt\n\\end{align*}\\]\nUsing integration by parts repeatedly (or recognizing this as the CDF of the gamma distribution), we get:\n\\[\\begin{align*}\n\\mathbb{P}(S_n \\leq 1) = 1 - e^{-\\lambda} \\sum_{k=0}^{n-1} \\frac{\\lambda^k}{k!}\n\\end{align*}\\]\nThus: \\[\\begin{align*}\n\\mathbb{P}(N = n) &= \\left(1 - e^{-\\lambda} \\sum_{k=0}^{n-1} \\frac{\\lambda^k}{k!}\\right) - \\left(1 - e^{-\\lambda} \\sum_{k=0}^{n} \\frac{\\lambda^k}{k!}\\right) \\\\\n&= e^{-\\lambda} \\frac{\\lambda^n}{n!}\n\\end{align*}\\]\nThis is exactly the PMF of the Poisson distribution with parameter \\(\\lambda\\). \\(\\blacksquare\\)\n\nThe Poisson-Exponential connection gives us a simple algorithm to sample from the Poisson distribution:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSet \\(S = 0\\) and \\(N = 0\\).\nWhile True:\n\nGenerate a random number \\(x \\sim \\mathrm{Exp}(\\lambda)\\).\nSet \\(S = S + x\\).\nIf \\(S &gt; 1\\), return \\(N\\).\nElse, set \\(N = N + 1\\).\n\n\n\n\n\n\n7.5.3 Knuth’s Algorithm\nIn the above algorithm, we rely on Theorem 7.3 to generate \\(X_i \\sim \\mathrm{Exp}(\\lambda)\\), we use inverse transform sampling. However, this is inefficient as taking logarithms repeatedly can lead to floating point errors. It is possible to avoid this by using the following insight.\nConsider the following set of inequalities:\n\\[\n\\begin{align*}\n    X_1 + X_2 + \\dots + X_n &&lt; 1 \\\\\n    \\Leftrightarrow -\\dfrac{1}{\\lambda} \\ln(U_1) + \\dfrac{1}{\\lambda} \\ln(U_2) + \\dots + \\dfrac{1}{\\lambda} \\ln(U_n) &&lt; -1 \\\\\n    \\Leftrightarrow \\ln(U_1 U_2 \\dots U_n) &&gt; \\lambda \\\\\n    \\Leftrightarrow U_1 U_2 \\dots U_n &&gt; e^\\lambda\n\\end{align*}\n\\]\nThis is the motivation behind Knuth’s algorithm:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSet \\(S = 1\\) and \\(N = 0\\).\nWhile True:\n\nGenerate a random number \\(u \\sim F(0, 1)\\).\nSet \\(S = S * u\\).\nIf \\(S &gt; e^{\\lambda}\\), return \\(N\\).\nElse, set \\(N = N + 1\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inverse Transform Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/other_distributions.html",
    "href": "chapters/sampling/other_distributions.html",
    "title": "8  Other Distributions",
    "section": "",
    "text": "8.1 Beta Distribution\nThe Beta distribution is a continuous distribution defined on the interval \\([0, 1]\\). Let \\(X\\) be a random variable with Beta distribution with parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = cx^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1,\n\\end{align*}\\]\nwhere \\(c= \\frac{(\\alpha + \\beta - 1)!}{(\\alpha-1)!(\\beta-1)!}\\) is the normalizing constant. (Note that when \\(\\alpha\\) and \\(\\beta\\) are not integers, we use the \\(\\Gamma\\) function as a generalization of the factorial function.)\nThis is a simple function supported over the interval \\([0, 1]\\). The Beta distribution is used as a prior distribution in Bayesian statistics. When \\(\\alpha\\) and \\(\\beta\\) are non-integers, the CDF of the Beta distribution does not have a closed-form expression, making inverse transform sampling impractical. Instead, we can use the Beta-Order Statistics connection to sample from the Beta distribution.\nThis theorem gives us a simple algorithm to sample from the Beta distribution:",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Other Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/other_distributions.html#beta-distribution",
    "href": "chapters/sampling/other_distributions.html#beta-distribution",
    "title": "8  Other Distributions",
    "section": "",
    "text": "Definition 8.1 (Order Statistics): Let \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables. The \\(k\\)-th order statistic, denoted \\(X_{(k)}\\), is the \\(k\\)-th smallest value among \\(X_1, X_2, \\ldots, X_n\\).\n\n\nTheorem 8.1 (Beta-Order Statistics): Let \\(U_1, U_2, \\ldots, U_n\\) be independent random variables with uniform distribution \\(U(0, 1)\\). Then the random variable \\(X = U_{(k)}\\) has Beta distribution with parameters \\(\\alpha = k\\) and \\(\\beta = n - k + 1\\).\n\n\nProof. We’ll work out a partial proof of the theorem. Let \\(X = U_{(k)}\\). The cumulative distribution function of \\(X\\) is given by\n\\[\\begin{align*}\nF(x)\n&= \\mathbb{P}(U_{(k)} \\leq x) \\\\\n&= \\mathbb{P}(\\text{at least } k \\text{ variables among } U_1, U_2, \\ldots, U_n \\text{ are less than } x)  \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} x^i (1-x)^{n-i}.\n\\end{align*}\\]\nWe differentiate both sides to get the probability density function of \\(X\\):\n\\[\\begin{align*}\nf(x) &= \\frac{d}{dx} F(x) \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} \\frac{d}{dx}x^i (1-x)^{n-i} \\\\\n&= \\sum_{i=k}^{n} \\binom{n}{i} i x^{i-1} (1-x)^{n-i} - \\binom{n}{i} (n-i) x^i (1-x)^{n-i-1}.\n\\end{align*}\\]\nThe rest of the proof involves checking that the higher terms in the alternating sum cancel out and only the first term with \\(i = k\\) remains. \\(\\blacksquare\\)\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nGenerate \\(n\\) random numbers \\(u_1, u_2, \\ldots, u_n\\) from the uniform distribution \\(U(0, 1)\\).\nSort the numbers in increasing order \\(u_{(1)} \\leq u_{(2)} \\leq \\ldots \\leq u_{(n)}\\).\nReturn \\(u_{(k)}\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Other Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/other_distributions.html#mixture-distributions",
    "href": "chapters/sampling/other_distributions.html#mixture-distributions",
    "title": "8  Other Distributions",
    "section": "8.2 Mixture Distributions",
    "text": "8.2 Mixture Distributions\nA mixture distribution is a probability distribution that is formed by taking a weighted sum of two or more probability distributions. Let \\(X\\) be a random variable that is a mixture of distributions \\(f_1(x), f_2(x), \\ldots, f_n(x)\\) with weights \\(w_1, w_2, \\ldots, w_n\\). The probability density function of \\(X\\) is given by\n\\[\\begin{align*}\nf(x) = w_1 f_1(x) + w_2 f_2(x) + \\ldots + w_n f_n(x).\n\\end{align*}\\]\nMixture distributions are used to model complex distributions that cannot be modeled by a single distribution. We can sample from a mixture distribution by first selecting which component distribution to sample from (with probabilities given by the weights), then sampling from that selected distribution.\nTo sample from a mixture distribution, we can use the following algorithm:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSample from the discrete distribution with probabilities \\([w_1, w_2, \\ldots, w_n]\\) to select a component distribution.\nSample from the selected component distribution.\nReturn the sample.\n\n\n\n\n\n\n\n\n\nMixture vs Linear Combination\n\n\n\nNote that this is not the same as constructing a linear combination of the component distributions. For example, if \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\(w_1 X_1 + w_2 X_2\\) is a normal distribution with mean \\(w_1 \\mu_1 + w_2 \\mu_2\\) and variance \\(w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2\\). This is not the same as a mixture of two normal distributions, which would be bimodal if \\(\\mu_1\\) and \\(\\mu_2\\) are sufficiently different.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Other Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html",
    "href": "chapters/sampling/accept_reject.html",
    "title": "9  Rejection Sampling",
    "section": "",
    "text": "9.1 Mathematical Foundations\nThe accept-reject method, also called rejection sampling, is a simple and general technique for generating random variables. It is based on the idea of sampling from a simple distribution and then rejecting the samples that are not in the desired distribution. This method is particularly useful when the desired distribution is difficult to sample from directly, but it is easy to evaluate the density function of the distribution.\nFor the accept-reject method, we need to recall the following definitions:\nLet \\(X\\) and \\(Y\\) be discrete random variables.\nThe joint distribution of \\(X\\) and \\(Y\\) is given by the probability mass function\n\\[f_{X,Y}(x,y) = \\mathbb{P}(X=x, Y=y) \\tag{9.1}\\]\nThe marginal distribution of \\(X\\) is given by the probability mass function\n\\[f_X(x) = \\mathbb{P}(X=x) = \\sum_{y} f_{X,Y}(x,y) \\tag{9.2}\\]\nThe conditional probability of \\(X\\) given \\(Y\\) is defined as\n\\[f_{X|Y}(x|y) = \\mathbb{P}(X=x|Y=y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\tag{9.3}\\]",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#accept-reject-method-v1",
    "href": "chapters/sampling/accept_reject.html#accept-reject-method-v1",
    "title": "9  Rejection Sampling",
    "section": "",
    "text": "Sample \\(x\\) uniformly from \\([a, b]\\).\nSample \\(y\\) uniformly from \\([0, M]\\).\nIf \\(y \\leq p(x)\\), return \\(x\\); otherwise, go back to step 1.\n\n\nExample 9.1 Consider the triangular distribution\n\\[\np(x) = \\begin{cases}\n4x, & \\text{if } 0 \\leq x \\leq 0.5, \\\\\n4(1-x), & \\text{if } 0.5 \\leq x \\leq 1 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\tag{9.2}\\]\nWe can use the accept-reject method to sample from this distribution. The density function is supported over \\([0, 1]\\) and is bounded by \\(M = 2\\).\n\n\n\n\n\n9.1.1 Efficiency\nNote that unlike the methods we have seen so far, the accept-reject method is probabilistic. The method generates uniformly distributed samples in the rectangle of area \\(M(b-a)\\), where \\(M\\) is the bound on \\(p(x)\\) and \\([a, b]\\) is the support of \\(p(x)\\). But because \\(p(x)\\) is a probability distribution, the area under the curve is 1. Thus the efficiency of the accept-reject method is given by\n\\[\\begin{align*}\n\\text{Efficiency} = \\frac{1}{M(b-a)}.\n\\end{align*}\\]\nIf \\(M\\) is large i.e. the probability distribution has a large peak, then the efficiency of the accept-reject method is low.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#accept-reject-method-v2",
    "href": "chapters/sampling/accept_reject.html#accept-reject-method-v2",
    "title": "9  Rejection Sampling",
    "section": "9.4 Accept-Reject Method v2",
    "text": "9.4 Accept-Reject Method v2\nA better version of the accept-reject method is obtained by replacing the enveloping rectangle with an enveloping curve. The closer the enveloping curve is to the distribution, the higher the efficiency of the method.\n\nDefinition 9.1 (Definition: Majorizing Function) Let \\(p(x)\\) be a probability distribution. A function \\(g(x)\\) is said to majorize \\(p(x)\\) if \\(g(x) \\geq p(x)\\) for all \\(x\\) in the support of \\(p(x)\\).\nIn the accept-reject jargon, we call \\(p(x)\\) the target distribution and \\(g(x)\\) the proposal distribution. Note that a probability distribution can never majorize another probability distribution as the area under the curve is 1. But we can scale the proposal distribution by a constant \\(M\\) such that \\(Mg(x)\\) majorizes \\(p(x)\\).\n\n\n\n\n\n\n\nKey Requirements for Proposal Distribution\n\n\n\nFor \\(g(x)\\) to be a valid proposal distribution, it must satisfy:\n\nMajorization: \\(Mg(x) \\geq p(x)\\) for all \\(x\\) in the support of \\(p(x)\\).\nEasy sampling: We must be able to easily generate samples from \\(g(x)\\).\nEasy evaluation: Both \\(g(x)\\) and \\(p(x)\\) must be easy to evaluate.\nEfficiency: \\(Mg(x)\\) should be as close to \\(p(x)\\) as possible to minimize rejections.\n\n\n\n\n9.4.1 Improved Algorithm\n\n\n\n\n\n\nAlgorithm: Accept-Reject with Proposal Distribution\n\n\n\nGiven a target distribution \\(p(x)\\) and a proposal distribution \\(g(x)\\) with scaling constant \\(M\\) such that \\(Mg(x) \\geq p(x)\\):\n\nGenerate proposal: Sample \\(x\\) from the proposal distribution \\(g(x)\\).\nGenerate test value: Sample \\(u\\) uniformly from \\([0, 1]\\).\nAccept or reject: If \\(u \\leq \\frac{p(x)}{Mg(x)}\\), accept and return \\(x\\); otherwise, reject and go back to step 1.\n\n\n\n\n\n\n\n\n\nWhy This Works\n\n\n\nThe acceptance probability \\(\\frac{p(x)}{Mg(x)}\\) ensures that we accept samples with probability proportional to how well the target density \\(p(x)\\) matches the scaled proposal density \\(Mg(x)\\) at point \\(x\\).\n\n\n\n\n9.4.2 Worked Example\n\nExample 9.2 (Example: Beta Proposal for Triangular Distribution) Consider the triangular distribution given by Equation 9.5. We saw that we can use the enveloping rectangle with \\(M = 2\\) to sample from this distribution. The Beta distribution \\(\\text{Beta}(2, 2)\\) provides a better majorizing function with constant \\(M = 4/3\\).\nThe \\(\\text{Beta}(2, 2)\\) distribution has density: \\[g(x) = 6x(1-x), \\quad x \\in [0, 1] \\tag{9.7}\\]\n\n\n\n\n\n\n\n\n\nEfficiency comparison:\n\nRectangle method: Efficiency = \\(\\frac{1}{2 \\cdot 1} = 50\\%\\)\nBeta proposal: Efficiency = \\(\\frac{1}{M} = \\frac{1}{4/3} = 75\\%\\)\n\nThe Beta proposal method is significantly more efficient!\n\n\n\n\n\n\n\nFinding the Optimal Scaling Constant\n\n\n\nThe scaling constant \\(M\\) should be chosen as the smallest value such that \\(Mg(x) \\geq p(x)\\) for all \\(x\\). This typically requires: \\[M = \\max_x \\frac{p(x)}{g(x)}\\] Finding this maximum may require calculus or numerical optimization.\n\n\nIn order to run the accept-reject method, we need to sample uniformly from the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis. This can be done using the following theorem:\n\nTheorem 9.2 (Uniform Sampling Under Majorizing Function) Let \\(g(x)\\) be a probability distribution and let \\(M\\) be a constant. Let \\(X\\) and \\(Y\\) be two random variables such that \\(X \\sim g(x)\\) and \\((Y | X = x) \\sim U(0, Mg(x))\\). Then the joint distribution of \\(X\\) and \\(Y\\) is uniform over the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis.\n\n\nProof. The conditional probability of \\(Y\\) given \\(X\\) is given by\n\\[f_{Y|X}(y|x) = \\begin{cases}\n\\frac{1}{Mg(x)}, & \\text{if } 0 \\leq y \\leq Mg(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\\]\nHence, the joint distribution of \\(X\\) and \\(Y\\) is given by\n\\[f_{X,Y}(x,y) = f_{Y|X}(y|x) g(x) = \\begin{cases}\n\\frac{1}{M}, & \\text{if } 0 \\leq y \\leq Mg(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\\]\nHence, the joint distribution is uniform over the region between the graph of \\(Mg(x)\\) and the \\(x\\)-axis.\nNote that we are seeing the constant \\(1/M\\) instead of 1 as the area under the curve \\(y = Mg(x)\\) is \\(M\\) and not 1.\n\\(\\blacksquare\\)\n\n\n\n\n\n\n\nGeometric Interpretation\n\n\n\nThis theorem establishes that we can generate uniform points under any scaled probability density \\(Mg(x)\\) by first sampling from the base distribution \\(g(x)\\), then sampling uniformly in the vertical direction up to the curve height.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#normalizing-constant",
    "href": "chapters/sampling/accept_reject.html#normalizing-constant",
    "title": "9  Rejection Sampling",
    "section": "9.6 Normalizing Constant",
    "text": "9.6 Normalizing Constant\nWe often find ourselves in a situation where we know the probability density function \\(f\\) up to a normalizing constant. For example, the gamma distribution has the density function\n\\[p(x) = c x^{\\alpha - 1} e^{-x/\\beta} \\tag{9.8}\\]\nwhere \\(c\\) is a normalizing constant. We can compute \\(c\\) by integrating \\(p(x)\\) and setting the integral to 1. But as it turns out we do not need to know \\(c\\) to sample from the distribution. We can use the accept-reject method to sample from the distribution without knowing the normalizing constant. For this we need the following theorem:\n\nTheorem 9.3 (Sampling with Unknown Normalizing Constant) Let \\(p(x)\\) be any function with a finite integral \\(c\\). Let \\(X, Y\\) be two random variables having the joint distribution\n\\[f_{X,Y}(x,y) = \\begin{cases}\n\\frac{1}{c}, & \\text{if } 0 \\leq y \\leq p(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases} \\tag{9.9}\\]\nThen the marginal distribution of \\(X\\) is given by \\(p(x)/c\\).\n\n\nProof. The marginal distribution of \\(X\\) is given by\n\\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) dy = \\int_{0}^{p(x)} \\frac{1}{c} dy = \\frac{p(x)}{c}\\]\n\\(\\blacksquare\\)\n\n\n\n\n\n\n\nKey Insight\n\n\n\nSampling from a uniform distribution over the graph of \\(p(x)\\) and then finding the marginal distribution of \\(X\\) gives us a sample from the distribution \\(p(x)/c\\). This is a powerful result as it allows us to sample from a distribution without knowing the normalizing constant.\n\n\n\n9.6.1 Efficiency with Unknown Normalizing Constants\nIf we majorize \\(p(x)\\) with a proposal distribution \\(Mg(x)\\), we can use the accept-reject method to sample from the distribution \\(p(x)/c\\) without knowing the normalizing constant. The efficiency in this case will be given by\n\\[\\text{Efficiency} = \\frac{\\text{area under the graph of } p(x)}{\\text{area under the graph of } Mg(x)} = \\frac{c}{M} \\tag{9.10}\\]\n\n\n\n\n\n\nNo Loss of Efficiency\n\n\n\nNote that there is no loss in efficiency due to the unknown normalizing constant. If we can majorize \\(p(x)\\) with \\(Mg(x)\\), we can majorize the normalized density \\(p(x)/c\\) by \\(Mg(x)/c\\), giving us the same efficiency ratio.\n\n\n\n\n\n\n\n\nAlgorithm: Accept-Reject with Unknown Normalizing Constant\n\n\n\nGiven an unnormalized target function \\(p(x)\\) and a proposal distribution \\(g(x)\\) with scaling constant \\(M\\) such that \\(Mg(x) \\geq p(x)\\):\n\nSample \\(x\\) from \\(g(x)\\).\nSample \\(u\\) uniformly from \\([0, 1]\\).\nIf \\(u \\leq \\frac{p(x)}{Mg(x)}\\), return \\(x\\); otherwise, go back to step 1.\n\nNote that we never need to compute the normalizing constant \\(c\\) explicitly!",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#final-remarks",
    "href": "chapters/sampling/accept_reject.html#final-remarks",
    "title": "9  Rejection Sampling",
    "section": "9.7 Final Remarks",
    "text": "9.7 Final Remarks\n\n\n\n\n\n\nLimitations of Accept-Reject Methods\n\n\n\nIf the proposal distribution is not close to the target distribution, then the efficiency of the method is low. In higher dimensions, it becomes increasingly difficult to come up with a good proposal distribution due to the curse of dimensionality.\nKey challenges include:\n\nVolume growth: In \\(d\\) dimensions, volumes grow as \\(r^d\\), making tight bounds harder to achieve.\nShape matching: Finding proposal distributions that match complex multivariate shapes becomes prohibitive.\nComputational cost: Evaluating density ratios becomes expensive in high dimensions.\n\n\n\n\n9.7.1 Advanced Methods\nIn higher dimensions, we need more sophisticated methods to sample from target distributions such as:\n\nMetropolis-Hastings algorithm: Uses proposal distributions but doesn’t require majorization.\nGibbs sampling: Samples from conditional distributions sequentially.\nHamiltonian Monte Carlo: Uses gradient information for efficient proposals.\n\nWe’ll explore these methods in future sections.\n\n\n9.7.2 Adaptive Rejection Sampling\nAdaptive rejection sampling is another method that tries to find a good proposal distribution by using the samples generated so far. Key features:\n\nAdaptive construction: Uses previous samples to build a piecewise linear majorizing function.\nEasy inversion: The CDF of a piecewise linear function is quadratic and can be inverted easily.\nLog-concave densities: The method works particularly well for log-concave densities.\nExtensions: Has been extended to more general densities beyond log-concave cases.\n\nThis adaptive approach would make an excellent topic for a research project, as it combines the simplicity of accept-reject with the adaptability needed for complex distributions.\n\n\n\nGilks, W. R., & Wild, P. (1992). Adaptive Rejection Sampling for Gibbs Sampling. Journal of the Royal Statistical Society. Series C (Applied Statistics), 41(2), 337–348. https://doi.org/10.2307/2347565",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html",
    "href": "chapters/sampling/gibbs_2d.html",
    "title": "10  Gibbs Sampling",
    "section": "",
    "text": "11 Gibbs Sampling\nGibbs Sampling is a Markov Chain Monte Carlo algorithm that is used to sample from a joint distribution using conditional distributions. For now, we’ll focus on sampling in 2D, but the algorithm generalizes to higher dimensions.\nThe algorithm generates a sample path of length \\(N\\) of the Markov Chain as described in Section 11.3. If needed, we can discard the initial samples to ensure that the Markov Chain has converged to the stationary distribution.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#sec-gibbs-markov-chain",
    "href": "chapters/sampling/gibbs_2d.html#sec-gibbs-markov-chain",
    "title": "10  Gibbs Sampling",
    "section": "11.3 Markov Chain",
    "text": "11.3 Markov Chain\nThe Gibbs sampling algorithm generates a Markov Chain whose state space is the product space of the state spaces of the individual variables \\(\\Omega = \\Omega_X \\times \\Omega_Y\\). In the above examples, the state space is \\([0, D_1] \\times [0, D_2]\\) for the exponential distribution and \\(\\mathbb{R}^2\\) for the bivariate normal distribution.\nThe transition matrix of the Markov Chain in the discrete case is given by:\n\\[\nP \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} \\right) = \\mathbb{P}(X_{i+1} = x' \\mid Y_i = y) \\mathbb{P}(Y_{i+1} = y' \\mid X_{i+1} = x').\n\\]\nIn the continuous case, the transition kernel is given by:\n\\[\nK \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} \\right)\n= f_{X|Y}(x' \\mid y) f_{Y|X}(y' \\mid x').\n\\]\n\n\n\n\n\n\nImportant Detail\n\n\n\nNote that in the transition kernel, we condition \\(Y_{i+1}\\) on the newly sampled value \\(X_{i+1} = x'\\), not on the previous value \\(X_i = x\\). This reflects the sequential nature of the Gibbs sampler.\n\n\n\nTheorem 11.1 (Theorem: Gibbs Chain Stationarity) The Gibbs sampling algorithm generates a Markov Chain with the transition kernel \\(K\\) as described above. The joint distribution \\(f_{X, Y}\\) is a stationary distribution of the Markov Chain. Hence, if the Markov Chain converges to the stationary distribution, the samples generated by the Gibbs algorithm will be distributed according to \\(f_{X, Y}\\).\n\n\nProof. To show that \\(f_{X,Y}\\) is a stationary distribution, we need to verify that if \\((X_i, Y_i) \\sim f_{X,Y}\\), then \\((X_{i+1}, Y_{i+1}) \\sim f_{X,Y}\\) as well.\nLet \\((X_i, Y_i)\\) be distributed according to \\(f_{X,Y}\\). We want to show that \\((X_{i+1}, Y_{i+1})\\) obtained through one step of the Gibbs sampler also follows \\(f_{X,Y}\\).\nThe Gibbs sampler performs the following operations: 1. Sample \\(X_{i+1} \\sim f_{X|Y}(\\cdot \\mid Y_i)\\) 2. Sample \\(Y_{i+1} \\sim f_{Y|X}(\\cdot \\mid X_{i+1})\\)\nWe need to compute the joint density of \\((X_{i+1}, Y_{i+1})\\). Using the law of total probability and the conditional sampling steps:\n\\[\nf_{X_{i+1}, Y_{i+1}}(x', y') = \\int_{\\Omega_Y} f_{X_{i+1}, Y_{i+1} \\mid Y_i}(x', y' \\mid y) f_{Y_i}(y) \\, dy\n\\]\nGiven \\(Y_i = y\\), the Gibbs steps give us: - \\(X_{i+1} \\mid Y_i = y \\sim f_{X|Y}(\\cdot \\mid y)\\) - \\(Y_{i+1} \\mid X_{i+1} = x', Y_i = y \\sim f_{Y|X}(\\cdot \\mid x')\\)\nNote that \\(Y_{i+1}\\) depends only on \\(X_{i+1}\\), not directly on \\(Y_i\\). Therefore:\n\\[\nf_{X_{i+1}, Y_{i+1} \\mid Y_i}(x', y' \\mid y) = f_{X|Y}(x' \\mid y) f_{Y|X}(y' \\mid x')\n\\]\nSubstituting back:\n\\[\n\\begin{aligned}\nf_{X_{i+1}, Y_{i+1}}(x', y') &= \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_{Y|X}(y' \\mid x') f_{Y}(y) \\, dy \\\\\n&= f_{Y|X}(y' \\mid x') \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_{Y}(y) \\, dy\n\\end{aligned}\n\\]\nUsing the definition of marginal density: \\[\nf_X(x') = \\int_{\\Omega_Y} f_{X,Y}(x', y) \\, dy = \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_Y(y) \\, dy\n\\]\nTherefore: \\[\nf_{X_{i+1}, Y_{i+1}}(x', y') = f_{Y|X}(y' \\mid x') f_X(x') = f_{X,Y}(x', y')\n\\]\nwhere the last equality follows from the fundamental relationship \\(f_{X,Y}(x,y) = f_{Y|X}(y \\mid x) f_X(x)\\).\nThus, \\((X_{i+1}, Y_{i+1}) \\sim f_{X,Y}\\), proving that \\(f_{X,Y}\\) is a stationary distribution of the Gibbs Markov Chain. \\(\\blacksquare\\)\n\n\n\n\n\n\n\nKey Insight\n\n\n\nThe proof relies on the fundamental relationships between joint, marginal, and conditional densities. The stationarity follows naturally from the fact that we’re sampling from the correct conditional distributions at each step.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html",
    "href": "chapters/sampling/mh.html",
    "title": "11  Metropolis–Hastings Algorithm",
    "section": "",
    "text": "11.1 Random Walks Metropolis–Hastings Algorithm\nMetropolis–Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to sample from a probability distribution. It is a type of a rejection sampling algorithm where we generate a sequence of samples from a target distribution by proposing a new sample and accepting or rejecting it based on a certain criterion. The algorithm is widely used in Bayesian statistics, statistical physics, and machine learning.\nWe’ll start a few examples to understand the algorithm and then discuss the general algorithm.\nConsider a region \\(\\Omega\\) in \\(\\mathbb{R}^n\\). Suppose we want to sample uniformly from this region. One way to do this is through rejection sampling. We envelope the region in simple shape like a cube and sample uniformly from the cube. If the sample lies in the region \\(\\Omega\\), we accept it, otherwise we reject it. The efficiency of this method depends on the ratio of the volume of the cube to the volume of the region \\(\\Omega\\). If the region is highly non-convex, this ratio can be very small and the rejection sampling can be very inefficient.\nAn alternative method is to use random walks. We start at a point \\(x_0\\) in the region \\(\\Omega\\) and take a random step in a random direction. If the new point \\(x_1\\) is in the region \\(\\Omega\\), we accept it, otherwise we stay at the point \\(x_0\\). We repeat this process to generate a sequence of points. This method is more efficient than rejection sampling for highly non-convex regions.\nThe rationale behind this is that if a point is in the region \\(\\Omega\\), then a point near it is also likely to be in the region. We can use this idea to sample from a probability distribution. The random walk Metropolis–Hastings algorithm for generating \\(N\\) samples from a region \\(\\Omega\\) is as follows:\nThis is an example of a Metropolis–Hastings algorithm. The acceptance criterion is that the new point \\(y\\) should be in the region \\(\\Omega\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#sec-random-walks",
    "href": "chapters/sampling/mh.html#sec-random-walks",
    "title": "11  Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Start at a point \\(x_0\\) in the region \\(\\Omega\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a random step \\(\\sigma x\\).\nCompute the new point \\(y\\) near \\(x_i\\).\nIf \\(y\\) is in the region \\(\\Omega\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\n\n11.1.1 Proposal Distribution\nThe missing piece in the above algorithm is the method to generate a new point \\(y\\) near the current point \\(x_i\\). This is done using a proposal distribution. The proposal distribution can be almost of any form, but it should be easy to sample from. The choice of the proposal distribution is crucial for the efficiency of the algorithm. A good proposal distribution should be able to explore the region \\(\\Omega\\) efficiently.\nIn the case of random walks, there are two common choices for the proposal distribution:\n\nGaussian proposal: We sample a new point from a Gaussian distribution centered at the current point \\(x_i\\) with a certain variance.\nUniform proposal: We sample a new point from a uniform distribution in a neighborhood of the current point \\(x_i\\).\n\n\nExample 11.1 Consider the L-shaped region in \\(\\mathbb{R}^2\\) as shown below. This is a non-convex region and rejection sampling would only provide an efficiency of \\(0.1 + 0.1 - 0.01 = 0.18\\). We’ll use the Metropolis–Hastings algorithm to sample from this region with higher efficiency.\n\n\n\n\n\n\n\n\n\nThe images below show scatter plots for the samples generated using the Metropolis–Hastings algorithm with four different choices of the Gaussian proposal distribution with standard deviations \\(1\\), \\(0.5\\), \\(0.2\\), and \\(0.01\\). The plots to the right are the running averages of the \\(x\\) and \\(y\\) coordinates. The scatter plot and the running averages provide a visual representation of the convergence of the samples to the target distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.1.2 Choosing the Proposal Distribution\nNotice that there is a tradeoff between the acceptance rate and the rate of convergence. In the above example,\n\nThe proposal distribution with \\(\\sigma = 1\\) has the lowest acceptance rate but the samples are well spread out and the underlying markov chain converges quickly.\nThe proposal distribution with \\(\\sigma = 0.01\\) has the highest acceptance rate but the samples are clustered and the underlying markov chain converges slowly. In the above simulation, even after \\(10^5\\) samples (most of which are accepted), the samples are still clustered.\n\nBoth of these are undesirable. We need to choose a proposal distribution that has a good balance between the acceptance rate and the rate of convergence.\nWe see that for \\(\\sigma = 0.5\\), the samples are well spread out and the underlying markov chain converges quickly to the target distribution. This is a good choice for the proposal distribution. However, the acceptance rate is \\(0.07\\) which is lower than the acceptance rate for naive rejection sampling AND the generated samples are correlated. It is better to use rejection sampling than to use this proposal distribution.\nFor \\(\\sigma = 0.2\\), the acceptance rate is \\(0.3\\) which is better than the acceptance rate for naive rejection sampling. The samples are well spread out and the underlying markov chain converges quickly, although not as quickly as for \\(\\sigma = 0.5.\\) For this example, \\(\\sigma = 0.2\\) is a good choice for the proposal distribution. We can fine tune this parameter to get a better acceptance rate, if needed.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#metropolishastings-algorithm",
    "href": "chapters/sampling/mh.html#metropolishastings-algorithm",
    "title": "11  Metropolis–Hastings Algorithm",
    "section": "11.2 Metropolis–Hastings Algorithm",
    "text": "11.2 Metropolis–Hastings Algorithm\nThe general algorithm for the Metropolis–Hastings algorithm is an algorithm for generating a sequence of samples from a probability distribution \\(p(x)\\).\n\n11.2.1 Structure of Metropolis–Hastings Algorithm\nThe above example illustrates the general structure of the Metropolis–Hastings algorithm for sampling from a probability distribution \\(p(x)\\). The algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y\\) close to \\(x_i\\).\nEvaluate the acceptance ratio \\(\\alpha (y | x_i)\\).\nAccept \\(y\\) with probability \\(\\alpha (y | x_i)\\).\n\nIf accepted, set \\(x_{i+1} = y\\).\nElse, set \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\n\n11.2.2 Proposal Distribution\nFor the MH algorithm, we need to specify a proposal distribution for generating “nearby points”. This is a joint distribution \\(q(x, y)\\) for two random variables \\(X\\) and \\(Y\\). However, for running the algorithm we only need the conditional distribution \\(q(y | x)\\). As such, it’s more common to say that the proposal distribution is the conditional distribution \\(q(y | x)\\). We think of \\(x\\) as the current point and \\(y\\) as the proposed point so that the proposal distribution \\(q(y | x)\\) is the distribution of the proposed point \\(y\\) given the current point \\(x\\).\nIn the Example 11.1, the proposal distribution \\(q(y|x)\\) was a Gaussian distribution centered at \\(x\\) with a chosen variance,\n\\[\\begin{align*}\nq(y | x) = \\mathcal{N}(y ; x, \\sigma^2 I).\n\\end{align*}\\]\nThe proposal distribution can be any distribution that is easy to sample from. The choice of the proposal distribution is crucial for the efficiency of the algorithm.\n\n\n11.2.3 Acceptance Criterion\nOnce we generate a proposal \\(y\\), we need to evaluate an acceptance criterion for \\(w\\). The acceptance criterion is based on the ratio of the target distribution \\(p(x)\\) and the proposal distribution \\(q(y | x)\\). We calculate the acceptance ratio as\n\\[\n\\alpha (y | x) = \\min \\left\\{ \\frac{p(y)}{p(x)} \\cdot \\frac{q(x | y)}{q(y | x)}, 1 \\right\\}.\n\\]\nThe complete Metropolis–Hastings algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y\\) from the proposal distribution \\(q(y | x_i)\\).\nCompute the acceptance ratio \\[\n\\alpha(y | x) = \\min\\left\\{ \\frac{p(y)}{p(x)} \\cdot \\frac{q(x | y)}{q(y | x)}, 1 \\right\\}.\n\\]\nAccept \\(y\\) with probability \\(\\alpha(y | x)\\):\n\nIf \\(y\\) is accepted, set \\(x_{i+1} = y\\).\nOtherwise, set \\(x_{i+1} = x_i\\).\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\nWe can expand the algorithm further as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y \\sim q(y | x_i)\\).\nCompute \\[ \\alpha_1 = p(y) \\cdot q(x_i | y), \\quad \\alpha_2 = p(x_i) \\cdot q(y | x_i). \\]\nIf \\(\\alpha_1 \\geq \\alpha_2\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nGenerate a random number \\(u \\sim \\text{Uniform}(0, 1)\\).\nIf \\(u &lt; \\alpha_1 / \\alpha_2\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#symmetric-proposal-distribution",
    "href": "chapters/sampling/mh.html#symmetric-proposal-distribution",
    "title": "11  Metropolis–Hastings Algorithm",
    "section": "11.3 Symmetric Proposal Distribution",
    "text": "11.3 Symmetric Proposal Distribution\nOften we use a symmetric proposal distribution \\(q(y | x) = q(x | y)\\). In this case, the acceptance ratio simplifies to\n\\[\n\\alpha(y | x) = \\min \\left\\{ \\frac{p(y)}{p(x)}, 1 \\right\\}.\n\\]\nThe Metropolis algorithm is a special case of the Metropolis–Hastings algorithm where the proposal distribution is symmetric. The Metropolis algorithm is widely used in practice. The algorithm is as follows:\n\nInitialization: Start at a point \\(x_0\\).\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate a proposal \\(y \\sim q(y | x_i)\\).\nIf \\(p(y) \\geq p(x_i)\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nGenerate a random number \\(u \\sim \\text{Uniform}(0, 1)\\).\nIf \\(u &lt; p(y) / p(x_i)\\),\n\nSet \\(x_{i+1} = y\\).\n\nElse,\n\nSet \\(x_{i+1} = x_i\\).\n\n\n\nReturn the sequence of points \\(x_0, x_1, \\ldots, x_{N-1}\\).\n\n\nExample 11.2 If \\(p(x)\\) is a uniform distribution over the region \\(\\Omega\\), then the Metropolis algorithm reduces to the random walk Metropolis–Hastings algorithm. In this case, the acceptance ratio becomes\n\\[\n\\alpha(y | x) = \\begin{cases}\n1, & \\text{if } y \\in \\Omega, \\\\\n0, & \\text{if } y \\notin \\Omega.\n\\end{cases}\n\\]\nThis means that we always accept a proposal \\(y\\) if it is in the region \\(\\Omega\\) and reject it otherwise. This is equivalent to the random walk Metropolis–Hastings algorithm in Section 11.1.\nSeveral other algorithms, such as rejection sampling, random walks on graphs, and Gibbs sampling, can be viewed as special cases of the Metropolis–Hastings algorithm.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/mh.html#metropolis-markov-chain",
    "href": "chapters/sampling/mh.html#metropolis-markov-chain",
    "title": "11  Metropolis–Hastings Algorithm",
    "section": "11.4 Metropolis Markov Chain",
    "text": "11.4 Metropolis Markov Chain\nIf the target distribution \\(p(x)\\) is defined over the sample space \\(\\Omega\\), then the Metropolis–Hastings algorithm generates a Markov chain with state space \\(\\Omega\\).\nFor simplicity,\n\nwe will analyze the Metropolis algorithm with a symmetric proposal distribution \\(q(y | x) = q(x | y)\\), and\nwe will assume that the target distribution \\(p(x)\\) is defined over a finite sample space \\(\\Omega\\).\n\nThe transition matrix for the Markov chain is given by\n\\[\n\\begin{aligned}\n\\mathbb{P}(X = b | X = a) = \\begin{cases}\nq(b | a) & \\text{if } p(b) \\geq p(a) \\text{ and } a \\neq b, \\\\\n\\frac{p(b)}{p(a)} \\cdot q(b | a) & \\text{if } p(b) &lt; p(a) \\text{ and } a \\neq b, \\\\\nq(a | a) + \\sum \\limits_{b \\in \\Omega, p(b) &lt; p(a)} q(b | a) \\cdot \\left(1 - \\frac{p(b)}{p(a)}\\right) & \\text{if } a = b.\n\\end{cases}\n\\end{aligned}\n\\]\nOne can check that this defines a valid transition matrix by showing that the sum of the transition probabilities for each state is \\(1\\).\n\\[\n\\begin{aligned}\n&\\sum \\limits_{b \\in \\Omega} \\mathbb{P}(X = b | X = a) \\\\\n&= \\sum \\limits_{p(b) \\ge p(a), a \\neq b} q(b | a)\n+ \\sum \\limits_{p(b) &lt; p(a)} \\frac{p(b)}{p(a)} \\cdot q(b | a)\n+ q(a|a)\n+ \\sum \\limits_{p(b) &lt; p(a)} q(b | a) \\cdot \\left(1 - \\frac{p(b)}{p(a)}\\right) \\\\\n&= \\sum \\limits_{b \\in \\Omega} q(b | a) \\\\\n&= 1.\n\\end{aligned}\n\\]\n\n11.4.1 Detailed Balance Equations\n\nTheorem 11.1 The Metropolis algorithm is reversible with respect to the target distribution \\(p(x)\\). This means that the Markov chain defined by the Metropolis algorithm satisfies the detailed balance equations:\n\\[\np(a) \\cdot \\mathbb{P}(X = b | X = a) = p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\]\n\n\nProof. We only need check the detailed balance equations for the case when \\(a \\neq b\\). The case when \\(a = b\\) is trivial. We make two cases:\n\nCase 1: \\(p(a) \\neq p(b)\\).\n\nWithout loss of generality, assume that \\(p(a) &lt; p(b)\\). Then we have\n\\[\n\\begin{aligned}\np(a) \\cdot \\mathbb{P}(X = b | X = a)\n&= p(a) \\cdot q(b | a) \\\\\n&= p(a) \\cdot q(a | b) \\\\\n&= p(b) \\cdot q(a | b) \\dfrac{p(a)}{p(b)} \\\\\n&= p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\end{aligned}\n\\]\n\nCase 2: \\(p(a) = p(b)\\). In this case, we have \\[\n\\begin{aligned}\np(a) \\cdot \\mathbb{P}(X = b | X = a)\n&= p(a) \\cdot q(b | a) \\\\\n&= p(b) \\cdot q(a | b) \\\\\n&= p(b) \\cdot \\mathbb{P}(X = a | X = b).\n\\end{aligned}\n\\] Thus, in both cases, we have shown that the detailed balance equations hold. This completes the proof.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metropolis--Hastings Algorithm</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html",
    "href": "chapters/variance_reduction.html",
    "title": "12  Variance Reduction",
    "section": "",
    "text": "12.0.1 Markov Chain Monte Carlo (MCMC)\nWhen the random variables \\(X_i\\) are not independent, the variance of the sum is given by\n\\[\n\\text{Var}(\\hat{\\ell}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\text{Var}(f(X_i)) + \\frac{2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1, j \\neq i}^{N} \\text{Cov}(f(X_i), f(X_j)).\n\\]\nThere are \\(n^2\\) terms in the covariance sum, and in general, we cannot guarantee that the covariance is small. So the above confidence interval is not valid. In the case of ergodic Markov chains, the central limit theorem for Markov chains allows us to estimate the CI as \\[\n\\left[ \\hat{\\ell} -  z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}}, \\hat{\\ell} + z_{1-\\alpha/2} \\frac{S_{eff}}{\\sqrt{N_{eff}}} \\right],\n\\] where \\(N_{eff}\\) is the effective sample size. We will not discuss the details of effective sample size here.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#importance-sampling",
    "href": "chapters/variance_reduction.html#importance-sampling",
    "title": "12  Variance Reduction",
    "section": "12.1 Importance Sampling",
    "text": "12.1 Importance Sampling\nImportance sampling is a method to reduce the variance of an estimator by changing the distribution from which we sample. The idea is to reduce the number of low probability events that contribute to the variance of the estimator.\nFor example, when estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\), if \\(\\ell\\) is small, then most of the samples will be in the region \\(X \\leq \\gamma\\), which contributes little to the estimate. However, we cannot just sample from the tail of the distribution as this would provide us no information about the rest of the distribution. Importance sampling allows us to sample from tail but then “fix” the estimate by weighting the samples appropriately.\n\nDefinition: Importance Sampling. Let \\(X\\) be a random variable with probability density function (pdf) \\(p(x)\\), and let \\(q(x)\\) be a proposal pdf such that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(p(x)\\). The importance sampling estimator of \\(\\ell = E[f(X)]\\) is given by \\[\n\\hat{\\ell} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) \\frac{p(X_i)}{q(X_i)},\n\\] where \\(X_1, X_2, \\ldots, X_N\\) are i.i.d. samples from the distribution with pdf \\(q(x)\\).\nFor clarity, we’ll denote the estimator in ?eq-crude-estimator as \\(\\hat{\\ell}_{crude}\\) and the importance sampling estimator as \\(\\hat{\\ell}_{IS}\\).\n\nSuppose \\(N = 1\\) so that the estimator is given by \\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}.\n\\] Note that here \\(X \\sim q(x)\\) and NOT \\(p(x)\\). We can check that this estimator is unbiased: \\[\n\\begin{aligned}\nE_q[\\hat{\\ell}_{IS}] &= E_q\\left[f(X) \\frac{p(X)}{q(X)}\\right] \\\\\n&= \\int f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\n&= \\int f(x) p(x) dx \\\\\n&= E[f(X)] \\\\\n&= \\ell.\n\\end{aligned}\n\\]\nHowever, \\[\n\\begin{aligned}\n\\text{Var}(\\hat{\\ell}_{IS})\n&\\neq \\text{Var}(\\hat{\\ell}_{crude}).\n\\end{aligned}\n\\]\nThis allows us to reduce the variance of the estimator by choosing \\(q(x)\\) appropriately.\nSuppose \\(f(X)\\) is a non-negative function. Then if we choose\n\\[\nq(x) \\propto p(x) f(x),\n\\]\nthen the importance sampling estimator for \\(N = 1\\)\n\\[\n\\hat{\\ell}_{IS} = f(X) \\frac{p(X)}{q(X)}\n\\]\nis a constant and has zero variance! When \\(H\\) is not non-negative, we can show that\n\\[\nq(x) \\propto p(x) |f(x)|\n\\]\nminimizes the variance of the estimator \\(\\hat{\\ell}_{IS}\\).\nHowever, note that our goal is to estimate \\(\\ell = E[f(X)]\\), which means that we do not know \\(f(x)\\) in advance. So we cannot choose this \\(q(x)\\) in advance. Even if we could, we might not be able to sample from \\(q(x)\\) easily. In practice, we choose \\(q(x)\\) to be a distribution that is easy to sample from and that is “close” to \\(p(x)\\) in some sense.\n\nExample: Importance Sampling for Rare Events. Consider the problem of estimating the tail probability \\(\\ell = P(X &gt; \\gamma)\\) for a random variable \\(X\\) with standard normal distribution. We can use importance sampling to estimate this probability by choosing a proposal distribution \\(q(x)\\) that is concentrated in the tail region. One such distribution is the exponential distribution with parameter \\(\\lambda\\), which has pdf \\[\nq(x) = \\lambda e^{-\\lambda (x-2)}, \\quad x \\ge 2.\n\\] The plots below show the running averages of the crude and importance sampling estimators for \\(N = 2000\\) samples. The importance sampling estimator is much more stable and converges to the true value of \\(\\ell\\) much faster than the crude estimator.\n\n\n\n\n\n\n\n\n\n\nVariance of Importance Sampling Estimate: 0.00000\nRelative Error of Importance Sampling Estimate: 0.01247\nVariance of Crude Monte Carlo Estimate: 0.00001\nRelative Error of Crude Monte Carlo Estimate: 0.09231\n\n\n\n12.1.1 Remarks\n\nThe optimal choice of the proposal distribution \\(q(x)\\) is not always easy to find. Even if we can find it, we may not be able to sample from it easily. For importance sampling algorithm, we need to be able to sample from \\(q(x)\\). Often, we use a distribution that is easy to sample from and that is “close” to \\(|f(x)|p(x)\\) in some sense.\nUnlike rejection sampling and MCMC methods, for importance sampling we need to know the normalizing constant of the proposal distribution \\(q(x)\\) in order to compute the weights. This means that we are fairly limited in the choice of \\(q(x)\\). Some common choices are the exponential distribution, the normal distribution, and the uniform distribution, and a mixture of these distributions.\nIn order for the estimator to be well-defined, we need to ensure that \\(q(x) &gt; 0\\) for all \\(x\\) in the support of \\(f(x) p(x)\\). As in the case of rare-event estimation, the support of \\(f(x) p(x)\\) may be very small compared to the support of \\(p(x)\\). We only need \\(q(x)\\) to be positive in this smaller region.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/variance_reduction.html#antithetic-and-control-random-variates",
    "href": "chapters/variance_reduction.html#antithetic-and-control-random-variates",
    "title": "12  Variance Reduction",
    "section": "12.2 Antithetic and Control Random Variates",
    "text": "12.2 Antithetic and Control Random Variates\nIn this section, we will discuss two methods of variance reduction that are based on the idea of using correlated random variables: antithetic variables and control variates. Recall that the variance of a sum of two random variables is given by\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y) + 2\\text{cov}(X, Y).\n\\]\nOftentimes, having correlated random variables in undesirable as it reduces to an increase in variance and a decrease in the effective sample size. However, in some cases, we can use this correlation to our advantage.\n\n12.2.1 Antithetic Variates\nAntithetic variates are pairs of random variables that are negatively correlated. If we can find an estimator that uses sums to two antithetic random variables, we can reduce its variance.\nConsider the example of estimating the integral from ?sec-estimating-integrals. Suppose \\(f(x)\\) is a monotonic function over \\([a, b]\\). Then you’ll show on the homework that if \\(X \\sim U(a, b)\\) then\n\\[\n\\text{cov}(f(X), f(a + b - X)) \\le 0.\n\\]\nWe can see intuitively why this is happening - if \\(f(x)\\) is increasing the \\(f(b - x)\\) is decreasing and vice versa, and hence the two are negatively correlated. In this case, we can reduce the variance of the crude estimator by instead using\n\\[\n\\hat{\\ell}_{anti} = \\dfrac{(b - a)}{N} \\sum \\limits_{i = 1}^{2N} \\left(f(X) + f(a + b - X)\\right)\n\\]\nNote that if \\(X \\sim U(a, b)\\) then so is \\(b - X\\) and so \\(\\hat{\\ell}_{anti}\\) is an unbiased estimator.\n\nTheorem 12.1 \\(\\text{var}(\\hat{\\ell}_{anti}) \\le \\text{var}(\\hat{\\ell}_{crude})\\).\n\n\nExample 12.1 Example: Antithetic Variates. Consider the problem of estimating the integral \\(\\ell = \\int_0^1 (1 + x^2)^{-1} dx\\) using antithetic variates. The plots below show the running averages of the crude and antithetic variate estimators for \\(N = 100\\) samples. The antithetic variate estimator achieves a \\(50x\\) reduction in variance compared to the crude estimator.\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Monte Carlo Estimate: 0.770064, Variance: 1.180981e-04\nAntithetic Variates Estimate: 0.787292, Variance: 1.917273e-06\nVariance Reduction Factor: 61.60x\n\n\n\n\n12.2.2 Control Variates\nControl random variables are examples of random variables that are positively correlated. In this case, the difference\n\\[\n\\text{var}(X - Y) = \\text{var}(X) + \\text{var}(Y) - 2 \\text{cov}(X, Y)\n\\]\nwill have lower variance. Let \\(X \\sim p(x)\\). Suppose we want to estimate \\(\\ell = \\mathbb{E}[f(x)]\\) for some function \\(f(x)\\). We can use a control variate \\(Y = h(X)\\) for some function \\(h(x)\\) such that\n\n\\(\\mathbb{E}[h(X)]\\) is known, say \\(\\mathbb{E}[h(X)] = h_0\\).\n\\(h(x)\\) is strongly positively correlated with \\(f(x)\\), i.e., \\(\\text{cov}(f(X), h(X)) \\gg 0\\).\n\nThen we can use the control variate estimator\n\\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (h(X_i) - h_0)\\right]\n\\]\nwhere \\(\\beta\\) is a constant. It is easy to see that \\(\\hat{\\ell}_\\text{CV}\\) is an unbiased estimator of \\(\\ell\\). The variance of the control variate estimator is given by\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\ell}_\\text{CV})\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta^2 \\text{var}(h(X)) - 2 \\beta \\text{cov}(f(X), h(X))\\right] \\\\\n&= \\frac{1}{n} \\left[\\text{var}(f(X)) + \\beta \\text{var}(h(X)) \\left[ \\beta - 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\right]\\right]\n\\end{aligned}\n\\]\nBy choosing \\(\\beta &lt; 2\\frac{\\text{cov}(f(X), h(X))}{\\text{var}(h(X))}\\), we can reduce the variance of the control variate estimator. In practice, it is not easy to calculate the covariance between \\(f(X)\\) and \\(h(X)\\), so we\n\nPick a control variate \\(h(X)\\) that is strongly correlated with \\(f(X)\\), and whose expectation is known.\nExperiment with different values of \\(\\beta\\) to find the one that minimizes the variance of the control variate estimator.\n\n\nExample 12.2 In the example below, we estimate the integral of \\(x e^{-x}\\) over the interval \\([0, 1]\\) using control function \\(g(x) = x\\). We know that \\(\\mathbb{E}[g(X)] = \\frac{1}{2}\\) so the estimator is given by \\[\n\\hat{\\ell}_\\text{CV} = \\frac{1}{n} \\sum_{i=1}^n \\left[f(X_i) - \\beta (g(X_i) - \\frac{1}{2})\\right]\n\\] where \\(\\beta\\) is a constant and \\(X_i \\sim \\text{Uniform}(0, 1)\\) are i.i.d. We choose \\(\\beta \\approx 0.35\\) which minimizes the variance of the control variate estimator. This results in an 8-fold reduction in variance compared to the naive estimator.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variance Reduction</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html",
    "href": "chapters/applications/SDE.html",
    "title": "13  Stochastic Differential Equations",
    "section": "",
    "text": "13.1 From Ordinary to Stochastic Differential Equations\nIn this module, we will look at the Euler-Maruyama method for solving stochastic differential equations. Our focus will be on understanding numerical stability and convergence of the method.\nRecall that the most basic differential equation, which is at the foundation of theory of ordinary differential equations, is the first order ordinary differential equation (ODE) of the form\n\\[\\frac{dy}{dt} = \\lambda y \\tag{13.1}\\]\nwhere \\(\\lambda\\) is a constant. The solution to this ODE is given by\n\\[y(t) = y(0) e^{\\lambda t} \\tag{13.2}\\]\nWe want to modify the ODE Equation 13.1 to include noise:\n\\[\\frac{dy}{dt} = \\lambda y + \\text{noise}\\]\nTo make sense of this, we’ll make two changes:",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#stochastic-processes",
    "href": "chapters/applications/SDE.html#stochastic-processes",
    "title": "13  Stochastic Differential Equations",
    "section": "13.2 Stochastic Processes",
    "text": "13.2 Stochastic Processes\nA stochastic process is a time-dependent random variable. More precisely, it is a function of the form\n\\[X(t, \\omega): \\mathbb{R} \\times \\Omega \\to \\mathbb{R} \\tag{13.3}\\]\nwhere \\(\\Omega\\) is the sample space. For example, if \\(\\Omega\\) is the set of gas molecules in a room, then \\(X(t, i)\\) could be the position of the \\(i\\)-th molecule at time \\(t\\). For a fixed \\(t\\), \\(X(t, \\cdot)\\) is a random variable. For a fixed \\(\\omega\\), \\(X(\\cdot, \\omega)\\) is a function of time. This function, \\(X(\\cdot, \\omega)\\), is called a sample path of the stochastic process.\n\n13.2.1 Wiener Process\nA Wiener process, \\(W(t)\\), also known as Brownian motion, is a stochastic process with the following properties:\n\n\\(W(0) = 0\\).\nFor \\(0 \\leq s &lt; t\\), the increment \\(W(t) - W(s)\\) is normally distributed with mean \\(0\\) and variance \\(t-s\\): \\[W(t) - W(s) \\sim \\mathcal{N}(0, t-s)\\]\nThe increments are independent.\nThe process is continuous but nowhere differentiable.\n\nThis results in a simple algorithm for generating sample paths of Brownian motion:\n\n\n\n\n\n\nAlgorithm: Brownian Motion Path Generation\n\n\n\n\nInitialize \\(W_0 = 0\\).\nChoose time step \\(\\Delta t = \\frac{T}{N}\\) where \\(T\\) is the final time and \\(N\\) is the number of steps.\nFor \\(n = 0, 1, 2, \\ldots, N-1\\), do:\n\nGenerate a random number \\(Z_n \\sim \\mathcal{N}(0, 1)\\).\nCompute \\(W_{n+1} = W_n + \\sqrt{\\Delta t} \\cdot Z_n\\).\n\nReturn \\(W_0, W_1, \\ldots, W_N\\) at times \\(t_0 = 0, t_1 = \\Delta t, \\ldots, t_N = T\\).\n\n\n\nThis algorithm exploits the fact that Brownian motion has independent increments with \\(W_{t+\\Delta t} - W_t \\sim \\mathcal{N}(0, \\Delta t)\\), making it a special case of the Euler-Maruyama method from Section 13.5 with \\(a(x,t) = 0\\) and \\(b(x,t) = 1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can derive the properties of the Wiener process by taking the continuous limit of a random walk. Note that for all time \\(t\\), the mean of \\(W(t)\\) is \\(0\\). However, the variance grows with time. Imagine a box of gas particles all starting at the origin without any initial velocity or external forces. As there is no external force or initial velocity, the center of mass of the gas particles will remain at the origin. However, the gas particles will spread out over time. The Wiener process models this spreading out of gas particles.\nFrom the definition, we can derive several important properties:\n\nMean: \\(\\mathbb{E}[W(t)] = 0\\) for all \\(t \\geq 0\\).\nVariance: \\(\\text{Var}(W(t)) = t\\) for all \\(t \\geq 0\\).\nCovariance: \\(\\text{Cov}(W(s), W(t)) = \\min(s,t)\\) for \\(s, t \\geq 0\\).\nDistribution: \\(W(t) \\sim \\mathcal{N}(0, t)\\) for any \\(t &gt; 0\\).\n\n\n\n\n\n\n\nMathematical Subtlety\n\n\n\nThe nowhere differentiable property of Wiener processes means that expressions like \\(\\frac{dW}{dt}\\) don’t exist in the classical sense. This is why we need special mathematical machinery (Itô calculus) to handle stochastic differential equations properly.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#stochastic-differential-equations",
    "href": "chapters/applications/SDE.html#stochastic-differential-equations",
    "title": "13  Stochastic Differential Equations",
    "section": "13.3 Stochastic Differential Equations",
    "text": "13.3 Stochastic Differential Equations\nA stochastic differential equation (SDE) is an equation of the form\n\\[dX(t) = a(X(t), t) dt + b(X(t), t) dW(t) \\tag{13.4}\\]\nwhere \\(a\\) and \\(b\\) are functions of \\(X(t)\\) and \\(t\\). The term \\(a(X(t), t) dt\\) is the deterministic part of the equation and \\(b(X(t), t) dW(t)\\) is the stochastic part. The solution to an SDE is a stochastic process \\(X(t)\\), and \\(W(t)\\) is the Wiener process.\nWe write the equation in this form because the Wiener process is not differentiable. We interpret the equation as saying\n\\[X(t) - X(0) = \\int_0^t a(X(s), s) ds + \\int_0^t b(X(s), s) dW(s) \\tag{13.5}\\]\n\n13.3.1 Itô vs Stratonovich Interpretation\nWe can try to define the above integrals in the usual sense using Riemann sums. The first integral can be written as the limit\n\\[\\int_0^t a(X(s), s) ds = \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} a(X(t_i), t_i) \\Delta t_i\\]\nwhere \\(t_i\\) is some point in the interval \\([t_{i}, t_{i+1}]\\) and \\(\\Delta t = t/n\\). Even though function \\(a(X(s), s)\\) is a stochastic process, it is mathematically still just a function. The integral can be defined in the usual sense.\nHowever, the second integral is more problematic. We can try to define it as\n\\[\\int_0^t b(X(s), s) dW(s) = \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} b(X(t_i), t_i) (W(t_{i+1}) - W(t_i))\\]\nwhere \\(t_i\\) is some point in the interval \\([t_{i}, t_{i+1}]\\), \\(\\Delta t = t/n\\), and \\(W(t_{i+1}) - W(t_i)\\) is normally distributed with mean \\(0\\) and variance \\(\\Delta t\\). The problem is that this integral does not converge in the usual sense. Where the sum converges depends on which point in the interval \\([t_{i}, t_{i+1}]\\) we choose to evaluate the integrand. This happens because the Wiener process is not differentiable.\nThere are two commonly used interpretations of the integral:\n\nItô Interpretation: In this interpretation, we use the left end point of the interval to evaluate the integrand. This is the most common interpretation.\nStratonovich Interpretation: In this interpretation, we use the midpoint of the interval to evaluate the integrand.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to convert between the two interpretations using the Itô formula. We will assume the Itô interpretation in this module.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#geometric-brownian-motion",
    "href": "chapters/applications/SDE.html#geometric-brownian-motion",
    "title": "13  Stochastic Differential Equations",
    "section": "13.4 Geometric Brownian Motion",
    "text": "13.4 Geometric Brownian Motion\nA simple example of a stochastic differential equation is the geometric Brownian motion. This is a model for the evolution of stock prices. The equation is\n\\[dX(t) = \\mu X(t) dt + \\sigma X(t) dW(t) \\tag{13.6}\\]\nwhere \\(\\mu\\) is the drift and \\(\\sigma\\) is the volatility. This is one of the few SDEs for which we can find an exact solution. The solution to the Itô version of the equation is\n\\[X(t) = X(0) e^{(\\mu - \\sigma^2/2)t + \\sigma W(t)} \\tag{13.7}\\]\nThis is a log-normal distribution whose mean is given by \\(X(0) e^{\\mu t}\\) and variance is given by \\(X(0)^2 e^{2\\mu t} (e^{\\sigma^2 t} - 1)\\). Note that when \\(\\sigma = 0\\), this reduces to a standard ODE. Unlike Equation 13.2, the solution to the SDE has a quadratic term \\(\\sigma^2/2\\) in the exponent. This term appears due to Itô’s lemma, which is the stochastic analog of the chain rule.\n\n\n\n\n\n\nThe Itô Correction Term\n\n\n\nThe appearance of the \\(-\\sigma^2/2\\) term in Equation 13.7 is a direct consequence of Itô’s lemma. This correction term distinguishes stochastic calculus from ordinary calculus and reflects the quadratic variation of the Wiener process.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#euler-maruyama-method",
    "href": "chapters/applications/SDE.html#euler-maruyama-method",
    "title": "13  Stochastic Differential Equations",
    "section": "13.5 Euler-Maruyama Method",
    "text": "13.5 Euler-Maruyama Method\nMost SDEs do not have analytical solutions. We need to solve them numerically. Similar to ODEs, some simple regularity conditions on the coefficients \\(a\\) and \\(b\\) imply that the SDE has a unique solution. Most common SDEs satisfy these conditions. However, unlike ODEs, the solution to an SDE is a stochastic process. We can’t just evaluate the solution at a few points to get an approximate solution. We need to generate several sample paths of the stochastic process to get an approximate solution.\nThe simplest method for solving SDEs is the Euler-Maruyama method. This is a stochastic analog of the Euler method for ODEs. The Euler-Maruyama method is a recursive method. Given the value of the stochastic process \\(X_n\\) at time \\(t_n\\), we can find the value of the process at time \\(t_{n+1} = t_n + \\Delta t\\) using the formula\n\\[X_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n \\tag{13.8}\\]\nwhere \\(\\Delta t = t_{n+1} - t_n\\) and \\(\\Delta W_n = W(t_{n+1}) - W(t_n) \\sim \\mathcal{N}(0, \\Delta t)\\).\n\n\n\n\n\n\nAlgorithm: Euler-Maruyama Method\n\n\n\nThis results in a simple algorithm for solving SDEs:\n\nInitialize \\(X_0\\).\nFor \\(n = 0, 1, 2, \\ldots, N-1\\), do:\n\nGenerate a random number \\(\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)\\).\nCompute \\(X_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n\\).\n\nReturn \\(X_0, X_1, \\ldots, X_N\\).",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#convergence-of-the-euler-maruyama-method",
    "href": "chapters/applications/SDE.html#convergence-of-the-euler-maruyama-method",
    "title": "13  Stochastic Differential Equations",
    "section": "13.6 Convergence of the Euler-Maruyama Method",
    "text": "13.6 Convergence of the Euler-Maruyama Method\nThe EM method provides a numerical approximation to the solution of the SDE. We want to understand how good this approximation is.\nFix a time interval \\([0, T]\\). Divide the interval \\([0, T]\\) into \\(N\\) subintervals of length \\(\\Delta t = T/N.\\) Let \\(t_i = i \\Delta t\\) so that \\(t_0 = 0\\) and \\(t_N = T\\). Let \\(X(t)\\) be the analytical solution to the SDE at time \\(t\\) with initial condition \\(X(0)\\). We first think of the EM method as generating a sequence of random variables \\(X_0, X_1, \\ldots, X_N\\) defined recursively by\n\\[\\begin{align*}\nX_0 &= X(0), \\\\\nX_{i+1} &= X_i + a(X_i, t_i) \\Delta t + b(X_i, t_i) \\Delta W_i\n\\end{align*}\\]\nwhere \\(\\Delta W_i \\sim \\mathcal{N}(0, \\Delta t)\\).\nThen we are interested in the question of how good the sequence \\(X_0, X_1, \\ldots, X_N\\) is as an approximation to the solution \\(X(t_0), X(t_1), \\ldots, X(t_N)\\). If the EM method is a good approximation method, we should get\n\\[X_i \\to X(t_i) \\text{ as } \\Delta t \\to 0\\]\nHowever, as \\(X_i\\) and \\(X(t_i)\\) are random variables, we need to be more precise about what we mean by convergence. There are two notions of convergence that we are interested in: weak convergence and strong convergence.\n\n13.6.1 Weak Convergence\nWith the setup as above, we say that the EM method converges weakly to the solution over the interval \\([0, T]\\) if\n\\[\\mathbb{E}[X_i] \\to \\mathbb{E}[X(t_i)] \\text{ as } \\Delta t \\to 0\\]\nSince we are interested in using the EM method to approximate the solution to the SDE, we need more than just convergence in expectation. We need to know how fast the method converges. We say that the EM method converges weakly to the solution over the interval \\([0, T]\\) with order \\(p\\) if\n\\[|\\mathbb{E}[X_i] - \\mathbb{E}[X(t_i)]| \\leq C \\Delta t^p \\tag{13.9}\\]\nfor some constant \\(C\\) that does not depend on \\(\\Delta t\\). Note that \\(C\\) is allowed to depend on \\(T\\) and \\(X(0)\\). We often write this as\n\\[|\\mathbb{E}[X_i] - \\mathbb{E}[X(t_i)]| = O(\\Delta t^p)\\]\nto emphasize the rate of convergence.\n\n\n13.6.2 Strong Convergence\nStrong convergence is the convergence of the sample paths. We say that the EM method converges strongly to the solution over the interval \\([0, T]\\) if\n\\[X_i \\xrightarrow{a.s.} X(t_i) \\text{ as } \\Delta t \\to 0\\]\nRecall that convergence almost surely means that the probability of the event that the sequence \\(X_i\\) does not converge to \\(X(t_i)\\) is \\(0\\). By Markov’s inequality it is enough to say that the expected value of the distance between \\(X_i\\) and \\(X(t_i)\\) goes to \\(0\\):\n\\[\\mathbb{E}[|X_i - X(t_i)|] \\to 0 \\text{ as } \\Delta t \\to 0\\]\nWe say that the EM method converges strongly to the solution over the interval \\([0, T]\\) with order \\(p\\) if\n\\[\\mathbb{E}[|X_i - X(t_i)|] = O(\\Delta t^p) \\tag{13.10}\\]\n\n\n\n\n\n\nEmpirical vs Theoretical Convergence Analysis\n\n\n\nIn the homework assignment, you will derive the rates of convergence for the EM method for the geometric Brownian motion empirically. However, it is necessary to derive these rates theoretically as for arbitrary SDEs, it is not possible to write an exact analytical solution. The rates of convergence tell us how close the analytical solution is to the numerical solution.\nAlso, you’ll only estimate \\(p\\) at the end of the interval i.e., \\(t = T\\). This is technically not correct. We should find a \\(p\\) that works for each time step \\(t_i\\). However, this is computationally expensive and the assumption is that error grows with time so that the error at the end of the interval is the largest.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#final-remarks",
    "href": "chapters/applications/SDE.html#final-remarks",
    "title": "13  Stochastic Differential Equations",
    "section": "13.7 Final Remarks",
    "text": "13.7 Final Remarks\nThe EM method has a slower rate of strong convergence compared to weak convergence. This is because even though the EM method is a natural extension of the Euler method for ODEs, there are second order terms in the Itô formula (chain rule for stochastic processes) that contribute to the error. This is fixed in the Milstein method, which is a second order method for solving SDEs. However, the Milstein method is more computationally expensive compared to the EM method and has the same weak rate of convergence as the EM method.\n\n\n\n\n\n\nTrade-offs in SDE Numerical Methods\n\n\n\n\nEuler-Maruyama: Simple to implement, but slower strong convergence.\nMilstein: Better strong convergence, but requires computing derivatives of the diffusion coefficient \\(b(x,t)\\).\nHigher-order methods: Exist but are significantly more complex and computationally expensive.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html",
    "href": "chapters/applications/ising.html",
    "title": "14  Ising Model",
    "section": "",
    "text": "14.1 The Physical Ising Model\nThe Ising model is a simple model of ferromagnetism from statistical mechanics. The setup is as follows:\nNext we create a mathematical model to describe the system.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#the-physical-ising-model",
    "href": "chapters/applications/ising.html#the-physical-ising-model",
    "title": "14  Ising Model",
    "section": "",
    "text": "Particles: We have a lattice of \\(N\\) particles, each of which can be have one of two states of magnetization: spin up (\\(X = 1\\)) or spin down (\\(X = -1\\)). For simplicity, we assume the grid is “wrapped” so that the top and bottom edges are connected, and the left and right edges are connected. This means that each site has four neighbors. Such a grid is called a torus.\nCoupling constant: Two adjacent particles have a tendency to have spins aligned. This tendency is quantified by a coupling constant \\(J\\). If two adjacent particles have the same spin, the energy of the system is lowered by \\(-J\\), and if they have opposite spins, the energy is raised by \\(J\\). Any physical system tends to minimize its energy, so the system will tend to have neighboring particles with the same spin.\nExternal magnetic field: For the same of simplicity we’ll assume that there is no external force field.\nTemperature: Temperature adds randomness to the system. At 0K, all the particles have their spins aligned. As the temperature increases, the particles start to flip their spins. The higher the temperature, the more likely it is for neighboring particles to have opposite spins.\nMagnetization: If the system is in a state where most of the particles have the same spin, the system is said to be magnetized.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#the-mathematical-ising-model",
    "href": "chapters/applications/ising.html#the-mathematical-ising-model",
    "title": "14  Ising Model",
    "section": "14.2 The Mathematical Ising Model",
    "text": "14.2 The Mathematical Ising Model\nWe model the system using \\(N^2\\) random variables \\(X_{i, j}\\). Each \\(X_{i, j}\\) is a binary random variable that takes the value \\(1\\) or \\(-1\\). The configuration of the system is given by the \\(N^2\\) length vector \\(\\mathbf{X} = (X_{1, 1}, X_{1, 2}, \\ldots, X_{N, N})\\). There are \\(2^{N^2}\\) possible configurations of the system i.e. the state space has size \\(2^{N^2}\\). We can imagine each configuration as being the vertex of a hypercube in \\(N^2\\) dimensions with vertices \\((\\pm 1, \\pm 1, \\ldots, \\pm 1)\\). We’ll let the variable \\(\\sigma\\) denote a configuration of the system. So \\(\\sigma\\) is a vector of length \\(N^2\\) with entries in \\(\\{-1, 1\\}\\).\nIn order to describe the system mathematically, we need to provide the joint probability distribution of the random variables \\(X_{i, j}\\). This is done using the Hamiltonian of the system and the Boltzmann distribution.\n\n14.2.1 Hamiltonian\nThe Hamiltonian of the system is given by\n\\[\nH(\\sigma) = -J \\sum \\limits_{(i, j) \\text{ and } (i', j') \\text{ are neighbors}} X_{i, j} X_{i', j'}\n\\]\nwhere the sum is over all pairs of neighboring particles. The Hamiltonian is just a measure of the energy of the system. Note that because \\(J\\) is positive, the energy is minimized when neighboring particles have the same spin.\n\n\n14.2.2 Boltzmann Distribution\nThe probability of the system being in a particular configuration \\(\\sigma\\) is given by the Boltzmann distribution:\n\\[\nf(\\sigma) = \\frac{e^{-H(\\sigma)/T}}{Z}\n\\]\nwhere \\(T\\) is the temperature, and \\(Z\\) is the normalization constant called the partition function. This is the joint probability mass function of the random variables \\(X_{i, j}\\). The partition function \\(Z\\) is not easy to compute, but thankfully we do not need it to sample from the joint distribution.\n\n\n14.2.3 Low Temperature\nIf we plugin the partition function explicitly, we can see that the probability distribution becomes\n\\[\nf(\\sigma) = \\frac{e^{-H(\\sigma)/T}}{\\sum \\limits_{\\sigma'}e^{-H(\\sigma')/T}}.\n\\]\nAs \\(T \\to 0\\), the term with the highest value of \\(-H(\\sigma)\\) will dominate the sum in the denominator. But the term with the highest value of \\(-H(\\sigma)\\) is the one that minimizes \\(H(\\sigma)\\). So,\n\\[\n\\lim \\limits_{T \\to \\infty} f(\\sigma) = \\lim \\limits_{T \\to \\infty}  \\frac{e^{-H(\\sigma)/T}}{e^{-H(\\sigma)_{\\min}/T}} = \\lim \\limits_{T \\to \\infty}  e^{(H(\\sigma)_{\\min} - H(\\sigma))/T}.\n\\]\nIf \\(H(\\sigma)_{\\min} = H(\\sigma)\\), then the probability of the system being in that state is \\(1\\) and \\(0\\) otherwise. This means that at low temperatures, the system will be in a state that minimizes the energy. But as \\(T\\) increases, more and more high energy states become probable. So temperature can be thought of as adding variability to the system.\n\n\n14.2.4 Magnetization\nThe magnetization of the system is given by\n\\[\nM(\\sigma) = \\frac{1}{N^2} \\sum \\limits_{i, j} X_{i, j}\n\\]\nIf the system is magnetized, then \\(M(\\sigma)\\) will be close to \\(1\\) or \\(-1\\). Our goal is to find the expected value of the magnetization of the system. To do this we need to sample from the distribution \\(f(\\sigma)\\) and then compute the magnetization of each sample. The average of these magnetizations will be the expected magnetization of the system.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#gibbs-sampling",
    "href": "chapters/applications/ising.html#gibbs-sampling",
    "title": "14  Ising Model",
    "section": "14.3 Gibbs Sampling",
    "text": "14.3 Gibbs Sampling\nThere are two natural ways of sampling from the distribution \\(f(\\sigma)\\): Metropolis-Hastings and Gibbs sampling. In the Metropolis-Hastings approach, we randomly choose a particle and flip its spin. We then accept or reject the new configuration based on the Metropolis-Hastings acceptance probability. In the Gibbs sampling approach, we update the entire grid of particles one at a time. We update each particle by sampling from the conditional distribution of that particle given the rest of the particles. In this notebook, we’ll use Gibbs sampling.\n\n14.3.1 Conditional Distribution\nConsider a single spin \\(X_{i, j}\\). Let \\(\\hat{X}_{i, j}\\) denote the set of spins at all sites except \\((i, j)\\). We want to find the conditional distribution of \\(X_{i, j}\\) given \\(\\hat{X}_{i, j}\\). We’ll assume the following result without proof:\n\\[\n\\begin{aligned}\nP(X_{i, j} = 1 | \\hat{X}_{i, j}) &= \\dfrac{1}{1 + \\exp(-2 h_{i, j} / T)}, \\\\\nP(X_{i, j} = -1 | \\hat{X}_{i, j}) &= 1 - P(X_{i, j} = 1 | \\hat{X}_{i, j}).\n\\end{aligned}\n\\tag{14.1}\\]\nwhere \\(T\\) is the temperature of the system and \\(h_{i, j}\\) is the effective field at site \\((i, j)\\), given by\n\\[\n\\begin{aligned}\nh_{i, j}\n&= J \\sum_{(i', j') \\in \\text{neighbors}(i, j)} X_{i', j'} \\\\\n&= J \\left( X_{i-1, j} + X_{i+1, j} + X_{i, j-1} + X_{i, j+1} \\right)\n\\end{aligned}\n\\]\nThe conditional distribution in Equation 14.1 is a logistic function. When \\(h_{i, j}\\) is positive, the probability of \\(X_{i, j}\\) being \\(1\\) is greater than \\(0.5\\), and when \\(h_{i, j}\\) is negative, the probability of \\(X_{i, j}\\) being \\(1\\) is less than \\(0.5\\). This makes sense because if the effective field is positive, most of the neighbors of \\((i, j)\\) have spin \\(1\\), so it is likely that \\((i, j)\\) will also have spin \\(1\\) and if the effective field is negative, most of the neighbors of \\((i, j)\\) have spin \\(-1\\), so it is likely that \\((i, j)\\) will also have spin \\(-1\\).\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 The Algorithm\n\nInitialize the grid of spins \\(X_i\\) for \\(i = 1\\) to \\(N\\) randomly.\nFor \\(i = 1\\) to \\(N\\)\n\nFor \\(j = 1\\) to \\(N\\)\n\nCompute the effective field \\(h_{i, j}\\).\nGenerate a random number \\(u\\) from a uniform distribution.\nSet \\(X_{i, j}\\) to \\(1\\) if \\(u &lt; \\dfrac{1}{1 + \\exp(-2 h_{i, j} / T)}\\) and \\(-1\\) otherwise.\n\n\nCompute the magnetization of the system.\nRepeat steps 2 and 3.\n\nEquilibrium/Mixing Time:\nEven when the grid size is modest, the dimensions of the state space is quite large, \\(2^{N^2}\\). This means that the Markov chain will take a long time to mix. Practically, this means that the initial configuration might be far from the stationary distribution i.e. the system has not reached thermal equilibrium. In this case it is important to discard the initial samples to not skew the magnetization estimate.\nOne simple way to this is to run the Gibbs sampler until the running average of the magnetization stabilizes. At that point, we can start using the samples to estimate the magnetization.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#phase-transitions",
    "href": "chapters/applications/ising.html#phase-transitions",
    "title": "14  Ising Model",
    "section": "14.4 Phase Transitions",
    "text": "14.4 Phase Transitions\nIt was discovered that the Ising model for a 2D grid exhibits a phenomenon called phase transition. If we slowly increase the temperature, we expect the magnetization to decrease. What is surprising is that the magnetization decreases smoothly until a certain temperature, after which it drops suddenly. This sudden drop is called a phase transition. The temperature at which this happens is called the critical temperature.\nIt is possible to calculate the critical temperature theoretically, in the limit when the size of the lattice \\(\\to \\infty\\). This limiting critical temperature for a square lattice is given by\n\\[\nT_c = \\dfrac{2J}{\\log(1 + \\sqrt{2})}.\n\\]\nOf course, in our simulation, we’ll be using finite sized lattices so we should expect some deviation from this result.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#simulation",
    "href": "chapters/applications/ising.html#simulation",
    "title": "14  Ising Model",
    "section": "14.5 Simulation",
    "text": "14.5 Simulation\nBelow are the results of simulating the Ising model on a \\(20 \\times 20\\) grid for \\(J = 1\\). The predicted critical temperature is \\(T_c = 2.269\\).\nWe can see that at \\(T=1.5\\), the system is magnetized and the magnetization is more or less constant and close to 1 or -1 over various runs. At \\(T=2.5\\), the system is not magnetized and the average magnetization is close to 0. It fluctuates wildly over various runs.\nIf we gradually increase the temperature from \\(T=1\\) to \\(T=3\\), we can see that the magnetization decreases smoothly until \\(T=2\\) after which it drops suddenly. This is the phase transition. Below the graph are the snapshots of the grid at different temperatures. One can see that at low temperatures, the grid is mostly monochromatic, but at high temperatures, the grid is more chaotic.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/ising.html#final-remarks",
    "href": "chapters/applications/ising.html#final-remarks",
    "title": "14  Ising Model",
    "section": "14.6 Final Remarks",
    "text": "14.6 Final Remarks\nNot all graphs show a phase transition. The Ising model in 1D has no phase transition at any temperature. It was conjectured that the Ising model in 2D has no phase transition either, but this was proven wrong when an analytical solution was found by Lars Onsager in 1944. The Ising model in 3D also has a phase transition, but the critical temperature is not known exactly. Most critical temperatures are only known for limiting cases when the size of the lattice goes to infinity. For finite cases, Monte Carlo simulations are the only way to estimate the critical temperature.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ising Model</span>"
    ]
  },
  {
    "objectID": "chapters/applications/mmn.html",
    "href": "chapters/applications/mmn.html",
    "title": "15  M/M/n Queue",
    "section": "",
    "text": "The M/M/n queue is a discrete stochastic process that models a queue with n servers. There are three parameters:\n\n\\(n\\): the number of servers\n\\(\\lambda\\): the arrival rate of customers\n\\(\\mu\\): the service rate of each server\n\nThe “M” in M/M/n stands for “memoryless” or Markovian. The M/M/n queue models the following process: - customers arrive at the queue according to a Poisson process with rate \\(\\lambda\\). - Each customer is served by one of the n servers, and the service times are exponentially distributed with rate \\(\\mu\\). - If all servers are busy, the customer waits in line until a server becomes available.\nWe are interested in testing the performance of the queue as function of the three parameters. There are various performance measures we can compute, including: - The average number of customers in the waiting line - The average time a customer spends waiting in line - The usage of the servers - The probability that a customer has to wait in line\nThere are two approaches we can take to simulate an M/M/n queue\n\nProcess driven\nEvent driven\n\n\n\nSimulated 1000 customers\nAverage wait time     : 0.4640\nAverage sojourn time  : 1.1262\nServer utilization    : 0.6667 (theoretical)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M/M/n Queue</span>"
    ]
  },
  {
    "objectID": "chapters/applications/bootstrap.html",
    "href": "chapters/applications/bootstrap.html",
    "title": "16  Bootstrap",
    "section": "",
    "text": "Sample Mean: 1.967\nTrue CI: [1.5979144  2.34577445]\nBootstrap CI: [1.62388954 2.35938801]\nNormal-Theory CI: [1.60463905 2.32850834]\n\n\n\n\n\n\n\n\n\n\n\nTrue CI: [1.5979144  2.34577445]\nBootstrap CI: [1.62388954 2.35938801]\nNormal-Theory CI: [1.60463905 2.32850834]\nNaive CI: [-1.65277273  5.58592012]\n\n\n\n\n\n\n\n\n\n\n\nSample Mean: 1.619\nTrue CI:          [0.96432733 3.4209794 ]\nBootstrap CI:     [0.98059327 2.28526628]\nNormal-Theory CI: [0.92412973 2.31300113]",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "22  References",
    "section": "",
    "text": "Rubinstein, R. Y., and D. P. Kroese. 2017. Simulation and the Monte\nCarlo Method. 3rd ed. USA: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html",
    "href": "appendices/probability.html",
    "title": "Appendix A — Review of Probability Theory",
    "section": "",
    "text": "A.1 Probability Spaces and Random Variables\nThis appendix reviews the mathematical foundations underlying Monte Carlo methods, covering probability spaces, random variables, estimation theory, and the fundamental limit theorems that justify Monte Carlo inference.\nA probability space is a triple \\((\\Omega, \\mathcal{F}, P)\\) where \\(\\Omega\\) is the sample space, \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\), and \\(P: \\mathcal{F} \\to [0,1]\\) is a probability measure satisfying Kolmogorov’s axioms.\nA random variable is a measurable function \\(X: \\Omega \\to \\mathbb{R}\\) such that \\(\\{X \\leq x\\} \\in \\mathcal{F}\\) for all \\(x \\in \\mathbb{R}\\). The cumulative distribution function (CDF) of \\(X\\) is: \\[F_X(x) = P(X \\leq x) = P(\\{\\omega \\in \\Omega : X(\\omega) \\leq x\\})\\]\nContinuous random variables have a probability density function (PDF) \\(f_X(x)\\) such that: \\[F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt \\quad \\text{and} \\quad f_X(x) = \\frac{dF_X(x)}{dx}\\]\nDiscrete random variables with support \\(\\{x_1, x_2, \\ldots\\}\\) satisfy \\(P(X = x_i) = p_i\\) where \\(\\sum_{i} p_i = 1\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#probability-spaces-and-random-variables",
    "href": "appendices/probability.html#probability-spaces-and-random-variables",
    "title": "Appendix A — Review of Probability Theory",
    "section": "",
    "text": "Definition (Moments)\n\n\n\nFor a random variable \\(X\\):\nExpected value: \\[\\mathbb{E}[X] = \\begin{cases}\n\\sum_{i} x_i \\cdot P(X = x_i) & \\text{if } X \\text{ is discrete} \\\\\n\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases}\\]\nVariance: \\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#sec-estimation-theory",
    "href": "appendices/probability.html#sec-estimation-theory",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.2 Statistical Estimation and the Sample Mean",
    "text": "A.2 Statistical Estimation and the Sample Mean\nConsider i.i.d. observations \\(X_1, \\ldots, X_N\\) with common distribution \\(F\\). We seek to estimate \\(\\theta = \\mathbb{E}[g(X)]\\) for some measurable function \\(g: \\mathbb{R} \\to \\mathbb{R}\\) using the sample mean estimator: \\[\\hat{\\theta}_N = \\frac{1}{N} \\sum_{i=1}^{N} g(X_i) \\tag{A.1}\\]\n\n\n\n\n\n\nTheorem (Properties of the Sample Mean Estimator)\n\n\n\nThe sample mean estimator \\(\\hat{\\theta}_N\\) satisfies:\n\nUnbiasedness: \\(\\mathbb{E}[\\hat{\\theta}_N] = \\theta\\)\nVariance: \\(\\text{Var}(\\hat{\\theta}_N) = \\frac{\\sigma^2}{N}\\) where \\(\\sigma^2 = \\text{Var}(g(X))\\)\nStandard Error: \\(\\text{SE}(\\hat{\\theta}_N) = \\sigma/\\sqrt{N}\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}_N) = \\sigma^2/N\\)\n\n\n\nFor any estimator \\(\\hat{\\theta}\\) of parameter \\(\\theta\\), we define:\n\nBias: \\(\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\)\nMean Squared Error: \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})\\)\n\nAn estimator sequence \\(\\{\\hat{\\theta}_N\\}\\) is consistent if \\(\\hat{\\theta}_N \\xrightarrow{P} \\theta\\) as \\(N \\to \\infty\\), and strongly consistent if \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\) as \\(N \\to \\infty\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#fundamental-limit-theorems",
    "href": "appendices/probability.html#fundamental-limit-theorems",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.3 Fundamental Limit Theorems",
    "text": "A.3 Fundamental Limit Theorems\nThe theoretical foundation of Monte Carlo methods rests on two cornerstone results from probability theory.\n\n\n\n\n\n\nTheorem (Strong Law of Large Numbers)\n\n\n\nLet \\(X_1, X_2, \\ldots\\) be i.i.d. random variables with \\(\\mathbb{E}[|X_i|] &lt; \\infty\\) and \\(\\mathbb{E}[X_i] = \\mu\\). Then: \\[\\frac{1}{N}\\sum_{i=1}^{N} X_i \\xrightarrow{a.s.} \\mu \\quad \\text{as } N \\to \\infty\\]\n\n\n\n\n\n\n\n\nTheorem (Central Limit Theorem)\n\n\n\nLet \\(X_1, X_2, \\ldots\\) be i.i.d. random variables with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(0 &lt; \\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Then: \\[\\frac{\\sqrt{N}(\\bar{X}_N - \\mu)}{\\sigma} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } N \\to \\infty\\] where \\(\\bar{X}_N = \\frac{1}{N}\\sum_{i=1}^{N} X_i\\) and \\(\\xrightarrow{d}\\) denotes convergence in distribution.\n\n\nConvergence notation: \\(\\xrightarrow{P}\\) denotes convergence in probability, \\(\\xrightarrow{a.s.}\\) denotes almost sure convergence, and \\(\\xrightarrow{d}\\) denotes convergence in distribution.\n\nA.3.1 Monte Carlo Implications\nFor our estimator \\(\\hat{\\theta}_N = \\frac{1}{N}\\sum_{i=1}^{N} g(X_i)\\) where \\(\\theta = \\mathbb{E}[g(X)]\\):\n\nConsistency (from SLLN): \\(\\hat{\\theta}_N \\xrightarrow{a.s.} \\theta\\)\nAsymptotic normality (from CLT): \\(\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\\) where \\(\\sigma^2 = \\text{Var}(g(X))\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/probability.html#confidence-intervals-and-convergence-analysis",
    "href": "appendices/probability.html#confidence-intervals-and-convergence-analysis",
    "title": "Appendix A — Review of Probability Theory",
    "section": "A.4 Confidence Intervals and Convergence Analysis",
    "text": "A.4 Confidence Intervals and Convergence Analysis\nLet \\(S_N^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(g(X_i) - \\hat{\\theta}_N)^2\\) be the sample variance. By the CLT: \\[\\frac{\\hat{\\theta}_N - \\theta}{S_N/\\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1)\\]\nAn asymptotic \\((1-\\alpha)\\)-level confidence interval is: \\[\\left[ \\hat{\\theta}_N - z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}, \\quad \\hat{\\theta}_N + z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}} \\right]\\] where \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\) and \\(\\Phi\\) is the standard normal CDF.\nThe absolute width is \\(2z_{1-\\alpha/2} \\frac{S_N}{\\sqrt{N}}\\) and the relative width is \\(\\frac{2z_{1-\\alpha/2} S_N}{|\\hat{\\theta}_N|\\sqrt{N}}\\).\n\n\n\n\n\n\nRemark (Monte Carlo Convergence Properties)\n\n\n\nThe sample mean estimator has standard error \\(\\text{SE}(\\hat{\\theta}_N) = \\sigma/\\sqrt{N} = O(N^{-1/2})\\), leading to several key properties:\n\nSquare Root Law: To halve the confidence interval width requires four times as many samples\nDimension Independence: The \\(O(N^{-1/2})\\) convergence rate is independent of problem dimension for independent samples\nMCMC Caveat: When using MCMC, dependence between samples can effectively reduce the number of independent observations, particularly in high dimensions\n\n\n\n\n\n\n\n\n\nConditions for Limit Theorems\n\n\n\nBoth the Law of Large Numbers and Central Limit Theorem require:\n\nIndependence: Samples must be independent (or satisfy weaker mixing conditions)\nIdentical distribution: Samples from the same distribution\n\nFinite moments: Finite mean for LLN, finite variance for CLT\n\nWhen using MCMC, the independence assumption is violated, requiring analysis of effective sample size.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Review of Probability Theory</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html",
    "href": "appendices/markov_chains.html",
    "title": "Appendix B — Markov Chains",
    "section": "",
    "text": "B.1 Definitions\nThis appendix provides the theoretical foundation for MCMC methods covered in the main course. We will learn about Markov chains and focus on discrete chains for simplicity, but all results extend to the continuous case used in practice.\nA Markov chain is a discrete-time stochastic process that satisfies the Markov property. The Markov property states that the future state of the process depends only on the current state and not on the sequence of events that preceded it. In other words, the future is conditionally independent of the past given the present.\nThe transition matrix of a Markov chain is a square matrix whose \\((i, j)\\)-th entry is the probability of transitioning from state \\(i\\) to state \\(j\\) in one time step.\nThroughout this notebook, we’ll let \\(X_0, X_1, X_2, \\ldots\\) be a Markov chain with state space \\(\\Omega = \\{1, 2, \\ldots, N\\}\\) and transition matrix \\(P\\).\nThe probability distribution of the Markov chain at time \\(n\\) is a row vector \\(\\pi_n\\) whose \\(i\\)-th entry is \\(\\mathbb{P}(X_n = i)\\) for each \\(i \\in \\Omega\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#definitions",
    "href": "appendices/markov_chains.html#definitions",
    "title": "Appendix B — Markov Chains",
    "section": "",
    "text": "Definition B.1 Markov Chain: A Markov chain is a sequence of random variables \\(X_0, X_1, X_2, \\ldots\\) satisfying the following properties:\n\nState Space: The random variables \\(X_i\\) take values in a finite set \\(\\Omega\\) called the state space.\nMarkov Property: For all \\(n \\geq 0\\) and all states \\(i_0, i_1, \\ldots, i_{n+1} \\in \\Omega\\), \\[P(X_{n+1} = i_{n+1} \\mid X_0 = i_0, X_1 = i_1, \\ldots, X_n = i_n) = P(X_{n+1} = i_{n+1} \\mid X_n = i_n).\\]\nTime Homogeneity: The transition probabilities \\(P(X_{n+1} = j \\mid X_n = i)\\) do not depend on \\(n\\).\n\n\n\n\nDefinition B.2 Transition Matrix: Let \\(X_1, X_2, X_3, \\ldots\\) be a Markov chain with state space \\(\\Omega = \\{1, 2, \\ldots, N\\}\\). The transition matrix \\(P\\) of the Markov chain is an \\(N \\times N\\) matrix whose \\((i, j)\\)-th entry is given by \\[P(i, j) = P(X_{n+1} = j \\mid X_n = i).\\]\n\n\n\n\n\n\n\n\nState Diagrams\n\n\n\nWe can represent a Markov chain by a directed graph called a state diagram. Each state is represented by a node, and the transition probabilities are represented by directed edges between the nodes. The transition matrix can be derived from the state diagram by assigning the transition probabilities to the corresponding entries of the matrix.\n\n\n\n\nTheorem B.1 Let \\(\\pi_n\\) be the probability distribution of the chain at time \\(n\\). Then, \\[\\pi_{n+1} = \\pi_n P.\\] And hence, \\[\\pi_n = \\pi_0 P^n.\\]\n\n\nProof. \\[\\begin{align*}\n\\pi_{n+1}(j) &= \\mathbb{P}(X_{n+1} = j) \\\\\n&= \\sum_{i \\in \\Omega} \\mathbb{P}(X_{n+1} = j \\mid X_n = i) \\mathbb{P}(X_n = i) \\\\\n&= \\sum_{i \\in \\Omega} \\pi_n(i) P(i, j) \\\\\n&= (\\pi_n P)(j).\n\\end{align*}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#stationary-distribution",
    "href": "appendices/markov_chains.html#stationary-distribution",
    "title": "Appendix B — Markov Chains",
    "section": "B.2 Stationary Distribution",
    "text": "B.2 Stationary Distribution\n\nDefinition B.3 A probability distribution \\(\\pi\\) is called a stationary distribution of a Markov chain with transition matrix \\(P\\) if \\(\\pi = \\pi P\\).\n\nThe transition matrix \\(P\\) is guaranteed to have an eigenvalue of 1 because its row sum is 1, i.e., \\[P \\vec{1} = \\vec{1},\\] where \\(\\vec{1}\\) is the vector of all ones. Since there is a right eigenvector corresponding to the eigenvalue 1, there will be a left eigenvector as well. The left eigenvector is a stationary distribution of the Markov chain.\nIt is not hard to see that every eigenvalue of \\(P\\) is less than or equal to 1 in magnitude. Suppose \\(\\vec{v}\\) is a left eigenvector of \\(P\\) corresponding to an eigenvalue \\(\\lambda\\). Let \\(v_I\\) be the largest component of \\(\\vec{v}\\) in magnitude. Then, we have \\[\\begin{align*}\n\\lambda \\vec{v} &= \\vec{v} P \\\\\n\\implies\n\\lambda v_I &= \\sum_{j} v_j P(j, I) \\\\\n&\\leq \\sum_{j} |v_j| P(j, I) \\\\\n&\\leq \\sum_{j} |v_I| P(j, I) \\\\\n&= |v_I|.\n\\end{align*}\\]\nThus, \\(|\\lambda| \\leq 1\\). In particular, this means that the spectral radius of \\(P\\) is less than or equal to 1, and for all vectors \\(\\vec{v}\\), we have \\[\\| \\vec{v} \\|_2 \\geq \\| \\vec{v} P \\|_2.\\]\nWe are particularly interested in the case when there is exactly one eigenvector with eigenvalue of magnitude 1, which would then correspond to the stationary distribution of the Markov chain.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#fundamental-theorem",
    "href": "appendices/markov_chains.html#fundamental-theorem",
    "title": "Appendix B — Markov Chains",
    "section": "B.3 Fundamental Theorem",
    "text": "B.3 Fundamental Theorem\n\nDefinition B.4 We say that a Markov chain is irreducible if for every pair of states \\(i, j \\in \\Omega\\), there exists an integer \\(n\\) such that \\(P^n(i, j) &gt; 0\\), i.e., it is possible to go from any state to any other state in a finite number of steps.\n\n\nDefinition B.5 We say that a state \\(i\\) is aperiodic if the greatest common divisor of the set \\(\\{n \\geq 1 : P^n(i, i) &gt; 0\\}\\) is 1. A Markov chain is called aperiodic if all its states are aperiodic.\n\n\n\n\n\n\n\nNote on Positive Recurrence\n\n\n\nSometimes we add a preliminary requirement that the set \\(\\{n \\geq 1 : P^n(i, i) &gt; 0\\}\\) is non-empty. This condition is called positive recurrence. We’ll assume this as part of the definition of aperiodicity.\n\n\n\nDefinition B.6 A Markov chain is called ergodic if it is irreducible and aperiodic.\n\n\nTheorem B.2 Fundamental Theorem of Markov Chains: If a Markov chain is ergodic, then it has a unique stationary distribution \\(\\Pi\\). Moreover, in this case, for any initial distribution \\(\\pi_0\\), the distribution of the chain converges to \\(\\Pi\\) as \\(n \\to \\infty\\), i.e., \\[\\lim_{n \\to \\infty} \\pi_0 P^n = \\Pi\\] for any initial distribution \\(\\pi_0\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#random-walk-on-graphs",
    "href": "appendices/markov_chains.html#random-walk-on-graphs",
    "title": "Appendix B — Markov Chains",
    "section": "B.4 Random Walk on Graphs",
    "text": "B.4 Random Walk on Graphs\nOur main example of a Markov chain is the random walk on a graph. Let \\(G = (V, E)\\) be a graph with vertex set \\(V\\) and edge set \\(E\\). Suppose you want to move from one vertex to another by following the edges of the graph. At each vertex, you choose an edge uniformly at random and move to the adjacent vertex. This process is called a random walk on the graph.\nThe random walk on \\(G\\) is a Markov chain with state space \\(\\Omega = V\\) and transition probabilities given by \\[P(i, j) =\n\\begin{cases}\n\\frac{1}{\\deg(i)} & \\text{if } (i, j) \\in E, \\\\\n0 & \\text{otherwise},\n\\end{cases}\\] where \\(\\deg(i)\\) is the degree of vertex \\(i\\), i.e., the number of edges incident to \\(i\\).\n\n\n\n\n\n\nAlgorithm: Random Walk on a Graph\n\n\n\nGiven a graph \\(G = (V, E)\\) and starting vertex \\(v_0 \\in V\\):\n\nSet current vertex \\(v = v_0\\) and time \\(t = 0\\)\nWhile \\(t &lt; T\\) (for some stopping time \\(T\\)):\n\nLet \\(N(v) = \\{u \\in V : (v, u) \\in E\\}\\) be the neighbors of \\(v\\)\nChoose next vertex \\(u\\) uniformly at random from \\(N(v)\\)\nSet \\(v = u\\) and \\(t = t + 1\\)\nRecord the current vertex \\(v\\)\n\n\n\n\nIn HW, you’ll prove the following theorem about ergodicity of random walks on graphs.\n\nTheorem B.3 A random walk on a graph is\n\nIrreducible if and only if the graph is connected.\nAperiodic if and only if the graph is not bipartite.\n\nIf these conditions hold, then the stationary distribution of the random walk is given by \\[\\Pi(i) = \\frac{\\deg(i)}{2|E|},\\] where \\(\\deg(i)\\) is the degree of vertex \\(i\\) and \\(|E|\\) is the number of edges in the graph.\n\n\nExample B.1 Consider a graph \\(G\\) with 4 vertices as shown below. The transition matrix of the random walk on \\(G\\) is given by \\[P =\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1/3 & 0 & 1/3 & 1/3 \\\\\n0 & 1/2 & 0 & 1/2 \\\\\n0 & 1/2 & 1/2 & 0\n\\end{pmatrix}.\\]\nSuppose we start at vertex \\(A\\). This means that the initial distribution is \\(\\pi_0 = [1, 0, 0, 0]\\). The \\(n\\)-th distribution \\(\\pi_n\\) can be obtained by multiplying \\(\\pi_0\\) with the transition matrix \\(P^n\\).\n\n\n\n\n\n\n\n\n\n\nTransition matrix of the Markov chain:\n[[0.   1.   0.   0.  ]\n [0.33 0.   0.33 0.33]\n [0.   0.5  0.   0.5 ]\n [0.   0.5  0.5  0.  ]]\n\nEvolution of the Markov chain starting from state A:\n\n        A      B      C      D\n0   1.000  0.000  0.000  0.000\n1   0.000  1.000  0.000  0.000\n2   0.333  0.000  0.333  0.333\n3   0.000  0.667  0.167  0.167\n4   0.222  0.167  0.306  0.306\n5   0.056  0.528  0.208  0.208\n6   0.176  0.264  0.280  0.280\n7   0.088  0.456  0.228  0.228\n8   0.152  0.316  0.266  0.266\n9   0.105  0.418  0.238  0.238\n10  0.139  0.344  0.259  0.259",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#mixing-time",
    "href": "appendices/markov_chains.html#mixing-time",
    "title": "Appendix B — Markov Chains",
    "section": "B.5 Mixing Time",
    "text": "B.5 Mixing Time\nAssume that the Markov chain is ergodic, and hence has a stationary distribution \\(\\Pi\\). We know that any initial distribution \\(\\pi_0\\) converges to \\(\\Pi\\) as \\(n \\to \\infty\\). The mixing time measures the rate of this convergence.\n\nDefinition B.7 The total variation distance between two probability distributions \\(\\mu\\) and \\(\\nu\\) on a finite state space \\(\\Omega\\) is defined as \\[\\| \\mu - \\nu \\|_{\\text{TV}}\n= \\frac{1}{2} \\sum_{i \\in \\Omega} |\\mu(i) - \\nu(i)|\n= \\frac{1}{2} \\| \\mu - \\nu \\|_{L^1}.\\]\n\nOne can show that \\[\\| \\mu - \\nu \\|_{\\text{TV}} = \\sup \\{ |\\mu(A) - \\nu(A)| : A \\subseteq \\Omega \\},\\] i.e., \\(\\|\\mu - \\nu \\|_{\\text{TV}}\\) is the maximum difference in the probability of any event under the two distributions.\nWe use the total variation distance to measure the distance between the distribution of the Markov chain at time \\(n\\) and the stationary distribution. The mixing time of the Markov chain is defined as the smallest \\(N\\) such that for the stationary distribution \\(\\Pi\\) and any initial distribution \\(\\pi_0\\), we have \\[\\| \\pi_0 P^n - \\Pi \\|_{\\text{TV}} \\leq \\frac{1}{4}\\] for all \\(n \\geq N\\). The constant \\(\\frac{1}{4}\\) is arbitrary and can be replaced by any other constant in \\((0, 1)\\). This will only change the value of the mixing time by a constant factor and not the order of magnitude.\n\n\n\n\n\n\nPractical Significance of Mixing Time\n\n\n\nWhen using Markov chains for sampling, we want the mixing time to be as small as possible. This ensures that the distribution of the chain is close to the stationary distribution after a small number of steps. We think of the time before the chain mixes as a transient phase—the chain has not yet reached equilibrium. This is a burn-in period where we discard the samples. The bigger the mixing time, the more the number of wasted samples in the burn-in phase.\n\n\nExample. Consider Example B.1 again. If we start at vertex \\(A\\), by step 4 we have already reached the distribution \\(\\pi_4 = [0.222, 0.167, 0.306, 0.306]\\). The stationary distribution is \\(\\Pi = [1/8, 4/8, 2/8, 2/8]\\). The total variation distance between \\(\\pi_4\\) and \\(\\Pi\\) is \\(\\| \\pi_4 - \\Pi \\|_{\\text{TV}} = 0.21\\). This is less than \\(1/4\\), and hence the mixing time is at most 4 for this initial distribution.\nThis only computes the mixing time for the initial distribution \\(\\pi_0 = [1, 0, 0, 0]\\). In general, we need to compute the mixing time for all possible initial distributions. The mixing time is the maximum of these mixing times over all initial distributions.\nIn practice, we either provide a theoretical bound on the mixing time or “visually” inspect the convergence of the chain to the stationary distribution. Running a simulation to compute the mixing time is computationally expensive and not commonly done.\n\n\n\n\n\n\n\n\n\n\nB.5.1 Connection to Spectral Theory\nContinuing the example from above, the matrix \\(I_4 - P\\) is called the normalized Laplacian matrix of the graph \\(G\\). \\[\\mathcal{L} = I_4 - P =\n\\begin{pmatrix}\n1 & -1 & 0 & 0 \\\\\n-1/3 & 1 & -1/3 & -1/3 \\\\\n0 & -1/2 & 1 & -1/2 \\\\\n0 & -1/2 & -1/2 & 1\n\\end{pmatrix}\\]\nOne can show that 0 is an eigenvalue of \\(\\mathcal{L}\\) and all eigenvalues are non-negative. The second smallest eigenvalue of \\(\\mathcal{L}\\) is called the spectral gap of the graph (which could be 0).\n\n\n\n\n\n\nSpectral Gap and Mixing Time\n\n\n\nSpectral graph theory, in particular Cheeger inequalities, prove that there is an inverse relationship between the spectral gap of the graph and the mixing time of the random walk on the graph. The smaller the spectral gap, the larger the mixing time. You’ll explore this connection in the homework.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#sampling-from-a-markov-chain",
    "href": "appendices/markov_chains.html#sampling-from-a-markov-chain",
    "title": "Appendix B — Markov Chains",
    "section": "B.6 Sampling from a Markov Chain",
    "text": "B.6 Sampling from a Markov Chain\nThe algorithm to generate sample paths of length \\(n\\) of a Markov chain is simple. Suppose \\(P\\) is the transition matrix of the Markov chain with mixing time \\(t\\).\n\n\n\n\n\n\nAlgorithm: Markov Chain Sampling\n\n\n\nGiven a transition matrix \\(P\\) and desired number of samples \\(N\\):\n\nStart at some initial state \\(X_0\\)\nFor \\(i = 0, 1, \\ldots, N-1\\):\n\nGenerate \\(X_{i+1} \\sim P(X_i, \\cdot)\\)\n\nDiscard the first \\(T\\) samples and return \\(X_{T+1}, X_{T+2}, \\ldots, X_N\\)\n\nWe can interpret this algorithm as generating \\(N-T\\) samples from the stationary distribution of the Markov chain. The number of samples discarded is called the burn-in period.\n\n\nOne big issue with this algorithm is that the samples are highly correlated. If independence is important, selecting every \\(k\\)-th sample may be beneficial. However, this leads to a lot of wasted samples and it might not get rid of all the correlation. In practice, it is better to generate a large number of samples than to “thin” the samples.\n\nExample B.2 In the example below we generate a uniform distribution over \\(\\{0, 1, \\ldots, 14\\}\\) by generating a random walk over a cycle of length 15.\nThe autocorrelation plot below shows the correlation between samples at different lags, i.e., \\(X_i\\) and \\(X_{i+k}\\) for different values of \\(k\\). We can see that the correlation decreases as \\(k\\) increases and stabilizes around \\(k = 40\\).\nWe can either discard the first 40 samples or select every 40th sample to reduce the correlation. If independence is important, then we should select every 40th sample. However, this leads to a lot of wasted samples and it might not get rid of all the correlation. This method is not preferred in practice. In practice, it is better to generate a large number of samples than to “thin” the samples.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#reversible-markov-chains",
    "href": "appendices/markov_chains.html#reversible-markov-chains",
    "title": "Appendix B — Markov Chains",
    "section": "B.7 Reversible Markov Chains",
    "text": "B.7 Reversible Markov Chains\nA Markov chain is reversible with respect to a distribution \\(\\pi\\) if the following holds: \\[\\pi_i P(i, j) = \\pi_j P(j, i) \\quad \\text{for all } i, j. \\tag{B.1}\\]\nThis is saying that the probability of transitioning from \\(x\\) to \\(y\\) is the same as the probability of transitioning from \\(y\\) to \\(x\\). Equation B.1 is known as the detailed balance equation.\n\nTheorem B.4 (Detailed Balance Equation). If a Markov chain is reversible with respect to a distribution \\(\\pi\\), then \\(\\pi\\) is a stationary distribution of the Markov chain.\n\n\nProof. Suppose the Markov chain is reversible with respect to \\(\\pi\\). Then, \\[\\begin{aligned}\n(\\pi P)_i &= \\sum_j \\pi_j P(j, i) \\\\\n&= \\sum_j \\pi_i P(i, j) \\\\\n&= \\pi_i \\sum_j P(i, j) \\\\\n&= \\pi_i.\n\\end{aligned}\\] Thus, \\(\\pi\\) is the stationary distribution of the Markov chain.\n\nEquation B.1 is a sufficient but not necessary condition for \\(\\pi\\) to be the stationary distribution of the Markov chain. You can have a Markov chain with a stationary distribution that is not reversible.\nNote that we did not use any properties of the Markov chain in the proof of the theorem, except that the row sum of the transition matrix is \\(1\\). A better way to phrase this theorem would be to say that “if the row sum of the transition matrix is \\(1\\) and the detailed balance equation holds, then \\(\\pi\\) is an eigenvector of the transition matrix with eigenvalue \\(1\\).”\n\nExample B.3 Random Walks on Graphs. Consider a graph \\(G = (V, E)\\) with vertices \\(V\\) and edges \\(E\\). Let \\(P(i, j) = 1/\\deg(i)\\) if \\((i, j) \\in E\\) and \\(0\\) otherwise, where \\(\\deg(i)\\) is the degree of vertex \\(i\\). Then, the stationary distribution of the Markov chain is \\(\\pi_i = \\deg(i)/(2|E|)\\), where \\(|E|\\) is the number of edges in the graph.\nThe Markov chain is reversible with respect to \\(\\pi\\). Consider two vertices \\(i\\) and \\(j\\). If \\((i, j) \\in E\\), then \\[\\begin{aligned}\n\\pi_i P(i, j) &= \\frac{\\deg(i)}{2|E|} \\cdot \\frac{1}{\\deg(i)} \\\\\n&= \\frac{1}{2|E|} \\\\\n&= \\frac{\\deg(j)}{2|E|} \\cdot \\frac{1}{\\deg(j)} \\\\\n&= \\pi_j P(j, i).\n\\end{aligned}\\] If \\((i, j) \\notin E\\), then \\(\\pi_i P(i, j) = \\pi_j P(j, i) = 0\\).\n\nMany Markov chains encountered in practice are reversible with respect to some distribution. It is much easier to check the detailed balance equation than to compute the stationary distribution directly. Moreover, reversible Markov chains can be analyzed using spectral methods and we can find good bounds on their mixing time.\n\nB.7.1 Reversibility and Symmetry\n\nTheorem B.5 If a Markov chain is reversible with respect to a distribution \\(\\pi\\), then the matrix \\[Q = \\text{diag}(\\sqrt{\\pi}) \\: P \\: \\text{diag}(\\sqrt{\\pi^{-1}})\\] is symmetric. In particular, \\(P\\) is similar to the symmetric matrix \\(Q\\) and hence has real eigenvalues.\n\n\nProof. This is because \\[\\begin{aligned}\nQ(i, j)\n&= \\sqrt{\\pi_i} P(i, j) \\sqrt{\\pi_j^{-1}} \\\\\n&= \\sqrt{\\pi_i} \\frac{\\pi_j P(j, i)}{\\pi_i} \\sqrt{\\pi_j^{-1}} \\\\\n&= \\sqrt{\\pi_j} P(j, i) \\sqrt{\\pi_i^{-1}} \\\\\n&= Q(j, i).\n\\end{aligned}\\] Thus, \\(Q\\) is symmetric.\n\nNow suppose \\(\\mathbf{v}\\) is a left eigenvector of \\(Q\\) with eigenvalue \\(\\lambda\\). Then, \\[\\begin{aligned}\n\\mathbf{v} Q &= \\lambda \\mathbf{v} \\\\\n\\mathbf{v} \\text{diag}(\\sqrt{\\pi}) \\: P \\: \\text{diag}(\\sqrt{\\pi^{-1}}) &= \\lambda \\mathbf{v} \\\\\n\\implies \\mathbf{v} \\text{diag}(\\sqrt{\\pi}) \\: P &= \\lambda \\mathbf{v} \\: \\text{diag}(\\sqrt{\\pi}).\n\\end{aligned}\\]\nHence, \\(\\mathbf{v} \\text{diag}(\\sqrt{\\pi})\\) will be an eigenvector of \\(P\\) with the same eigenvalue. As \\(Q\\) is symmetric, it has real eigenvalues and orthogonal eigenvectors. It is easier to do spectral analysis of \\(Q\\) and use that to deduce properties of \\(P\\). For example, we can find the eigenvector corresponding to the largest eigenvalue of \\(Q\\) by solving the optimization problem \\[\\mathbf{v} = \\underset{\\mathbf{x} \\neq 0}{\\text{arg max}} \\frac{\\mathbf{x}^T Q \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}.\\]\nMultiplying the above vector \\(\\mathbf{v}\\) by \\(\\text{diag}(\\sqrt{\\pi})\\) gives us the stationary distribution for \\(P\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#transition-kernels",
    "href": "appendices/markov_chains.html#transition-kernels",
    "title": "Appendix B — Markov Chains",
    "section": "B.8 Transition Kernels",
    "text": "B.8 Transition Kernels\nThe sample spaces we encounter in MCMC methods are not discrete but continuous. Instead of a transition matrix, we use a transition kernel \\(K(x, y)\\) that gives the “probability of transitioning from state \\(x\\) to state \\(y\\)”. However, because the sample space is continuous, the probability of being in a particular state is zero. Instead, we use the density of the distribution at that point. The transition kernel satisfies the equation: \\[\\mathbb{P}(X_{n+1} \\in A \\mid X_n = x) = \\int_{A} K(x, y) \\, dy.\\]\nAll the properties of Markov chains that we discussed earlier can be extended to transition kernels. The fundamental theorem of Markov chains becomes\n\nTheorem B.6 (Fundamental Theorem of Markov Chains for Kernels). If a Markov chain with transition kernel \\(K\\) is\n\nIrreducible: For all \\(x, y\\), there exists \\(n\\) such that \\(K^n(x, y) &gt; 0\\).\nPositive recurrent: The expected return time to a state is finite.\nAperiodic: For all \\(x\\), \\(\\gcd\\{n : K^n(x, x) &gt; 0\\} = 1\\).\n\nThen, the Markov chain has a unique stationary distribution \\(\\Pi\\) and for all initial distributions \\(\\pi_0\\), \\[\\int \\pi_0(x) K^n(x, y) \\, dx \\xrightarrow[TV]{} \\Pi(y) \\quad \\text{as } n \\to \\infty.\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#analyzing-convergence-of-markov-chains",
    "href": "appendices/markov_chains.html#analyzing-convergence-of-markov-chains",
    "title": "Appendix B — Markov Chains",
    "section": "B.9 Analyzing Convergence of Markov Chains",
    "text": "B.9 Analyzing Convergence of Markov Chains\nIn practice we need to decide how many samples to burn and how many samples to generate. We use various heuristics to decide this.\n\n\n\n\n\n\nAlgorithm: Trace Plot Analysis\n\n\n\nTo assess convergence of a Markov chain:\n\nGenerate a long chain of samples \\(X_1, X_2, \\ldots, X_N\\)\nPlot the sequence of samples against time\nLook for:\n\nStabilization around a central value\nAbsence of trends or drifts\nConsistent variability across the chain\n\nIf the chain appears to have converged, determine burn-in period\n\n\n\n\nB.9.1 Trace Plots\nA trace plot is simply a scatter plot of the samples generated by the Markov chain. It is useful to see if the Markov chain has converged. If the Markov chain has converged, the trace plot should look like a cloud of points. If the Markov chain has not converged, the trace plot will show a trend.\nBelow are the trace plots for Example B.2. We can see that the chain does not look uniform even after 1000 samples but after 5000 samples it is starting to look uniform.\n\n\n\n\n\n\n\n\n\n\n\nB.9.2 Running Average\nThe running average is the average of the first \\(n\\) samples. It is useful to see if the Markov chain has converged. If the Markov chain has converged, the running average should stabilize around the true mean. If the Markov chain has not converged, the running average will show a trend.\nBelow is the running average plot for Example B.2. We can see that the running average is stabilizing around the true mean around 3000 samples.\n\n\n\n\n\n\nLimitations of Running Averages\n\n\n\nRunning averages can be deceptive and show stability even when the Markov chain has not converged. It is better to use multiple diagnostics to check for convergence. They are better at telling when the Markov chain has not converged than when it has converged.\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.9.3 Autocorrelation Function\nOne method for finding the burn-in period is to use the autocorrelation function. The autocorrelation function of a sequence of numbers \\(x = (x_0, x_1, \\ldots, x_n)\\) at lag \\(k\\) is defined as \\[\\text{ACF}(k) = \\text{Corr}(x[k:], x[:-k])\\] where by \\(x[k:]\\) we mean the subsequence \\(x_k, x_{k+1}, \\ldots, x_n\\) and by \\(x[:-k]\\) we mean the subsequence \\(x_0, x_1, \\ldots, x_{n-k}\\). It is the correlation between the sequence \\(x\\) and the same sequence shifted by \\(k\\).\n\n\n\n\n\n\nAlgorithm: Choosing Burn-in Period Using ACF\n\n\n\nTo determine the burn-in period using the autocorrelation function:\n\nCompute the autocorrelation function \\(\\text{ACF}(k)\\) for various lags \\(k\\)\nChoose a threshold \\(\\epsilon\\) (e.g., \\(\\epsilon = 0.1\\) or \\(\\epsilon = 0.05\\))\nFind the smallest \\(T\\) such that \\(\\text{ACF}(T) &lt; \\epsilon\\)\nTo be conservative, choose a burn-in period of \\(2T\\) or \\(3T\\)\n\n\n\nBelow is the autocorrelation plot for Example B.2. We can see that the correlation decreases as \\(k\\) increases and drops below \\(0.1\\) around \\(17\\). So a burn-in period of \\(40\\) is a good conservative choice.\nThis is just one way to choose the burn-in period. There are many other methods and the choice depends on the application. We will stick to this method for the rest of the course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "appendices/hidden_markov.html",
    "href": "appendices/hidden_markov.html",
    "title": "Appendix C — Hidden Markov Chains",
    "section": "",
    "text": "D Particle Filter with Resampling\n\nInputs:\n\nObservations \\(\\{y_1, y_2, \\dots, y_T\\}\\)\n\nNumber of particles \\(N\\)\n\nTransition model \\(p(x_t \\mid x_{t-1})\\)\n\nEmission model \\(p(y_t \\mid x_t)\\)\n\nInitial distribution \\(p(x_1)\\)\n\nInitialization: For \\(i = 1\\) to \\(N\\)\n\nSet \\(x_0^{(i)} \\leftarrow x_0\\)\n\nSet weight \\(w_1^{(i)} \\leftarrow 1/N\\)\n\nFor \\(t = 1\\) to \\(T\\)\n\nFor \\(i = 1\\) to \\(N\\)\n\nSample \\(x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)})\\)\n\nCompute weight \\(w_t^{(i)} \\leftarrow w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})\\)\n\n\nNormalize weights \\(w_t^{(i)} \\leftarrow \\frac{w_t^{(i)}}{\\sum_{j=1}^N w_t^{(j)}}\\)\n\nReturn: Particles \\(\\{x_t^{(i)}\\}\\) and weights \\(w_t^{(i)}\\) for all \\(t\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hidden Markov Chains</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#mathematical-foundations",
    "href": "chapters/sampling/accept_reject.html#mathematical-foundations",
    "title": "9  Rejection Sampling",
    "section": "",
    "text": "Continuous Case\n\n\n\nWhen \\(X\\) and \\(Y\\) are continuous random variables, the definitions are similar, but we replace the probability mass functions with probability density functions and sums with integrals.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#theoretical-foundation",
    "href": "chapters/sampling/accept_reject.html#theoretical-foundation",
    "title": "9  Rejection Sampling",
    "section": "9.2 Theoretical Foundation",
    "text": "9.2 Theoretical Foundation\nThe accept-reject method is based on the following fundamental theorem:\n\nTheorem 9.1 (Marginal of Uniform Distribution) Let \\(p(x)\\) be a probability distribution. Let \\(X, Y\\) be two random variables having the joint distribution\n\\[f_{X,Y}(x,y) = \\begin{cases}\n1, & \\text{if } 0 \\leq y \\leq p(x), \\\\\n0, & \\text{otherwise}.\n\\end{cases} \\tag{9.4}\\]\nThen the marginal distribution of \\(X\\) is given by \\(p(x)\\).\n\n\nProof. The marginal distribution of \\(X\\) is given by\n\\[f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy = \\int_{0}^{p(x)} 1 \\, dy = p(x)\\]\n\\(\\blacksquare\\)\n\n\n\n\n\n\n\nGeometric Interpretation\n\n\n\nThis theorem tells us that if we sample uniformly from the region under the curve \\(y = p(x)\\), the \\(x\\)-coordinates of these samples will follow the distribution \\(p(x)\\). This is the key insight behind the accept-reject method.\n\n\nThus, in order to sample from a distribution \\(p(x)\\), we want to devise a method to sample from the joint distribution given by Equation 9.4. The accept-reject method provides exactly such a technique.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#accept-reject-method-basic-algorithm",
    "href": "chapters/sampling/accept_reject.html#accept-reject-method-basic-algorithm",
    "title": "9  Rejection Sampling",
    "section": "9.3 Accept-Reject Method: Basic Algorithm",
    "text": "9.3 Accept-Reject Method: Basic Algorithm\nSuppose \\(p(x)\\) is a probability distribution from which we want to sample. Suppose further that \\(p(x)\\) is supported over the interval \\([a, b]\\) and is bounded by \\(M\\), i.e., \\(p(x) \\leq M\\) for all \\(x \\in [a, b]\\).\n\n\n\n\n\n\nAlgorithm: Basic Accept-Reject Method\n\n\n\nThe simplest version of the accept-reject method proceeds as follows:\n\nGenerate proposal: Sample \\(x\\) uniformly from \\([a, b]\\).\nGenerate test value: Sample \\(y\\) uniformly from \\([0, M]\\).\n\nAccept or reject: If \\(y \\leq p(x)\\), accept and return \\(x\\); otherwise, reject and go back to step 1.\n\n\n\n\n\n\n\n\n\nEfficiency Considerations\n\n\n\nThe acceptance probability for this basic method is \\(\\frac{\\int_a^b p(x) dx}{M(b-a)} = \\frac{1}{M(b-a)}\\) (since \\(p(x)\\) is a probability density). For efficiency, we want \\(M(b-a)\\) to be as small as possible, which means finding a tight upper bound \\(M\\).\n\n\n\n9.3.1 Worked Example\n\nExample 9.1 (Example: Triangular Distribution) Consider the triangular distribution\n\\[p(x) = \\begin{cases}\n4x, & \\text{if } 0 \\leq x \\leq 0.5, \\\\\n4(1-x), & \\text{if } 0.5 \\leq x \\leq 1 \\\\\n0, & \\text{otherwise}.\n\\end{cases} \\tag{9.5}\\]\nAnalysis:\n\nSupport: \\([a,b] = [0, 1]\\).\n\nMaximum value: \\(M = \\max_{x \\in [0,1]} p(x) = p(0.5) = 2\\).\nAcceptance probability: \\(\\frac{1}{M(b-a)} = \\frac{1}{2 \\cdot 1} = 0.5\\).\n\nAlgorithm application:\n\nSample \\(x \\sim \\text{Uniform}(0, 1)\\).\nSample \\(y \\sim \\text{Uniform}(0, 2)\\).\n\nIf \\(y \\leq p(x)\\), accept \\(x\\); otherwise reject and repeat.\n\nOn average, we expect to reject half of our proposals.\n\n\n\n\n\n\n\nImplementation Note\n\n\n\nIn practice, you would implement this algorithm in a loop until you obtain the desired number of samples. The triangular distribution is actually simple enough that direct methods exist, but it serves as a good example for understanding the accept-reject principle.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3.2 Efficiency\nNote that unlike the methods we have seen so far, the accept-reject method is probabilistic. The method generates uniformly distributed samples in the rectangle of area \\(M(b-a)\\), where \\(M\\) is the bound on \\(p(x)\\) and \\([a, b]\\) is the support of \\(p(x)\\). But because \\(p(x)\\) is a probability distribution, the area under the curve is 1. Thus the efficiency of the accept-reject method is given by\n\\[\\text{Efficiency} = \\frac{1}{M(b-a)} \\tag{9.6}\\]\n\n\n\n\n\n\nEfficiency Trade-offs\n\n\n\nIf \\(M\\) is large (i.e., the probability distribution has a large peak), then the efficiency of the accept-reject method is low. This means:\n\nHigh peaks: Lead to many rejected samples and slower convergence.\nTight bounds: Finding the smallest possible \\(M\\) is crucial for good performance.\nWide support: Large intervals \\([a,b]\\) also reduce efficiency.\n\n\n\n\n\n\n\n\n\nGeometric Interpretation of Efficiency\n\n\n\nThe efficiency represents the fraction of the bounding rectangle that lies under the target density curve. In our triangular distribution example:\n\nRectangle area: \\(M(b-a) = 2 \\times 1 = 2\\)\nArea under curve: \\(\\int_0^1 p(x) dx = 1\\)\nEfficiency: \\(\\frac{1}{2} = 50\\%\\)\n\nThis means we accept exactly half of our proposals on average.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/accept_reject.html#complete-algorithm-with-proposal-distribution",
    "href": "chapters/sampling/accept_reject.html#complete-algorithm-with-proposal-distribution",
    "title": "9  Rejection Sampling",
    "section": "9.5 Complete Algorithm with Proposal Distribution",
    "text": "9.5 Complete Algorithm with Proposal Distribution\nThis gives us a way to sample from the majorizing function \\(g(x)\\). We can then use the accept-reject method to sample from the target distribution \\(p(x)\\).\n\n\n\n\n\n\nAlgorithm: Accept-Reject with Majorizing Function (Version 1)\n\n\n\n\nSample \\(x\\) from \\(g(x)\\).\nSample \\(y\\) uniformly from \\([0, Mg(x)]\\).\nIf \\(y \\leq p(x)\\), return \\(x\\); otherwise, go back to step 1.\n\n\n\n\n9.5.1 Equivalent Formulations\nSteps 2 and 3 are often rewritten as:\n\n\n\n\n\n\nAlgorithm: Accept-Reject with Majorizing Function (Version 2)\n\n\n\n\nSample \\(x\\) from \\(g(x)\\).\nSample \\(u\\) uniformly from \\([0, 1]\\).\nIf \\(u \\leq \\frac{p(x)}{Mg(x)}\\), return \\(x\\); otherwise, go back to step 1.\n\n\n\n\n\n\n\n\n\nNumerical Stability Considerations\n\n\n\nTaking a ratio of two small numbers can lead to numerical instability. We can avoid this by taking the logarithm of the ratio. The third step of the algorithm becomes:\nStep 3 (log version): If \\(\\log(u) \\leq \\log(p(x)) - \\log(Mg(x))\\), return \\(x\\); otherwise, go back to step 1.\nThis is particularly important when dealing with distributions that have very small density values.\n\n\n\n\n9.5.2 Complete Worked Example\n\nExample 9.3 (Example: Complete Beta Proposal Implementation) Consider the triangular distribution given by Equation 9.5. We saw that the Beta distribution \\(\\text{Beta}(2, 2)\\) majorizes the triangular distribution with \\(M = 4/3\\).\nStep-by-step implementation:\n\nSample from proposal: Generate \\(x \\sim \\text{Beta}(2, 2)\\) with density \\(g(x) = 6x(1-x)\\).\nGenerate uniform test: Sample \\(u \\sim U(0, 1)\\).\nAcceptance test: Compute the ratio \\[\\frac{p(x)}{Mg(x)} = \\frac{p(x)}{\\frac{4}{3} \\cdot 6x(1-x)} = \\frac{3p(x)}{8x(1-x)}\\]\nIf \\(u \\leq \\frac{3p(x)}{8x(1-x)}\\), accept \\(x\\); otherwise reject and repeat.\n\nEfficiency improvement:\n\nRectangle method: Efficiency = \\(50\\%\\)\nBeta proposal method: Efficiency = \\(75\\%\\)\n\nThis represents a 50% reduction in the expected number of iterations!\n\n\n\n\n\n\n\nChoosing Good Proposal Distributions\n\n\n\nThe best proposal distributions:\n\nHave a similar shape to the target distribution.\nAre easy to sample from (e.g., normal, beta, gamma distributions).\nAllow for easy evaluation of the density ratio \\(\\frac{p(x)}{g(x)}\\).\nMinimize the scaling constant \\(M\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Rejection Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#from-ordinary-to-stochastic-differential-equations",
    "href": "chapters/applications/SDE.html#from-ordinary-to-stochastic-differential-equations",
    "title": "13  Stochastic Differential Equations",
    "section": "",
    "text": "We’ll replace the function \\(y(t)\\) with a stochastic process \\(X(t)\\).\nWe’ll model the noise as a Wiener process \\(W(t)\\).\n\n\n\n\n\n\n\nApplications of Stochastic Differential Equations\n\n\n\nStochastic differential equations arise naturally in many applications including financial modeling (stock prices with random market fluctuations), physics (particle motion with thermal noise), biology (population dynamics with environmental randomness), and engineering (control systems with measurement noise).",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  },
  {
    "objectID": "appendices/markov_chains.html#concluding-remarks",
    "href": "appendices/markov_chains.html#concluding-remarks",
    "title": "Appendix B — Markov Chains",
    "section": "B.10 Concluding Remarks",
    "text": "B.10 Concluding Remarks\nIn this module, we have established the foundational theory of Markov chains that underpins modern Monte Carlo methods. The key insights we have developed include:\nTheoretical Foundations: We have seen how the Markov property leads to a rich mathematical structure, where the long-term behavior of the chain is determined by the spectral properties of the transition matrix. The fundamental theorem guarantees convergence to a unique stationary distribution under ergodicity conditions, providing the theoretical justification for using Markov chains as sampling algorithms.\nPractical Considerations: The gap between theory and practice is bridged by understanding mixing times and convergence diagnostics. While theoretical convergence is guaranteed, practical implementation requires careful attention to burn-in periods, autocorrelation, and the trade-offs between computational efficiency and sample quality.\nSpectral Connections: The connection between reversibility and spectral theory provides powerful tools for analysis. Reversible chains, which satisfy detailed balance, can be analyzed using symmetric matrix theory, leading to better bounds on mixing times and deeper understanding of convergence rates.\nLimitations and Challenges: Despite their theoretical elegance, Markov chains face practical challenges. High-dimensional problems often suffer from slow mixing, and convergence diagnostics can be misleading. The autocorrelation function and trace plots provide useful heuristics, but they cannot guarantee convergence—they are better at detecting non-convergence than confirming convergence.\n\n\n\n\n\n\nLooking Ahead\n\n\n\nThe theory developed here forms the backbone for advanced MCMC algorithms such as Metropolis-Hastings, Gibbs sampling, and Hamiltonian Monte Carlo. Each of these methods constructs specific transition kernels designed to have desired stationary distributions while maintaining good mixing properties. Understanding the fundamental principles of Markov chain theory is essential for both implementing these algorithms correctly and diagnosing their performance in practice.\n\n\nThe interplay between theory and computation in Markov chain Monte Carlo exemplifies the power of probability theory in solving practical problems. While we must always be mindful of the assumptions underlying our theoretical guarantees, the robustness of these methods across diverse applications demonstrates the value of this mathematical framework.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Markov Chains</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#joint-exponential-distribution",
    "href": "chapters/sampling/gibbs_2d.html#joint-exponential-distribution",
    "title": "10  Gibbs Sampling",
    "section": "11.1 Joint Exponential Distribution",
    "text": "11.1 Joint Exponential Distribution\nLet \\(X\\) and \\(Y\\) be two random variables with the truncated exponential distribution:\n\\[\nf_{X,Y}(x, y) = c e^{-\\lambda xy} \\quad \\text{for } 0 \\leq x \\leq D_1, \\, 0 \\leq y \\leq D_2,\n\\]\nwhere \\(c\\) is the normalization constant. We want to sample from the joint distribution \\(f_{X,Y}\\).\nOne can show that the conditionals are:\n\\[\n\\begin{aligned}\nf_{X|Y}(x \\mid y_0) &= c_{y_0} e^{-\\lambda x y_0} \\quad \\text{for } 0 \\leq x \\leq D_1 \\\\\nf_{Y|X}(y \\mid x_0) &= c_{x_0} e^{-\\lambda y x_0} \\quad \\text{for } 0 \\leq y \\leq D_2,\n\\end{aligned}\n\\]\nwhere \\(c_{y_0}\\) and \\(c_{x_0}\\) are the normalization constants.\n\n\n\n\n\n\nKey Insight\n\n\n\nNotice that each conditional distribution is simply a truncated exponential distribution with rate parameter that depends on the conditioning variable. This makes sampling straightforward using the inverse transform method.\n\n\nWe can easily sample from the two conditionals \\(f_{X|Y}\\) and \\(f_{Y|X}\\) using the inverse transform method (even for truncated distributions).\n\n\n\n\n\n\nGibbs Algorithm for Joint Exponential Distribution\n\n\n\n\nStart with some initial values \\(X_0\\) and \\(Y_0\\).\nFor \\(i = 1, 2, \\ldots, N\\):\n\nSample \\(X_i \\sim \\text{Exp}(\\lambda Y_{i-1})\\) restricted to \\([0, D_1]\\) using the inverse transform method.\nSample \\(Y_i \\sim \\text{Exp}(\\lambda X_i)\\) restricted to \\([0, D_2]\\) using the inverse transform method.\n\nReturn the sequence \\((X_0, Y_0), (X_1, Y_1), \\ldots, (X_N, Y_N)\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#bivariate-normal-distribution",
    "href": "chapters/sampling/gibbs_2d.html#bivariate-normal-distribution",
    "title": "10  Gibbs Sampling",
    "section": "11.2 Bivariate Normal Distribution",
    "text": "11.2 Bivariate Normal Distribution\nSuppose \\((X, Y)\\) has a bivariate normal distribution with mean \\((0, 0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\). We want to sample from the joint distribution \\(f_{X, Y}\\).\nOne can show that the conditional distributions are:\n\\[\n\\begin{aligned}\n(X \\mid Y = y_0) &\\sim \\mathcal{N}(\\rho y_0, 1 - \\rho^2) \\\\\n(Y \\mid X = x_0) &\\sim \\mathcal{N}(\\rho x_0, 1 - \\rho^2).\n\\end{aligned}\n\\]\n\n\n\n\n\n\nKey Property\n\n\n\nNotice that both conditional distributions have the same variance \\(1 - \\rho^2\\), which decreases as the correlation \\(|\\rho|\\) increases. When \\(\\rho = 0\\) (independence), the conditional variance equals the marginal variance.\n\n\nWe can sample from the two conditionals \\(f_{X|Y}\\) and \\(f_{Y|X}\\) using standard normal sampling methods such as the Box-Muller transform.\n\n\n\n\n\n\nGibbs Algorithm for Bivariate Normal Distribution\n\n\n\n\nStart with some initial values \\(X_0\\) and \\(Y_0\\).\nFor \\(i = 1, 2, \\ldots, N\\):\n\nSample \\(X_i \\sim \\mathcal{N}(\\rho Y_{i-1}, 1 - \\rho^2)\\).\nSample \\(Y_i \\sim \\mathcal{N}(\\rho X_i, 1 - \\rho^2)\\).\n\nReturn the sequence \\((X_0, Y_0), (X_1, Y_1), \\ldots, (X_N, Y_N)\\).",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#sec-gibbs-proof",
    "href": "chapters/sampling/gibbs_2d.html#sec-gibbs-proof",
    "title": "10  Gibbs Sampling",
    "section": "11.4 Proof of Stationarity",
    "text": "11.4 Proof of Stationarity\nWe now provide a proof that the joint distribution \\(f_{X,Y}\\) is indeed a stationary distribution of the Markov Chain generated by the Gibbs sampler.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#proof-of-theorem-thm-gibbs-markov-chain",
    "href": "chapters/sampling/gibbs_2d.html#proof-of-theorem-thm-gibbs-markov-chain",
    "title": "10  Gibbs Sampling",
    "section": "11.5 Proof of Theorem Theorem 11.1",
    "text": "11.5 Proof of Theorem Theorem 11.1\nTo show that \\(f_{X,Y}\\) is a stationary distribution, we need to verify that if \\((X_i, Y_i) \\sim f_{X,Y}\\), then \\((X_{i+1}, Y_{i+1}) \\sim f_{X,Y}\\) as well.\nLet \\((X_i, Y_i)\\) be distributed according to \\(f_{X,Y}\\). We want to show that \\((X_{i+1}, Y_{i+1})\\) obtained through one step of the Gibbs sampler also follows \\(f_{X,Y}\\).\nThe Gibbs sampler performs the following operations: 1. Sample \\(X_{i+1} \\sim f_{X|Y}(\\cdot \\mid Y_i)\\) 2. Sample \\(Y_{i+1} \\sim f_{Y|X}(\\cdot \\mid X_{i+1})\\)\nWe need to compute the joint density of \\((X_{i+1}, Y_{i+1})\\). Using the law of total probability and the conditional sampling steps:\n\\[\n\\begin{aligned}\nf_{X_{i+1}, Y_{i+1}}(x', y') &= \\int_{\\Omega_Y} f_{X_{i+1}, Y_{i+1} \\mid Y_i}(x', y' \\mid y) f_{Y_i}(y) \\, dy\n\\end{aligned}\n\\]\nGiven \\(Y_i = y\\), the Gibbs steps give us: - \\(X_{i+1} \\mid Y_i = y \\sim f_{X|Y}(\\cdot \\mid y)\\) - \\(Y_{i+1} \\mid X_{i+1} = x', Y_i = y \\sim f_{Y|X}(\\cdot \\mid x')\\)\nNote that \\(Y_{i+1}\\) depends only on \\(X_{i+1}\\), not directly on \\(Y_i\\). Therefore:\n\\[\nf_{X_{i+1}, Y_{i+1} \\mid Y_i}(x', y' \\mid y) = f_{X|Y}(x' \\mid y) f_{Y|X}(y' \\mid x')\n\\]\nSubstituting back:\n\\[\n\\begin{aligned}\nf_{X_{i+1}, Y_{i+1}}(x', y') &= \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_{Y|X}(y' \\mid x') f_{Y}(y) \\, dy \\\\\n&= f_{Y|X}(y' \\mid x') \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_{Y}(y) \\, dy\n\\end{aligned}\n\\]\nUsing the definition of marginal density, we have: \\[\nf_X(x') = \\int_{\\Omega_Y} f_{X,Y}(x', y) \\, dy = \\int_{\\Omega_Y} f_{X|Y}(x' \\mid y) f_Y(y) \\, dy\n\\]\nTherefore: \\[\nf_{X_{i+1}, Y_{i+1}}(x', y') = f_{Y|X}(y' \\mid x') f_X(x') = f_{X,Y}(x', y')\n\\]\nwhere the last equality follows from the fundamental relationship \\(f_{X,Y}(x,y) = f_{Y|X}(y \\mid x) f_X(x)\\).\nThus, \\((X_{i+1}, Y_{i+1}) \\sim f_{X,Y}\\), proving that \\(f_{X,Y}\\) is a stationary distribution of the Gibbs Markov Chain.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/sampling/gibbs_2d.html#concluding-remarks",
    "href": "chapters/sampling/gibbs_2d.html#concluding-remarks",
    "title": "10  Gibbs Sampling",
    "section": "11.4 Concluding Remarks",
    "text": "11.4 Concluding Remarks\nWe have introduced the Gibbs sampling algorithm as a powerful MCMC method for sampling from joint distributions when the conditional distributions are known and easy to sample from. The key insights from our analysis are:\n\nAlgorithmic Simplicity: Gibbs sampling reduces the complex problem of sampling from a joint distribution to the simpler task of iteratively sampling from conditional distributions.\nTheoretical Foundation: We proved that the joint distribution \\(f_{X,Y}\\) is a stationary distribution of the Markov Chain generated by the Gibbs sampler, ensuring that our samples will asymptotically follow the target distribution.\nPractical Examples: Through the joint exponential and bivariate normal distributions, we demonstrated how the algorithm adapts to different types of conditional distributions—from truncated exponentials to Gaussians.\n\nThe examples we studied involved continuous random variables with relatively simple conditional forms. However, the true power of Gibbs sampling becomes apparent when dealing with high-dimensional discrete systems where direct sampling from the joint distribution is computationally intractable.\n\n\n\n\n\n\nLooking Ahead: The Ising Model\n\n\n\nIn our next topic, we will apply Gibbs sampling to the Ising model—a fundamental model in statistical physics that describes magnetic systems. The Ising model presents several new challenges:\n\nDiscrete state space: Each site takes values in \\(\\{-1, +1\\}\\) rather than continuous values\nHigh dimensionality: We’ll work with lattices containing hundreds or thousands of sites\nComplex dependencies: Each site’s conditional distribution depends on all its neighbors\nPhase transitions: The model exhibits different behavior at different temperatures\n\nThe Ising model will demonstrate how Gibbs sampling can tackle problems where the joint distribution is known up to a normalization constant, but direct sampling is impossible due to the combinatorial explosion of possible configurations.\n\n\nThe techniques we’ve developed here—understanding the Markov chain structure, working with conditional distributions, and ensuring proper convergence—will be essential tools as we move to this more complex and practically important application.",
    "crumbs": [
      "Sampling Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gibbs Sampling</span>"
    ]
  },
  {
    "objectID": "chapters/applications/SDE.html#sec-euler-maruyama",
    "href": "chapters/applications/SDE.html#sec-euler-maruyama",
    "title": "13  Stochastic Differential Equations",
    "section": "13.5 Euler-Maruyama Method",
    "text": "13.5 Euler-Maruyama Method\nMost SDEs do not have analytical solutions. We need to solve them numerically. Similar to ODEs, some simple regularity conditions on the coefficients \\(a\\) and \\(b\\) imply that the SDE has a unique solution. Most common SDEs satisfy these conditions. However, unlike ODEs, the solution to an SDE is a stochastic process. We can’t just evaluate the solution at a few points to get an approximate solution. We need to generate several sample paths of the stochastic process to get an approximate solution.\nThe simplest method for solving SDEs is the Euler-Maruyama method. This is a stochastic analog of the Euler method for ODEs. The Euler-Maruyama method is a recursive method. Given the value of the stochastic process \\(X_n\\) at time \\(t_n\\), we can find the value of the process at time \\(t_{n+1} = t_n + \\Delta t\\) using the formula\n\\[X_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n \\tag{13.8}\\]\nwhere \\(\\Delta t = t_{n+1} - t_n\\) and \\(\\Delta W_n = W(t_{n+1}) - W(t_n) \\sim \\mathcal{N}(0, \\Delta t)\\).\n\n\n\n\n\n\nAlgorithm: Euler-Maruyama Method\n\n\n\nThis results in a simple algorithm for solving SDEs:\n\nInitialize \\(X_0\\).\nFor \\(n = 0, 1, 2, \\ldots, N-1\\), do:\n\nGenerate a random number \\(\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)\\).\nCompute \\(X_{n+1} = X_n + a(X_n, t_n) \\Delta t + b(X_n, t_n) \\Delta W_n\\).\n\nReturn \\(X_0, X_1, \\ldots, X_N\\).",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Stochastic Differential Equations</span>"
    ]
  }
]