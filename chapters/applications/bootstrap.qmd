---
title: "Bootstrap"
date: 2025-06-18
toc: true

execute:
  echo: false
---

## Introduction

Bootstrapping is a powerful statistical resampling method that allows us to estimate the sampling distribution of a statistic without making strong parametric assumptions. Introduced by Bradley Efron in 1979, the bootstrap provides a way to quantify uncertainty, construct confidence intervals, and perform hypothesis tests when traditional analytical methods are difficult or impossible to apply.

The core idea is elegantly simple: if we don't know the true population distribution, we can use our sample as a proxy for the population and repeatedly resample from it to understand how our statistic would behave across different samples.

### Connection to Particle Filtering

The bootstrap principle shares fundamental similarities with particle filtering, which we explored in the previous chapter. Both methods use **resampling** to approximate complex distributions:

- **Particle filters** resample particles based on their importance weights to maintain a good approximation of the posterior distribution over time.
- **Bootstrap methods** resample from observed data to approximate the sampling distribution of statistics.

In particle filtering, we saw how resampling addresses particle degeneracy - when most particles have negligible weights. Similarly, bootstrap resampling addresses the problem of not knowing the true population distribution by treating our sample as a "mini-population" to resample from.

The **multinomial resampling** we used in particle filters is essentially the same procedure used in non-parametric bootstrapping: we draw samples with replacement according to a probability distribution (uniform weights in bootstrap, importance weights in particle filtering).

### The Bootstrap Principle

Consider a sample $\mathbf{X} = \{X_1, X_2, \ldots, X_n\}$ drawn from an unknown distribution $F$. We want to understand the sampling distribution of some statistic $T(\mathbf{X})$ - for example, the sample mean, median, or a regression coefficient.

The **bootstrap principle** states that:
> The relationship between the sample and the population mirrors the relationship between a bootstrap sample and the original sample.

Mathematically:
$$
\text{Population} \rightarrow \text{Sample} \quad \text{as} \quad \text{Sample} \rightarrow \text{Bootstrap Sample}
$$

This analogy allows us to approximate the unknown sampling distribution $F_{T}$ of our statistic using the empirical distribution of bootstrap statistics.

## Non-parametric Bootstrap

### The Empirical Distribution Function

The foundation of non-parametric bootstrapping is the **empirical distribution function** (EDF). Given a sample $\mathbf{x} = \{x_1, x_2, \ldots, x_n\}$, the EDF is defined as:

$$
\hat{F}_n(t) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}(x_i \leq t)
$$

where $\mathbf{1}(\cdot)$ is the indicator function. The EDF assigns probability $1/n$ to each observed data point, making it a discrete uniform distribution over the sample values.

**Key Properties of the EDF**:

- **Unbiased**: $\mathbb{E}[\hat{F}_n(t)] = F(t)$ for all $t$.
- **Consistent**: $\hat{F}_n(t) \rightarrow F(t)$ almost surely as $n \rightarrow \infty$.
- **Glivenko-Cantelli**: $\sup_t |\hat{F}_n(t) - F(t)| \rightarrow 0$ almost surely.

### Bootstrap Sampling Procedure

The non-parametric bootstrap generates **bootstrap samples** by sampling with replacement from the original data, exactly like the resampling step in particle filtering:

::: {.callout-note}
## Non-parametric Bootstrap Algorithm

**Input**: Original sample $\mathbf{x} = \{x_1, x_2, \ldots, x_n\}$, number of bootstrap samples $B$

**For** $b = 1, 2, \ldots, B$:

1. **Resample**: Draw $\mathbf{x}^{*(b)} = \{x_1^{*(b)}, x_2^{*(b)}, \ldots, x_n^{*(b)}\}$ by sampling $n$ observations with replacement from $\mathbf{x}$.
2. **Compute**: Calculate the statistic $T^{*(b)} = T(\mathbf{x}^{*(b)})$.

**Output**: Bootstrap distribution $\{T^{*(1)}, T^{*(2)}, \ldots, T^{*(B)}\}$
:::

Each bootstrap sample $\mathbf{x}^{*(b)}$ has the same size as the original sample but may contain repeated observations and miss others entirely.

### Mathematical Foundation

Let $T_n = T(\mathbf{X})$ be a statistic computed from the original sample, and $T_n^* = T(\mathbf{X}^*)$ be the same statistic computed from a bootstrap sample. The bootstrap approximation is:

$$
\mathcal{L}(T_n^* | \mathbf{X}) \approx \mathcal{L}(T_n | F)
$$

where $\mathcal{L}(\cdot | \cdot)$ denotes the conditional distribution.

**Bootstrap Estimate of Bias**:
$$
\text{bias}_{\text{boot}}(T_n) = \mathbb{E}[T_n^* | \mathbf{X}] - T_n = \frac{1}{B} \sum_{b=1}^{B} T^{*(b)} - T_n
$$

**Bootstrap Estimate of Variance**:
$$
\text{var}_{\text{boot}}(T_n) = \text{Var}(T_n^* | \mathbf{X}) = \frac{1}{B-1} \sum_{b=1}^{B} (T^{*(b)} - \bar{T}^*)^2
$$

where $\bar{T}^* = \frac{1}{B} \sum_{b=1}^{B} T^{*(b)}$.

### Bootstrap Confidence Intervals

The bootstrap enables several methods for constructing confidence intervals:

#### 1. Normal Approximation Method

Assumes the bootstrap distribution is approximately normal:

$$
T_n \pm z_{\alpha/2} \cdot \text{se}_{\text{boot}}(T_n)
$$

where $\text{se}_{\text{boot}}(T_n) = \sqrt{\text{var}_{\text{boot}}(T_n)}$ and $z_{\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution.

#### 2. Percentile Method

Uses the quantiles of the bootstrap distribution directly:

$$
\text{CI}_{1-\alpha} = [T^*_{(\alpha/2)}, T^*_{(1-\alpha/2)}]
$$

where $T^*_{(q)}$ is the $q$-th quantile of the bootstrap distribution $\{T^{*(1)}, \ldots, T^{*(B)}\}$.

### Example: Bootstrap for Sample Mean

Consider estimating the mean $\mu$ from a sample $\mathbf{x} = \{x_1, \ldots, x_n\}$.

**Original Statistic**: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$

**Bootstrap Procedure**:

1. For $b = 1, \ldots, B$: Sample $\mathbf{x}^{*(b)}$ with replacement from $\mathbf{x}$.
2. Compute $\bar{x}^{*(b)} = \frac{1}{n}\sum_{i=1}^n x_i^{*(b)}$.

**Theoretical Results**:

- $\mathbb{E}[\bar{x}^* | \mathbf{x}] = \bar{x}$ (unbiased).
- $\text{Var}(\bar{x}^* | \mathbf{x}) = \frac{1}{n}\hat{\sigma}^2$ where $\hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$.

This matches the classical result, demonstrating bootstrap consistency for the sample mean.

### Bootstrap Hypothesis Testing

The bootstrap can also be used for hypothesis testing. Consider testing $H_0: \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$.

**Bootstrap p-value**:

1. Compute the test statistic $T_{\text{obs}} = T(\mathbf{x})$ from the original sample.
2. Generate bootstrap samples under $H_0$ (this may require centering or other transformations).
3. Compute bootstrap test statistics $\{T^{*(1)}, \ldots, T^{*(B)}\}$.
4. Calculate the p-value: $p = \frac{\#\{|T^{*(b)}| \geq |T_{\text{obs}}|\}}{B}$.

### Limitations of Non-parametric Bootstrap

1. **Boundary problems**: Poor performance when the parameter is near a boundary.
2. **Extreme values**: May not capture tail behavior well with finite samples.
3. **Dependence structures**: Assumes observations are i.i.d.
4. **Computational cost**: Requires $B \times n$ resampling operations.
5. **Discrete distributions**: Can be problematic for discrete data with few unique values.

## Parametric Bootstrap

### Motivation and Framework

While the non-parametric bootstrap makes minimal distributional assumptions, there are situations where we have good reason to believe the data follows a specific parametric family. The **parametric bootstrap** leverages this knowledge to potentially improve bootstrap performance.

This approach mirrors the distinction we saw in particle filtering between the **bootstrap filter** (which samples from the natural dynamics) and filters that use **informed proposal distributions** (which incorporate additional information to guide sampling).

**Key Idea**: Instead of resampling from the empirical distribution, we:

1. Estimate the parameters of the assumed distribution.
2. Generate bootstrap samples from the fitted parametric distribution.


```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Set random seed for reproducibility
np.random.seed(42)

# True population parameters
true_mean, true_std = 0, 1
true_dist = stats.norm(true_mean, true_std)

def bootstrap_means(sample, n_bootstrap=1000):
    """Bootstrap the sample mean"""
    bootstrap_means = []
    n = len(sample)
    for _ in range(n_bootstrap):
        bootstrap_sample = np.random.choice(sample, size=n, replace=True)
        bootstrap_means.append(np.mean(bootstrap_sample))
    return np.array(bootstrap_means)

# Different sample sizes to demonstrate
sample_sizes = [3, 5, 10, 25]
n_bootstrap = 2000

# Create the visualization
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
fig.suptitle('Bootstrap Convergence to Normal Distribution', fontsize=16, fontweight='bold')

# Top row: Original samples
# Bottom row: Bootstrap distributions

x_range = np.linspace(-4, 4, 1000)
true_pdf = true_dist.pdf(x_range)

for i, n in enumerate(sample_sizes):
    # Generate small sample from true normal distribution
    sample = np.random.normal(true_mean, true_std, n)
    
    # Top row: Show the sample with true population
    axes[0, i].plot(x_range, true_pdf, 'k-', linewidth=2, alpha=0.8, label='True Population N(0,1)')
    axes[0, i].hist(sample, bins=max(3, n//2), alpha=0.7, color='lightblue', 
                    edgecolor='black', density=True, label=f'Sample (n={n})')
    axes[0, i].scatter(sample, np.zeros(len(sample)), color='red', s=50, zorder=5, alpha=0.8)
    axes[0, i].axvline(np.mean(sample), color='red', linestyle='--', linewidth=2, 
                       label=f'Sample Mean: {np.mean(sample):.2f}')
    axes[0, i].set_title(f'Original Sample (n={n})', fontsize=12, fontweight='bold')
    axes[0, i].set_xlim(-4, 4)
    axes[0, i].set_ylim(0, 0.5)
    axes[0, i].legend(fontsize=9)
    axes[0, i].grid(True, alpha=0.3)
    
    # Bootstrap the sample means
    boot_means = bootstrap_means(sample, n_bootstrap)
    
    # Bottom row: Show bootstrap distribution
    axes[1, i].hist(boot_means, bins=50, alpha=0.8, color='lightcoral', 
                    edgecolor='darkred', density=True, label=f'Bootstrap Distribution')
    
    # Overlay theoretical normal distribution for bootstrap means
    # Theory: bootstrap means should be ~ N(sample_mean, sample_std/sqrt(n))
    theoretical_mean = np.mean(sample)
    theoretical_std = np.std(sample, ddof=1) / np.sqrt(n)
    theoretical_x = np.linspace(boot_means.min(), boot_means.max(), 1000)
    theoretical_pdf = stats.norm.pdf(theoretical_x, theoretical_mean, theoretical_std)
    axes[1, i].plot(theoretical_x, theoretical_pdf, 'b-', linewidth=3, 
                    label=f'Theoretical N({theoretical_mean:.2f}, {theoretical_std:.2f})')
    
    axes[1, i].axvline(np.mean(boot_means), color='red', linestyle='--', linewidth=2,
                       label=f'Bootstrap Mean: {np.mean(boot_means):.2f}')
    axes[1, i].axvline(theoretical_mean, color='blue', linestyle=':', linewidth=2, alpha=0.7)
    
    axes[1, i].set_title(f'Bootstrap Distribution of Sample Mean\n({n_bootstrap:,} bootstrap samples)', 
                         fontsize=12, fontweight='bold')
    axes[1, i].legend(fontsize=9)
    axes[1, i].grid(True, alpha=0.3)
    
    # Add text with key statistics
    boot_mean = np.mean(boot_means)
    boot_std = np.std(boot_means)
    axes[1, i].text(0.05, 0.95, f'Bootstrap SE: {boot_std:.3f}\nTheory SE: {theoretical_std:.3f}', 
                    transform=axes[1, i].transAxes, fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

# Add row labels
fig.text(0.02, 0.75, 'Original\nSamples', fontsize=14, fontweight='bold', 
         ha='center', va='center', rotation=90)
fig.text(0.02, 0.25, 'Bootstrap\nDistributions', fontsize=14, fontweight='bold', 
         ha='center', va='center', rotation=90)

plt.tight_layout()
plt.subplots_adjust(left=0.06, top=0.92)
plt.show()

# Show the convergence more clearly with a simpler approach
fig, ax = plt.subplots(1, 1, figsize=(12, 8))

sample_sizes_detailed = [3, 5, 10, 20, 50]
colors = ['red', 'orange', 'green', 'blue', 'purple']

# Set up the plot
x_min, x_max = -3, 3
y_positions = np.arange(len(sample_sizes_detailed))

for i, (n, color) in enumerate(zip(sample_sizes_detailed, colors)):
    # Generate sample
    np.random.seed(42 + i)  # Different seed for each sample size
    sample = np.random.normal(true_mean, true_std, n)
    
    # Bootstrap
    boot_means = bootstrap_means(sample, 1000)
    
    # Create histogram data
    hist_counts, hist_bins = np.histogram(boot_means, bins=40, density=True)
    hist_centers = (hist_bins[:-1] + hist_bins[1:]) / 2
    
    # Scale histogram heights for better visualization
    hist_heights = hist_counts * 0.15  # Scale factor for visibility
    
    # Plot the histogram as a filled curve at the appropriate y-position
    y_base = i
    ax.fill_between(hist_centers, y_base, y_base + hist_heights, 
                    alpha=0.7, color=color, label=f'n={n}')
    
    # Add theoretical normal curve
    sample_mean = np.mean(sample)
    sample_se = np.std(sample, ddof=1) / np.sqrt(n)
    x_theory = np.linspace(x_min, x_max, 200)
    y_theory = stats.norm.pdf(x_theory, sample_mean, sample_se)
    y_theory_scaled = y_theory * 0.15  # Same scale factor
    
    ax.plot(x_theory, y_base + y_theory_scaled, 'k-', linewidth=2, alpha=0.8)
    
    # Add vertical line at sample mean
    ax.axvline(sample_mean, ymin=i/len(sample_sizes_detailed), 
               ymax=(i+0.8)/len(sample_sizes_detailed), 
               color='black', linestyle='--', alpha=0.6)

ax.set_xlim(x_min, x_max)
ax.set_ylim(-0.5, len(sample_sizes_detailed))
ax.set_xlabel('Bootstrap Sample Mean', fontsize=14)
ax.set_ylabel('Sample Size', fontsize=14)
ax.set_yticks(y_positions)
ax.set_yticklabels([f'n={n}' for n in sample_sizes_detailed])
ax.set_title('Bootstrap Distribution Convergence\n(Colored: Actual Bootstrap, Black: Theoretical Normal)', 
             fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3)

# Add legend
ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))

plt.tight_layout()
plt.show()

print("Key Insights:")
print("• Even with very small samples (n=3), bootstrap gives approximately normal distribution")
print("• As sample size increases, bootstrap distribution becomes more concentrated around true mean")
print("• Bootstrap standard error decreases as 1/√n (Central Limit Theorem)")
print("• Bootstrap distribution matches theoretical normal distribution remarkably well")
```


### Parametric Bootstrap Procedure

Assume the data $\mathbf{X} = \{X_1, \ldots, X_n\}$ follows a distribution $F_{\boldsymbol{\theta}}$ with parameter vector $\boldsymbol{\theta}$.

::: {.callout-note}
## Parametric Bootstrap Algorithm

**Input**: Original sample $\mathbf{x} = \{x_1, x_2, \ldots, x_n\}$, parametric family $F_{\boldsymbol{\theta}}$, number of bootstrap samples $B$

1. **Estimate**: Compute parameter estimate $\hat{\boldsymbol{\theta}} = \hat{\boldsymbol{\theta}}(\mathbf{x})$ (e.g., MLE).

**For** $b = 1, 2, \ldots, B$:

2. **Generate**: Draw $\mathbf{x}^{*(b)} = \{x_1^{*(b)}, x_2^{*(b)}, \ldots, x_n^{*(b)}\}$ from $F_{\hat{\boldsymbol{\theta}}}$.
3. **Compute**: Calculate the statistic $T^{*(b)} = T(\mathbf{x}^{*(b)})$.

**Output**: Bootstrap distribution $\{T^{*(1)}, T^{*(2)}, \ldots, T^{*(B)}\}$
:::

### Mathematical Properties

The parametric bootstrap approximation is:
$$
\mathcal{L}(T_n^* | \hat{\boldsymbol{\theta}}) \approx \mathcal{L}(T_n | \boldsymbol{\theta})
$$

**Advantages over Non-parametric Bootstrap**:

- **Efficiency**: Can be more accurate when the parametric assumption is correct.
- **Extrapolation**: Better at capturing tail behavior and extreme values.
- **Smoothness**: Generates continuous bootstrap distributions even from discrete data.
- **Small samples**: Often performs better with limited data.

**Disadvantages**:

- **Model dependence**: Performance degrades if parametric assumption is wrong.
- **Bias**: Can introduce bias if the model is misspecified.

### Example: Normal Distribution

Suppose $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ and we want to bootstrap the sample variance $S^2$.

**Parameter Estimation**:
$$
\hat{\mu} = \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i, \quad \hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2
$$

**Bootstrap Procedure**:

1. For $b = 1, \ldots, B$: Generate $x_1^{*(b)}, \ldots, x_n^{*(b)} \sim \mathcal{N}(\hat{\mu}, \hat{\sigma}^2)$.
2. Compute $(S^2)^{*(b)} = \frac{1}{n-1}\sum_{i=1}^n (x_i^{*(b)} - \bar{x}^{*(b)})^2$.

**Theoretical Result**: For the normal distribution, we know that:
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
$$

The parametric bootstrap will approximate this distribution, while the non-parametric bootstrap provides a discrete approximation based on the specific sample values.

### Example: Linear Regression

Consider the linear model $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$.

**Parameter Estimation**:
$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}, \quad \hat{\sigma}^2 = \frac{1}{n-p}\|\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}\|^2
$$

**Parametric Bootstrap Approaches**:

**Method 1 - Residual Bootstrap**:

1. Compute fitted values $\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}$ and residuals $\hat{\boldsymbol{\epsilon}} = \mathbf{y} - \hat{\mathbf{y}}$.
2. For each bootstrap: $\mathbf{y}^{*(b)} = \hat{\mathbf{y}} + \boldsymbol{\epsilon}^{*(b)}$ where $\boldsymbol{\epsilon}^{*(b)} \sim \mathcal{N}(\mathbf{0}, \hat{\sigma}^2\mathbf{I})$.
3. Compute $\hat{\boldsymbol{\beta}}^{*(b)} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}^{*(b)}$.

**Method 2 - Pairs Bootstrap** (semi-parametric):

1. Resample pairs $(\mathbf{x}_i, y_i)$ with replacement.
2. Fit regression to bootstrap sample.


```{python}

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Set random seed for reproducibility
np.random.seed(42)

def bootstrap_statistic_nonparametric(data, statistic_func, n_bootstrap=1000):
    """Non-parametric bootstrap"""
    bootstrap_stats = []
    n = len(data)
    for _ in range(n_bootstrap):
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        bootstrap_stats.append(statistic_func(bootstrap_sample))
    return np.array(bootstrap_stats)

def bootstrap_statistic_parametric(data, distribution, statistic_func, n_bootstrap=1000):
    """Parametric bootstrap assuming a specific distribution"""
    params = distribution.fit(data)
    n = len(data)
    
    bootstrap_stats = []
    for _ in range(n_bootstrap):
        bootstrap_sample = distribution.rvs(*params, size=n)
        bootstrap_stats.append(statistic_func(bootstrap_sample))
    return np.array(bootstrap_stats)

def sample_90th_percentile(x):
    """90th percentile is very sensitive to distribution shape"""
    return np.percentile(x, 90)

# Create two extreme scenarios
n_samples = 30  # Smaller sample to make differences more pronounced
n_bootstrap = 2000

# Scenario 1: Normal data (parametric assumption CORRECT)
normal_data = np.random.normal(loc=5, scale=2, size=n_samples)

# Scenario 2: EXTREMELY skewed data (parametric assumption VERY WRONG)
# Use exponential distribution - very different from normal
exponential_data = np.random.exponential(scale=2, size=n_samples)

# Create clean 2x2 visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# LEFT COLUMN: Normal data (correct assumption)
# Top left: Original data
axes[0, 0].hist(normal_data, bins=12, alpha=0.7, color='lightblue', 
                edgecolor='black', density=True)
mu_hat, sigma_hat = stats.norm.fit(normal_data)
x_range = np.linspace(normal_data.min()-2, normal_data.max()+2, 200)
fitted_curve = stats.norm.pdf(x_range, mu_hat, sigma_hat)
axes[0, 0].plot(x_range, fitted_curve, 'r-', linewidth=3, 
                label='Fitted Normal (Good fit!)')
axes[0, 0].set_title('Normal Data\n✓ Parametric assumption CORRECT', 
                     fontsize=12, fontweight='bold', color='green')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_xlim(-2, 12)

# Bottom left: Bootstrap comparison for normal data
nonparam_percentiles_normal = bootstrap_statistic_nonparametric(normal_data, sample_90th_percentile, n_bootstrap)
param_percentiles_normal = bootstrap_statistic_parametric(normal_data, stats.norm, sample_90th_percentile, n_bootstrap)

axes[1, 0].hist(nonparam_percentiles_normal, bins=40, alpha=0.8, color='blue', 
                density=True, label='Non-parametric', edgecolor='darkblue')
axes[1, 0].hist(param_percentiles_normal, bins=40, alpha=0.8, color='red', 
                density=True, label='Parametric', edgecolor='darkred')
axes[1, 0].axvline(np.percentile(normal_data, 90), color='black', linestyle='--', 
                   linewidth=2, label=f'True 90th %ile: {np.percentile(normal_data, 90):.1f}')
axes[1, 0].set_title('Bootstrap: 90th Percentile', 
                     fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('90th Percentile')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# RIGHT COLUMN: Exponential data (VERY wrong assumption)
# Top right: Original data
axes[0, 1].hist(exponential_data, bins=12, alpha=0.7, color='lightcoral', 
                edgecolor='black', density=True)
mu_hat_wrong, sigma_hat_wrong = stats.norm.fit(exponential_data)
x_range_exp = np.linspace(0, exponential_data.max()+2, 200)
fitted_curve_wrong = stats.norm.pdf(x_range_exp, mu_hat_wrong, sigma_hat_wrong)
axes[0, 1].plot(x_range_exp, fitted_curve_wrong, 'r-', linewidth=3, 
                label='Fitted Normal (Terrible fit!)')
# Show true exponential curve
true_exp = stats.expon.pdf(x_range_exp, scale=2)
axes[0, 1].plot(x_range_exp, true_exp, 'g--', linewidth=3, 
                label='True Exponential', alpha=0.9)
axes[0, 1].set_title('Exponential Data\n✗ Parametric assumption VERY WRONG', 
                     fontsize=12, fontweight='bold', color='red')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].set_xlim(0, 12)

# Bottom right: Bootstrap comparison for exponential data
nonparam_percentiles_exp = bootstrap_statistic_nonparametric(exponential_data, sample_90th_percentile, n_bootstrap)
param_percentiles_exp = bootstrap_statistic_parametric(exponential_data, stats.norm, sample_90th_percentile, n_bootstrap)

axes[1, 1].hist(nonparam_percentiles_exp, bins=40, alpha=0.8, color='blue', 
                density=True, label='Non-parametric (Robust)', edgecolor='darkblue')
axes[1, 1].hist(param_percentiles_exp, bins=40, alpha=0.8, color='red', 
                density=True, label='Parametric (BIASED!)', edgecolor='darkred')
axes[1, 1].axvline(np.percentile(exponential_data, 90), color='black', linestyle='--', 
                   linewidth=2, label=f'True 90th %ile: {np.percentile(exponential_data, 90):.1f}')
axes[1, 1].set_title('Bootstrap: 90th Percentile', 
                     fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('90th Percentile')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

# Add dramatic difference annotations
np_mean_exp = np.mean(nonparam_percentiles_exp)
p_mean_exp = np.mean(param_percentiles_exp)
difference = abs(np_mean_exp - p_mean_exp)

axes[1, 1].annotate('', xy=(p_mean_exp, 0.15), xytext=(np_mean_exp, 0.15),
                    arrowprops=dict(arrowstyle='<->', color='red', lw=2))
axes[1, 1].text((np_mean_exp + p_mean_exp)/2, 0.17, f'Bias: {difference:.1f}', 
                ha='center', fontsize=11, fontweight='bold', color='red',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))

plt.tight_layout()
plt.subplots_adjust(top=0.93)
plt.show()

print("DRAMATIC DIFFERENCE:")
print(f"• Normal data: Non-param vs Param difference = {abs(np.mean(nonparam_percentiles_normal) - np.mean(param_percentiles_normal)):.2f}")
print(f"• Exponential data: Non-param vs Param difference = {difference:.2f} (10x larger!)")
print(f"• Parametric bootstrap severely underestimates the 90th percentile when assumption is wrong")

```


### Comparison: Parametric vs Non-parametric

| Aspect | Non-parametric | Parametric |
|--------|---------------|------------|
| **Assumptions** | Minimal (i.i.d. data) | Strong (distributional family) |
| **Robustness** | High | Low (sensitive to model misspecification) |
| **Efficiency** | Lower when model is correct | Higher when model is correct |
| **Tail behavior** | Limited by sample | Can extrapolate beyond sample |
| **Computational cost** | Resampling only | Parameter estimation + generation |
| **Small samples** | Can be unstable | Often more stable |

### Model-based Bootstrap Extensions

**Semiparametric Bootstrap**: Combines parametric and non-parametric elements. For example, in regression:

- Use parametric assumptions for the error distribution.
- Use non-parametric resampling for the covariate distribution.

**Bayesian Bootstrap**: Treats the empirical distribution as a realization from a Dirichlet process, providing a Bayesian interpretation of the bootstrap.

**Wild Bootstrap**: Used for heteroscedastic regression models, where residuals are multiplied by random weights rather than resampled.

### Choosing Between Bootstrap Methods

**Use Non-parametric Bootstrap when**:

- Uncertain about distributional assumptions.
- Robustness is more important than efficiency.
- Sample size is large.
- Exploring data without strong prior beliefs.

**Use Parametric Bootstrap when**:

- Confident in the parametric model.
- Sample size is small.
- Need to extrapolate beyond observed data.
- Theoretical distribution is well-established.
- Computational efficiency is important.

**Diagnostic Approach**:

1. Compare parametric and non-parametric bootstrap results.
2. If similar: parametric assumptions likely reasonable.
3. If different: investigate model assumptions or prefer non-parametric.
4. Use residual plots and goodness-of-fit tests to validate parametric assumptions.

### Connection Back to Particle Filtering

The choice between parametric and non-parametric bootstrap mirrors the choice between different proposal distributions in particle filtering:

- **Non-parametric bootstrap** ≈ **Bootstrap particle filter**: Uses the "natural" sampling distribution (empirical distribution or system dynamics).
- **Parametric bootstrap** ≈ **Optimal proposal**: Uses additional information (parametric model or observations) to guide sampling more efficiently.

Just as particle filters must balance computational efficiency with robustness to model misspecification, bootstrap methods face the same trade-off between parametric efficiency and non-parametric robustness.

The bootstrap, in both its parametric and non-parametric forms, provides a unified framework for statistical inference that bridges classical theory with modern computational methods. Its flexibility and intuitive appeal have made it one of the most important statistical innovations of the late 20th century, with deep connections to other resampling-based methods like particle filtering.

