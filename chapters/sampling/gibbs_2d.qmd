---
title: "Gibbs Sampling"
date: 2025-06-18
author: Unknown Author
toc: true  

execute:
  echo: false
---
```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

np.random.seed(0)
```

```{python}
lags = 20
threshold = 0.1

# Correlation between X and Y
def autocorr(x, t):
  return np.corrcoef(x[:-t], x[t:])[0, 1]

n = 10 # burn-in period
N = 10000
```


Gibbs Sampling is a MCMC algorithm that is used to sample from a joint distribution using conditional distributions. For now, we'll focus on sampling in 2D, but the algorithm generalizes to higher dimensions.

The Gibbs sampling algorithm for sampling from a joint distribution $f_{X, Y}(X, Y)$ is as follows:

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$
   1. Sample $X_i \sim f_{X|Y}(\: \cdot \: | Y_{i-1})$.
   2. Sample $Y_i \sim f_{Y|X}(\: \cdot \:  | X_i)$.
3. Return the $(X_0, Y_0)$, $(X_1, Y_1)$, $\ldots$, $(X_N, Y_N)$.

The algorithm generates a sample path of length $N$ of the Markov Chain as described in @sec-gibbs-markov-chain. If needed, we can discard the initial samples to ensure that the Markov Chain has converged to the stationary distribution.

### Joint Exponential Distribution

Let $X$ and $Y$ be two random variables with the truncated exponential distribution:

$$
f_{X,Y}(x, y) = c e^{-\lambda xy} \text{ for } 0 \le x \le D_1, 0 \le y \le D_2,
$$

where $c$ is the normalization constant. We want to sample from the joint distribution $f_{X,Y}$. 
One can show that the conditionals are,

$$
\begin{aligned}
f_{X|Y}(x | y_0) &= c_{y_0} e^{-\lambda xy_0} \text{ for } 0 \le x \le D_1 \\
f_{Y|X}(y | x_0) &= c_{x_0} e^{-\lambda yx_0} \text{ for } 0 \le y \le D_2,
\end{aligned}
$$

where $c_{y_0}$ and $c_{x_0}$ are the normalization constants.
We can easily sample from the two conditionals $f_{X|Y}$ and $f_{Y|X}$ using the inverse method function (even for truncated distributions). The Gibbs algorithm becomes

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$
   1. Sample $X_i \sim (\text{Exp}(\lambda Y_{i-1})$ restricted to $[0, D_1])$ using the inverse transform method.
   2. Sample $Y_i \sim (\text{Exp}(\lambda X_i)$ restricted to $[0, D_2]$) using the inverse transform method.
3. Return the $(X_0, Y_0)$, $(X_1, Y_1)$, $\ldots$, $(X_N, Y_N)$.

```{python}
class truncated_exponential:
    def __init__(self, lambda_, D):
        self.lambda_ = lambda_
        self.D = D

    def rvs(self, size=1):
        """Samples from a truncated exponential distribution over [0, D]"""
        U = np.random.uniform(0, 1, size)
        Z = -np.log(1 - U * (1 - np.exp(-self.lambda_ * self.D))) / self.lambda_
        return Z

    def pdf(self, x):
        """Computes the probability density function of the truncated exponential distribution."""
        return (self.lambda_ * np.exp(-self.lambda_ * x)) / (1 - np.exp(-self.lambda_ * self.D)) * (x >= 0) * (x <= self.D)


class truncated_2D_exponential:
    def __init__(self, lambda_, D1, D2):
        self.lambda_ = lambda_
        self.D1 = D1
        self.D2 = D2

    def X_given_Y(self, y):
        """Returns the conditional truncated exponential distribution P(X | Y=y)"""
        return truncated_exponential(self.lambda_ * y, self.D1)

    def Y_given_X(self, x):
        """Returns the conditional truncated exponential distribution P(Y | X=x)"""
        return truncated_exponential(self.lambda_ * x, self.D2)

    def rvs(self, size=1000):
        """Gibbs sampler for the truncated 2D exponential distribution"""
        X = np.zeros(size)
        Y = np.zeros(size)

        # Initialize Y randomly
        Y[0] = np.random.uniform(0, self.D2)  # Ensure it's within [0, D2]

        for i in range(1, size):
            # Sample X given the previous Y
            X[i] = self.X_given_Y(Y[i - 1]).rvs(size=1)[0]
            # Sample Y given the new X
            Y[i] = self.Y_given_X(X[i]).rvs(size=1)[0]

        return X, Y
    
    def pdf(self, x, y):
        """Computes the probability density function of the truncated 2D exponential distribution."""
        if x < 0 or x > self.D1 or y < 0 or y > self.D2:
            return 0
        return self.lambda_ ** 2 * np.exp(-self.lambda_ * (x + y)) / (1 - np.exp(-self.lambda_ * self.D1)) / (1 - np.exp(-self.lambda_ * self.D2))


# Parameters
lambda_ = 2
D1 = 5
D2 = 3
num_samples = 5000

# Generate samples
model = truncated_2D_exponential(lambda_, D1, D2)
X, Y = model.rvs(num_samples)
```


```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plots

# Assuming model.pdf is already defined
x = np.linspace(-0.5, 3, 100)
y = np.linspace(-0.5, 3, 100)
X, Y = np.meshgrid(x, y)
pos = np.empty(X.shape + (2,))
pos[:, :, 0] = X
pos[:, :, 1] = Y
Z = np.array([[model.pdf(xi, yi) for xi, yi in zip(x_row, y_row)] for x_row, y_row in zip(X, Y)])

# Set figure width to 7 inches (700 px at 100 DPI), and height to 14 for vertical stacking
fig = plt.figure(figsize=(7, 14), tight_layout=True)

# First subplot: Contour plot (top)
ax1 = fig.add_subplot(211)
cp = ax1.contour(x, y, Z, levels=20, alpha=0.6)
ax1.set_xlim(0, 2)
ax1.set_ylim(0, 2)
ax1.set_title('Bivariate Exponential Contour Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')

# Second subplot: 3D surface plot (bottom)
ax2 = fig.add_subplot(212, projection='3d')
ax2.plot_surface(X, Y, Z, cmap='viridis')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_zlabel('Density')
ax2.set_title('Bivariate Exponential Density')

plt.show()
```

```{python}
def gibbs_sampler_exp(lambda_, D1, D2, x0=1.0, y0=1.0):
    X = np.zeros(N + 1)
    Y = np.zeros(N + 1)
    X[0] = x0
    Y[0] = y0
    for i in range(N):
        # Sample X given previous Y
        rate_x = lambda_ * Y[i] * D1
        # Prevent rate_x=0 division by zero
        if rate_x > 0:
            u = np.random.uniform()
            X[i+1] = -np.log(1 - u * (1 - np.exp(-rate_x))) / rate_x
        else:
            X[i+1] = 0.0

        # Sample Y given new X
        rate_y = lambda_ * X[i+1] * D2
        if rate_y > 0:
            u = np.random.uniform()
            Y[i+1] = -np.log(1 - u * (1 - np.exp(-rate_y))) / rate_y
        else:
            Y[i+1] = 0.0
    return X, Y

X, Y = gibbs_sampler_exp(lambda_, D1, D2)
```

```{python}
#| fig-width: 7

k = 20

plt.figure(figsize=(7, 7))  # 7 inches wide = 672 px at default 96 DPI

plt.xlim(min(X[:k])-0.5, max(Y[:k])+0.5)
plt.ylim(min(Y[:k])-0.5, max(X[:k])+0.5)

for i in range(k + 1):
    if i > 0:
        alpha = np.log(1 + (i / k)) * np.log(2)
        plt.plot([X[i - 1], X[i]], [Y[i - 1], Y[i - 1]], 'b-', linewidth=1, alpha=alpha)
        plt.plot([X[i], X[i]], [Y[i - 1], Y[i]], 'b-', linewidth=1, alpha=alpha)
    plt.plot(X[i], Y[i], 'ro', markersize=2)

# Start and end points
plt.plot(X[0], Y[0], 'go', markersize=8, label='Start')
plt.plot(X[k], Y[k], 'bo', markersize=8, label=f'{k}-th sample')
plt.legend()

plt.title('Gibbs Sampler Trajectories for Bivariate Exponential')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

### Bivariate Normal Distribution

Suppose $(X, Y)$ has a bivariate normal distribution with mean $(0, 0)$ and covariance matrix $\Sigma = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$. We want to sample from the joint distribution $f_{X, Y}$.
One can show that 

$$
\begin{aligned}
(X|Y = y_0) &\sim \mathcal{N}(\rho y_0, 1 - \rho^2) \\
(Y|X = x_0) &\sim \mathcal{N}(\rho x_0, 1 - \rho^2).
\end{aligned}
$$

We can sample from the two conditionals $f_{X|Y}$ and $f_{Y|X}$ using the Box-Muller method. The Gibbs algorithm becomes

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$
   1. Sample $X_i \sim \mathcal{N}(\rho Y_{i-1}, 1 - \rho^2)$.
   2. Sample $Y_i \sim \mathcal{N}(\rho X_i, 1 - \rho^2)$.
3. Return the $(X_0, Y_0)$, $(X_1, Y_1)$, $\ldots$, $(X_N, Y_N)$.

```{python}
#| fig-width: 7

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plots

rho = 0.5
rv = multivariate_normal([0, 0], [[1, rho], [rho, 1]])

# 3D plot of the bivariate normal distribution
x = np.linspace(-3, 3, 100)
y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(x, y)
pos = np.empty(X.shape + (2,))
pos[:, :, 0] = X
pos[:, :, 1] = Y
Z = rv.pdf(pos)

fig = plt.figure(figsize=(7, 14), tight_layout=True)

# Top subplot: Contour plot
ax1 = fig.add_subplot(211)
ax1.contour(x, y, Z, alpha=0.6)
ax1.set_title('Bivariate Normal Contour Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')

# Bottom subplot: 3D surface plot
ax2 = fig.add_subplot(212, projection='3d')
ax2.plot_surface(X, Y, Z, cmap='viridis')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_zlabel('Density')
ax2.set_title('Bivariate Normal PDF')

plt.show()
```

```{python}
def gibbs_sampler(rho, N):
  # In Gibbs we have a 100% acceptance rate
  X = np.zeros(N + 1) 
  Y = np.zeros(N + 1)
  for i in range(N): #np.arange(0,N,1):
    X[i+1] = rho * Y[i] + np.sqrt(1-rho**2) * np.random.normal(0,1)
    Y[i+1] = rho * X[i+1] + np.sqrt(1-rho**2) * np.random.normal(0,1)
  return X, Y

X, Y = gibbs_sampler(rho, N)
```

```{python}
#| fig-width: 7

plt.figure(figsize=(7, 7))
plt.scatter(X, Y, alpha=0.5, color='r', s=5)
plt.title('Gibbs Sampler for Bivariate Normal')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

```{python}
#| fig-width: 7

k = 20

plt.figure(figsize=(7, 7))  # 7 inches wide = 700px at 100 DPI

plt.xlim(min(X[:k]) - 0.5, max(X[:k]) + 0.5)
plt.ylim(min(Y[:k]) - 0.5, max(Y[:k]) + 0.5)

for i in range(k + 1):
    if i > 0:
        alpha = np.log(1 + (i / k)) * np.log(2)
        plt.plot([X[i - 1], X[i]], [Y[i - 1], Y[i - 1]], 'b-', linewidth=1, alpha=alpha)
        plt.plot([X[i], X[i]], [Y[i - 1], Y[i]], 'b-', linewidth=1, alpha=alpha)
    plt.plot(X[i], Y[i], 'ro', markersize=2)

# Mark start and end points
plt.plot(X[0], Y[0], 'go', markersize=8, label='Start')
plt.plot(X[k], Y[k], 'bo', markersize=8, label=f'{k}-th sample')
plt.legend()

plt.title('Gibbs Sampler Trajectories for Bivariate Normal')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

## Markov Chain {#sec-gibbs-markov-chain}

The Gibbs sampling algorithm generates a Markov Chain whose state space is the product space of the state spaces of the individual variables $\Omega = \Omega_X \times \Omega_Y$. In the above examples, the state space is $[0, D_1] \times [0, D_2]$ for the exponential distribution and $\mathbb{R}^2$ for the bivariate normal distribution. The transition matrix of the Markov Chain in discrete case is given by

$$
P \left( \begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} x' \\ y' \end{bmatrix} \right) = \mathbb{P}(X_{i+1} = x' | Y_i = y) \mathbb{P}(Y_{i+1} = y' | X_i = x').
$$

In the continuous case, the transition *kernel* is given by

$$
K \left( \begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} x' \\ y' \end{bmatrix} \right)
= f_{X|Y}(x | y) f_{Y|X}(y | x').
$$

::: {#thm-Gibbs-Markov-Chain}
The Gibbs sampling algorithm generates a Markov Chain with the transition matrix $P$ as described above. The joint distribution $f_{X, Y}$ is a stationary distribution of the Markov Chain. Hence, if the Markov Chain converges to the stationary distribution, the samples generated by the Gibbs algorithm will be distributed according to $f_{X, Y}$.
:::

We'll provide a proof of the above theorem in the next section.
