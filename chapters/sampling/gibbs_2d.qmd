---
title: "Gibbs Sampling"
date: 2025-06-18
author: Unknown Author
toc: true  

execute:
  echo: false
---
```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

np.random.seed(0)
```

```{python}
lags = 20
threshold = 0.1

# Correlation between X and Y
def autocorr(x, t):
  return np.corrcoef(x[:-t], x[t:])[0, 1]

n = 10 # burn-in period
N = 10000
```


# Gibbs Sampling

Gibbs Sampling is a Markov Chain Monte Carlo algorithm that is used to sample from a joint distribution using conditional distributions. For now, we'll focus on sampling in 2D, but the algorithm generalizes to higher dimensions.

::: {.callout-note}
## Gibbs Sampling Algorithm

The Gibbs sampling algorithm for sampling from a joint distribution $f_{X, Y}(x, y)$ is as follows:

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$:
   a. Sample $X_i \sim f_{X|Y}(\cdot \mid Y_{i-1})$.
   b. Sample $Y_i \sim f_{Y|X}(\cdot \mid X_i)$.
3. Return the sequence $(X_0, Y_0), (X_1, Y_1), \ldots, (X_N, Y_N)$.
:::

The algorithm generates a sample path of length $N$ of the Markov Chain as described in @sec-gibbs-markov-chain. If needed, we can discard the initial samples to ensure that the Markov Chain has converged to the stationary distribution.

## Joint Exponential Distribution

Let $X$ and $Y$ be two random variables with the truncated exponential distribution:

$$
f_{X,Y}(x, y) = c e^{-\lambda xy} \quad \text{for } 0 \leq x \leq D_1, \, 0 \leq y \leq D_2,
$$

where $c$ is the normalization constant. We want to sample from the joint distribution $f_{X,Y}$.

One can show that the conditionals are:

$$
\begin{aligned}
f_{X|Y}(x \mid y_0) &= c_{y_0} e^{-\lambda x y_0} \quad \text{for } 0 \leq x \leq D_1 \\
f_{Y|X}(y \mid x_0) &= c_{x_0} e^{-\lambda y x_0} \quad \text{for } 0 \leq y \leq D_2,
\end{aligned}
$$

where $c_{y_0}$ and $c_{x_0}$ are the normalization constants.

::: {.callout-tip}
## Key Insight
Notice that each conditional distribution is simply a truncated exponential distribution with rate parameter that depends on the conditioning variable. This makes sampling straightforward using the inverse transform method.
:::

We can easily sample from the two conditionals $f_{X|Y}$ and $f_{Y|X}$ using the inverse transform method (even for truncated distributions).

::: {.callout-note}
## Gibbs Algorithm for Joint Exponential Distribution

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$:
   a. Sample $X_i \sim \text{Exp}(\lambda Y_{i-1})$ restricted to $[0, D_1]$ using the inverse transform method.
   b. Sample $Y_i \sim \text{Exp}(\lambda X_i)$ restricted to $[0, D_2]$ using the inverse transform method.
3. Return the sequence $(X_0, Y_0), (X_1, Y_1), \ldots, (X_N, Y_N)$.
:::

```{python}
class truncated_exponential:
    def __init__(self, lambda_, D):
        self.lambda_ = lambda_
        self.D = D

    def rvs(self, size=1):
        """Samples from a truncated exponential distribution over [0, D]"""
        U = np.random.uniform(0, 1, size)
        Z = -np.log(1 - U * (1 - np.exp(-self.lambda_ * self.D))) / self.lambda_
        return Z

    def pdf(self, x):
        """Computes the probability density function of the truncated exponential distribution."""
        return (self.lambda_ * np.exp(-self.lambda_ * x)) / (1 - np.exp(-self.lambda_ * self.D)) * (x >= 0) * (x <= self.D)


class truncated_2D_exponential:
    def __init__(self, lambda_, D1, D2):
        self.lambda_ = lambda_
        self.D1 = D1
        self.D2 = D2

    def X_given_Y(self, y):
        """Returns the conditional truncated exponential distribution P(X | Y=y)"""
        return truncated_exponential(self.lambda_ * y, self.D1)

    def Y_given_X(self, x):
        """Returns the conditional truncated exponential distribution P(Y | X=x)"""
        return truncated_exponential(self.lambda_ * x, self.D2)

    def rvs(self, size=1000):
        """Gibbs sampler for the truncated 2D exponential distribution"""
        X = np.zeros(size)
        Y = np.zeros(size)

        # Initialize Y randomly
        Y[0] = np.random.uniform(0, self.D2)  # Ensure it's within [0, D2]

        for i in range(1, size):
            # Sample X given the previous Y
            X[i] = self.X_given_Y(Y[i - 1]).rvs(size=1)[0]
            # Sample Y given the new X
            Y[i] = self.Y_given_X(X[i]).rvs(size=1)[0]

        return X, Y
    
    def pdf(self, x, y):
        """Computes the probability density function of the truncated 2D exponential distribution."""
        if x < 0 or x > self.D1 or y < 0 or y > self.D2:
            return 0
        return self.lambda_ ** 2 * np.exp(-self.lambda_ * (x + y)) / (1 - np.exp(-self.lambda_ * self.D1)) / (1 - np.exp(-self.lambda_ * self.D2))


# Parameters
lambda_ = 2
D1 = 5
D2 = 3
num_samples = 5000

# Generate samples
model = truncated_2D_exponential(lambda_, D1, D2)
X, Y = model.rvs(num_samples)
```


```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plots

# Assuming model.pdf is already defined
x = np.linspace(-0.5, 3, 100)
y = np.linspace(-0.5, 3, 100)
X, Y = np.meshgrid(x, y)
pos = np.empty(X.shape + (2,))
pos[:, :, 0] = X
pos[:, :, 1] = Y
Z = np.array([[model.pdf(xi, yi) for xi, yi in zip(x_row, y_row)] for x_row, y_row in zip(X, Y)])

# Set figure width to 7 inches (700 px at 100 DPI), and height to 14 for vertical stacking
fig = plt.figure(figsize=(7, 14), tight_layout=True)

# First subplot: Contour plot (top)
ax1 = fig.add_subplot(211)
cp = ax1.contour(x, y, Z, levels=20, alpha=0.6)
ax1.set_xlim(0, 2)
ax1.set_ylim(0, 2)
ax1.set_title('Bivariate Exponential Contour Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')

# Second subplot: 3D surface plot (bottom)
ax2 = fig.add_subplot(212, projection='3d')
ax2.plot_surface(X, Y, Z, cmap='viridis')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_zlabel('Density')
ax2.set_title('Bivariate Exponential Density')

plt.show()
```

```{python}
def gibbs_sampler_exp(lambda_, D1, D2, x0=1.0, y0=1.0):
    X = np.zeros(N + 1)
    Y = np.zeros(N + 1)
    X[0] = x0
    Y[0] = y0
    for i in range(N):
        # Sample X given previous Y
        rate_x = lambda_ * Y[i] * D1
        # Prevent rate_x=0 division by zero
        if rate_x > 0:
            u = np.random.uniform()
            X[i+1] = -np.log(1 - u * (1 - np.exp(-rate_x))) / rate_x
        else:
            X[i+1] = 0.0

        # Sample Y given new X
        rate_y = lambda_ * X[i+1] * D2
        if rate_y > 0:
            u = np.random.uniform()
            Y[i+1] = -np.log(1 - u * (1 - np.exp(-rate_y))) / rate_y
        else:
            Y[i+1] = 0.0
    return X, Y

X, Y = gibbs_sampler_exp(lambda_, D1, D2)
```

```{python}
#| fig-width: 7

plt.figure(figsize=(7, 7))
plt.scatter(X, Y, alpha=0.5, color='r', s=5)
plt.title('Gibbs Sampler for Bivariate Normal')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

```{python}
#| fig-width: 7

k = 20

plt.figure(figsize=(7, 7))  # 7 inches wide = 672 px at default 96 DPI

plt.xlim(min(X[:k])-0.5, max(Y[:k])+0.5)
plt.ylim(min(Y[:k])-0.5, max(X[:k])+0.5)

for i in range(k + 1):
    if i > 0:
        alpha = np.log(1 + (i / k)) * np.log(2)
        plt.plot([X[i - 1], X[i]], [Y[i - 1], Y[i - 1]], 'b-', linewidth=1, alpha=alpha)
        plt.plot([X[i], X[i]], [Y[i - 1], Y[i]], 'b-', linewidth=1, alpha=alpha)
    plt.plot(X[i], Y[i], 'ro', markersize=2)

# Start and end points
plt.plot(X[0], Y[0], 'go', markersize=8, label='Start')
plt.plot(X[k], Y[k], 'bo', markersize=8, label=f'{k}-th sample')
plt.legend()

plt.title('Gibbs Sampler Trajectories for Bivariate Exponential')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

## Bivariate Normal Distribution

Suppose $(X, Y)$ has a bivariate normal distribution with mean $(0, 0)$ and covariance matrix $\Sigma = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$. We want to sample from the joint distribution $f_{X, Y}$.

One can show that the conditional distributions are:

$$
\begin{aligned}
(X \mid Y = y_0) &\sim \mathcal{N}(\rho y_0, 1 - \rho^2) \\
(Y \mid X = x_0) &\sim \mathcal{N}(\rho x_0, 1 - \rho^2).
\end{aligned}
$$

::: {.callout-important}
## Key Property
Notice that both conditional distributions have the same variance $1 - \rho^2$, which decreases as the correlation $|\rho|$ increases. When $\rho = 0$ (independence), the conditional variance equals the marginal variance.
:::

We can sample from the two conditionals $f_{X|Y}$ and $f_{Y|X}$ using standard normal sampling methods such as the Box-Muller transform.

::: {.callout-note}
## Gibbs Algorithm for Bivariate Normal Distribution

1. Start with some initial values $X_0$ and $Y_0$.
2. For $i = 1, 2, \ldots, N$:
   a. Sample $X_i \sim \mathcal{N}(\rho Y_{i-1}, 1 - \rho^2)$.
   b. Sample $Y_i \sim \mathcal{N}(\rho X_i, 1 - \rho^2)$.
3. Return the sequence $(X_0, Y_0), (X_1, Y_1), \ldots, (X_N, Y_N)$.
:::

```{python}
#| fig-width: 7

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plots

rho = 0.5
rv = multivariate_normal([0, 0], [[1, rho], [rho, 1]])

# 3D plot of the bivariate normal distribution
x = np.linspace(-3, 3, 100)
y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(x, y)
pos = np.empty(X.shape + (2,))
pos[:, :, 0] = X
pos[:, :, 1] = Y
Z = rv.pdf(pos)

fig = plt.figure(figsize=(7, 14), tight_layout=True)

# Top subplot: Contour plot
ax1 = fig.add_subplot(211)
ax1.contour(x, y, Z, alpha=0.6)
ax1.set_title('Bivariate Normal Contour Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')

# Bottom subplot: 3D surface plot
ax2 = fig.add_subplot(212, projection='3d')
ax2.plot_surface(X, Y, Z, cmap='viridis')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_zlabel('Density')
ax2.set_title('Bivariate Normal PDF')

plt.show()
```

```{python}
def gibbs_sampler(rho, N):
  # In Gibbs we have a 100% acceptance rate
  X = np.zeros(N + 1) 
  Y = np.zeros(N + 1)
  for i in range(N): #np.arange(0,N,1):
    X[i+1] = rho * Y[i] + np.sqrt(1-rho**2) * np.random.normal(0,1)
    Y[i+1] = rho * X[i+1] + np.sqrt(1-rho**2) * np.random.normal(0,1)
  return X, Y

X, Y = gibbs_sampler(rho, N)
```

```{python}
#| fig-width: 7

plt.figure(figsize=(7, 7))
plt.scatter(X, Y, alpha=0.5, color='r', s=5)
plt.title('Gibbs Sampler for Bivariate Normal')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

```{python}
#| fig-width: 7

k = 20

plt.figure(figsize=(7, 7))  # 7 inches wide = 700px at 100 DPI

plt.xlim(min(X[:k]) - 0.5, max(X[:k]) + 0.5)
plt.ylim(min(Y[:k]) - 0.5, max(Y[:k]) + 0.5)

for i in range(k + 1):
    if i > 0:
        alpha = np.log(1 + (i / k)) * np.log(2)
        plt.plot([X[i - 1], X[i]], [Y[i - 1], Y[i - 1]], 'b-', linewidth=1, alpha=alpha)
        plt.plot([X[i], X[i]], [Y[i - 1], Y[i]], 'b-', linewidth=1, alpha=alpha)
    plt.plot(X[i], Y[i], 'ro', markersize=2)

# Mark start and end points
plt.plot(X[0], Y[0], 'go', markersize=8, label='Start')
plt.plot(X[k], Y[k], 'bo', markersize=8, label=f'{k}-th sample')
plt.legend()

plt.title('Gibbs Sampler Trajectories for Bivariate Normal')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

## Markov Chain {#sec-gibbs-markov-chain}

The Gibbs sampling algorithm generates a Markov Chain whose state space is the product space of the state spaces of the individual variables $\Omega = \Omega_X \times \Omega_Y$. In the above examples, the state space is $[0, D_1] \times [0, D_2]$ for the exponential distribution and $\mathbb{R}^2$ for the bivariate normal distribution.

The transition matrix of the Markov Chain in the discrete case is given by:

$$
P \left( \begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} x' \\ y' \end{bmatrix} \right) = \mathbb{P}(X_{i+1} = x' \mid Y_i = y) \mathbb{P}(Y_{i+1} = y' \mid X_{i+1} = x').
$$

In the continuous case, the transition *kernel* is given by:

$$
K \left( \begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} x' \\ y' \end{bmatrix} \right)
= f_{X|Y}(x' \mid y) f_{Y|X}(y' \mid x').
$$

::: {.callout-warning}
## Important Detail
Note that in the transition kernel, we condition $Y_{i+1}$ on the *newly sampled* value $X_{i+1} = x'$, not on the previous value $X_i = x$. This reflects the sequential nature of the Gibbs sampler.
:::

::: {#thm-Gibbs-Markov-Chain}
## Theorem: Gibbs Chain Stationarity

The Gibbs sampling algorithm generates a Markov Chain with the transition kernel $K$ as described above. The joint distribution $f_{X, Y}$ is a stationary distribution of the Markov Chain. Hence, if the Markov Chain converges to the stationary distribution, the samples generated by the Gibbs algorithm will be distributed according to $f_{X, Y}$.
:::

<!--
::: {.proof}
To show that $f_{X,Y}$ is a stationary distribution, we need to verify that if $(X_i, Y_i) \sim f_{X,Y}$, then $(X_{i+1}, Y_{i+1}) \sim f_{X,Y}$ as well.

Let $(X_i, Y_i)$ be distributed according to $f_{X,Y}$. We want to show that $(X_{i+1}, Y_{i+1})$ obtained through one step of the Gibbs sampler also follows $f_{X,Y}$.

The Gibbs sampler performs the following operations:

1. Sample $X_{i+1} \sim f_{X|Y}(\cdot \mid Y_i).$
2. Sample $Y_{i+1} \sim f_{Y|X}(\cdot \mid X_{i+1}).$

We need to compute the joint density of $(X_{i+1}, Y_{i+1})$. Using the law of total probability and the conditional sampling steps:

$$
f_{X_{i+1}, Y_{i+1}}(x', y') = \int_{\Omega_Y} f_{X_{i+1}, Y_{i+1} \mid Y_i}(x', y' \mid y) f_{Y_i}(y) \, dy
$$

Given $Y_i = y$, the Gibbs steps give us:

- $X_{i+1} \mid Y_i = y \sim f_{X|Y}(\cdot \mid y).$
- $Y_{i+1} \mid X_{i+1} = x', Y_i = y \sim f_{Y|X}(\cdot \mid x').$

Note that $Y_{i+1}$ depends only on $X_{i+1}$, not directly on $Y_i$. Therefore:

$$
f_{X_{i+1}, Y_{i+1} \mid Y_i}(x', y' \mid y) = f_{X|Y}(x' \mid y) f_{Y|X}(y' \mid x')
$$

Substituting back:

$$
\begin{aligned}
f_{X_{i+1}, Y_{i+1}}(x', y') &= \int_{\Omega_Y} f_{X|Y}(x' \mid y) f_{Y|X}(y' \mid x') f_{Y}(y) \, dy \\
&= f_{Y|X}(y' \mid x') \int_{\Omega_Y} f_{X|Y}(x' \mid y) f_{Y}(y) \, dy
\end{aligned}
$$

Using the definition of marginal density:
$$
f_X(x') = \int_{\Omega_Y} f_{X,Y}(x', y) \, dy = \int_{\Omega_Y} f_{X|Y}(x' \mid y) f_Y(y) \, dy
$$

Therefore:
$$
f_{X_{i+1}, Y_{i+1}}(x', y') = f_{Y|X}(y' \mid x') f_X(x') = f_{X,Y}(x', y')
$$

where the last equality follows from the fundamental relationship $f_{X,Y}(x,y) = f_{Y|X}(y \mid x) f_X(x)$.

Thus, $(X_{i+1}, Y_{i+1}) \sim f_{X,Y}$, proving that $f_{X,Y}$ is a stationary distribution of the Gibbs Markov Chain.
$\blacksquare$
:::
-->
::: {.proof}
For simplicity, we assume $f$ represents a discrete distribution, but the proof follows the same structure in the continuous case.

Let $P_{\text{odd}}$ and $P_{\text{even}}$ denote the transition matrices for the two steps of the Gibbs sampler:
- $P_{\text{odd}}$: update $X$ given current $Y$ (keep $Y$ unchanged)
- $P_{\text{even}}$: update $Y$ given current $X$ (keep $X$ unchanged)

One complete Gibbs iteration is $P = P_{\text{even}} P_{\text{odd}}$.

**Step 1:** Show $f_{X,Y}$ is invariant under $P_{\text{odd}}$

The transition matrix $P_{\text{odd}}$ has entries:
$$P_{\text{odd}}[(x',y'), (x,y)] = \begin{cases} 
f_{X|Y}(x \mid y') & \text{if } y' = y \\
0 & \text{if } y' \neq y
\end{cases}$$

This means we can only transition to $(x,y)$ from states $(x',y)$ with the same $y$-coordinate.

Starting from distribution $f_{X,Y}$, the distribution after applying $P_{\text{odd}}$ is:
$$[f_{X,Y} P_{\text{odd}}](x,y) = \sum_{(x',y')} f_{X,Y}(x',y') \cdot P_{\text{odd}}[(x',y'), (x,y)]$$

$$= \sum_{x'} f_{X,Y}(x',y) \cdot f_{X|Y}(x \mid y)$$

$$= f_{X|Y}(x \mid y) \sum_{x'} f_{X,Y}(x',y)$$

$$= f_{X|Y}(x \mid y) \cdot f_Y(y) = f_{X,Y}(x,y)$$

Therefore, $f_{X,Y} P_{\text{odd}} = f_{X,Y}$.

**Step 2:** Show $f_{X,Y}$ is invariant under $P_{\text{even}}$

The transition matrix $P_{\text{even}}$ has entries:
$$P_{\text{even}}[(x',y'), (x,y)] = \begin{cases} 
f_{Y|X}(y \mid x') & \text{if } x' = x \\
0 & \text{if } x' \neq x
\end{cases}$$

Starting from distribution $f_{X,Y}$:
$$[f_{X,Y} P_{\text{even}}](x,y) = \sum_{(x',y')} f_{X,Y}(x',y') \cdot P_{\text{even}}[(x',y'), (x,y)]$$

$$= \sum_{y'} f_{X,Y}(x,y') \cdot f_{Y|X}(y \mid x)$$

$$= f_{Y|X}(y \mid x) \sum_{y'} f_{X,Y}(x,y')$$

$$= f_{Y|X}(y \mid x) \cdot f_X(x) = f_{X,Y}(x,y)$$

Therefore, $f_{X,Y} P_{\text{even}} = f_{X,Y}$.

**Step 3:** Conclude stationarity for complete Gibbs step

Since both individual steps preserve $f_{X,Y}$:
$$f_{X,Y} P = f_{X,Y} P_{\text{even}} P_{\text{odd}} = (f_{X,Y} P_{\text{even}}) P_{\text{odd}} = f_{X,Y} P_{\text{odd}} = f_{X,Y}$$

Therefore, $f_{X,Y}$ is the stationary distribution of the Gibbs sampler.
$\blacksquare$
:::



## Concluding Remarks

We have introduced the Gibbs sampling algorithm as a powerful MCMC method for sampling from joint distributions when the conditional distributions are known and easy to sample from. The key insights from our analysis are:

1. **Algorithmic Simplicity**: Gibbs sampling reduces the complex problem of sampling from a joint distribution to the simpler task of iteratively sampling from conditional distributions.

2. **Theoretical Foundation**: We proved that the joint distribution $f_{X,Y}$ is a stationary distribution of the Markov Chain generated by the Gibbs sampler, ensuring that our samples will asymptotically follow the target distribution.

3. **Practical Examples**: Through the joint exponential and bivariate normal distributions, we demonstrated how the algorithm adapts to different types of conditional distributions—from truncated exponentials to Gaussians.

The examples we studied involved continuous random variables with relatively simple conditional forms. However, the true power of Gibbs sampling becomes apparent when dealing with high-dimensional discrete systems where direct sampling from the joint distribution is computationally intractable.

::: {.callout-note}
## Looking Ahead: The Ising Model
In our next topic, we will apply Gibbs sampling to the **Ising model**—a fundamental model in statistical physics that describes magnetic systems. The Ising model presents several new challenges:

- **Discrete state space**: Each site takes values in $\{-1, +1\}$ rather than continuous values
- **High dimensionality**: We'll work with lattices containing hundreds or thousands of sites
- **Complex dependencies**: Each site's conditional distribution depends on all its neighbors
- **Phase transitions**: The model exhibits different behavior at different temperatures

The Ising model will demonstrate how Gibbs sampling can tackle problems where the joint distribution is known up to a normalization constant, but direct sampling is impossible due to the combinatorial explosion of possible configurations.
:::

The techniques we've developed here—understanding the Markov chain structure, working with conditional distributions, and ensuring proper convergence—will be essential tools as we move to this more complex and practically important application.