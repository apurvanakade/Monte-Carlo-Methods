---
title: "Random Number Generators"

execute:
  echo: false
---

In the problem on estimating the value of $\pi$ using a Monte Carlo method, we encountered three main principles of Monte Carlo simulations:

1. All Monte Carlo simulations require a **source of random numbers**. In our case, we used `np.random.rand()` to generate random numbers. The choice of random number generator can have a significant impact on the quality of the simulation. A poor random number generator can lead to biased results.
2. Our simulation result could be described as **generating samples from a distribution $X_N$** (more precisely generating samples from the pdf of $X_N$) whose mean is $\pi/4$. This is a general principle in Monte Carlo simulations: we generate samples from a distribution whose mean (or some other property) we want to estimate. 
3. It is not enough to find the mean of the distribution. We also need to **estimate the uncertainty** in our estimate. A large uncertainty means that we need to run the simulation multiple times to get a reliable estimate. 

We will keep these principles in mind and revisit them from time to time as we explore more advanced Monte Carlo simulations. For now, let's focus on the first principle: generating random numbers.

## Pseudo-random number generators

A **random number generator** is a function that produces a sequence of numbers that meet certain statistical requirements for randomness. *True* random number generators are based on physical processes that are fundamentally random, such as radioactive decay or thermal noise. Such systems are useful in cryptography and other applications where true randomness is important for security.

Below is an example of a true random number generator based on lava lamps called the "wall of entropy". The lava lamps are used to generate random bits, which are then combined to generate random keys for encryption.

<img src="../images/lava-lamps.jpg" width="600">

There are less exotic ways to generate random numbers. Computer chips have a hardware random number generator that uses thermal noise to generate random bits.

However, true random number generators are slow and expensive. These are not useful for simulations as they are not reproducible. Instead, we use **pseudo-random number generators** (PRNGs) to generate random numbers for simulations.

A **pseudo-random number generator** (PRNG) is a random number generator that produces a sequence of numbers that are not truly random, but are generated by a deterministic algorithm. The sequence of numbers produced by a PRNG is completely determined by the seed: if you know the seed, you can predict the entire sequence of numbers.

The reason for using PRNGs in simulations is to be able to reproduce the results. If you run a simulation with a given seed, you should get the same results every time. This is important for testing, debugging, and for sharing results with others.

We test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. A good PRNG should produce numbers that are indistinguishable from true random numbers. It is said to fool the statistical tests of randomness.

PRNGs need to satisfy several statistical requirements to be useful in simulations. The most important requirements are:

1. **Uniformity**: The numbers generated should be uniformly distributed between 0 and 1.
2. **Independence**: The numbers generated should be independent of each other. Knowing one number should not give you any information about the next number.
3. **Speed**: The PRNG should be fast. Generating random numbers is a common operation in simulations, so the PRNG should be as fast as possible.

These are all difficult requirements to satisfy simultaneously. In practice, most PRNGs are imperfect. You have to choose a PRNG that is appropriate for your application. You have to decide which statistical tests of randomness are most important for your application, and choose a PRNG that satisfies those tests. 

For example, 

1. **Linear congruential generators** are simple and fast, but they have some statistical problems, as you'll see in the exercises. These are good enough if you only need a few random numbers. These were the first PRNGs to be widely used and have stayed popular for a long time because of their simplicity.
2. The **Mersenne Twister** is a widely-used PRNG that is fast and has good statistical properties. It is the default PRNG in many programming languages, including Python. However, it is not suitable for cryptographic applications.
3. **Cryptographically secure PRNGs** are designed to be secure against cryptographic attacks. They are slower than other PRNGs, but they are necessary for applications where security is important. 
 
### Cycle length

In practice, PRNGs generate a random integer between 0 and $M$ for some large number $M$, and then divide by $M$ to get a random number between 0 and 1. The **period** or **cycle length** of a PRNG is the number of random numbers it can generate before it starts repeating itself. A good PRNG should have a long cycle length.

However, relying solely on the cycle length to determine the quality of a PRNG is a mistake. A PRNG can have a long cycle length and still have poor statistical properties. For example, a PRNG that generates the sequence 

$$1, 2, 3, 4, 5, \ldots, M-1, M, 1, 2, 3, 4, 5,$$

has a cycle length of $M$, but it is a terrible PRNG!

### Linear Congruential Generator

A **linear congruential generator (LCG)** is an algorithm that yields a sequence of pseudo-randomized *integers* using a simple recurrence relation. The generator is defined by the recurrence relation:

\begin{equation}
X_{n+1} = (aX_n + c) \mod m
\end{equation}

where:
- $X_n$ is the sequence of pseudo-randomized numbers
- $a$ is the multiplier
- $c$ is the increment

The above equation generates a sequence of integers between 0 and $m-1$. To generate a sequence of random numbers between 0 and 1, we divide the sequence by $m$:

\begin{equation}
  r_n = \frac{X_n}{m}
\end{equation}

LCGs are simple to implement and are computationally efficient. However, as you'll see in the homework, they have some statistical problems. The modulus $m$ is the largest integer that the generator can produce. The modulus $m$ is usually a power of 2, which makes the modulo operation fast. 

The **Hull-Dobell Theorem** states that an LCG will have a full period for all seed values if and only if:

1. $c$ and $m$ are relatively prime,
2. $a - 1$ is divisible by all prime factors of $m$,
3. $a - 1$ is a multiple of 4 if $m$ is a multiple of 4.

In the special case when $m$ is a power of 2, the Hull-Dobell Theorem simplifies to:

1. $c$ is odd,
2. $a$ is congruent to 1 modulo 4.

It is in fact enough to take $c = 1$. Thus when the modulus is a power of 2, a full period LCG will be of the form: 

\begin{equation}
X_{n+1} = (aX_n + 1) \mod 2^b,
\end{equation}

with $a \equiv 1 \mod 4$.

## Statistical test of randomness

As mentioned earlier, we test the quality of a PRNG by running statistical tests on the sequence of numbers it generates. The more tests a PRNG passes, the better it is. There exist many "tests suites" that are used to evaluate the quality of PRNGs such as the Diehard tests, the TestU01 suite, and the NIST Statistical Test Suite. 

We'll look at the following three simple statistical tests of randomness from the NIST Statistical Test Suite:

1. **Chi-square test**: The chi-square test checks whether the observed frequency of the sequence is consistent with the expected frequency. If the sequence is truly random, then the observed frequency should be consistent with the expected frequency.
2. **Mono-bit test**: The mono-bit test checks whether the number of 0s and 1s in the sequence is approximately equal. If the sequence is truly random, then the number of 0s and 1s should be roughly equal.
<!-- 3. **Serial test**: The serial test checks whether the number of 00s, 01s, 10s, and 11s in the sequence are approximately equal. If the sequence is truly random, then the number of 00s, 01s, 10s, and 11s should be roughly equal. -->
1. **Runs test**: The runs test checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. If the sequence is truly random, then the number of runs of 0s and 1s should be consistent with a random sequence.


For each of these tests, we'll set up the null hypothesis and the alternative hypothesis as 

- $H_0$: The sequence is random.
- $H_1$: The sequence is not random.

We'll use the p-value to determine whether to reject the null hypothesis. If the p-value is less than the significance level $\alpha$, we reject the null hypothesis. If the p-value is greater than $\alpha$, we fail to reject the null hypothesis.


### Chi-square test

The chi-square test is a one-sided statistical test that measures how well a sample of data matches a theoretical distribution. The chi-square test is used to test whether the observed data is consistent with the expected data. The test statistic is given by:

\begin{equation}
\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
\end{equation}

where:
- $O_i$ is the observed frequency of the $i$-th bin
- $E_i$ is the expected frequency of the $i$-th bin
- $k$ is the number of bins

In our case of testing the randomness of a sequence of random numbers, we divide the interval $[0, 1]$ into $k$ bins and count the number of random numbers that fall into each bin. The expected frequency of each bin is $n/k$, where $n$ is the total number of random numbers.

The chi-square test is used to test the null hypothesis that the observed data is consistent with the expected data. If the chi-square test statistic is large, then the null hypothesis is rejected. 

The critical value of the chi-square test statistic depends on the number of degrees of freedom. The degrees of freedom is given by $k-1$. In python, you can use the `scipy.stats.chi2.ppf` function to perform the chi-square test.

The chi-square test is sensitive to the number of bins $k$. If $k$ is too small, then the test may not be sensitive enough to detect deviations from the expected distribution. If $k$ is too large, then the test may be too sensitive and may detect deviations that are not significant.


```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2
from scipy.stats import norm

# Generate x values and plot chi-square pdf
x = np.linspace(0, 20, 1000)
y = chi2.pdf(x, 3)

# Plot chi-square pdf
plt.plot(x, y, 'r-', lw=2)
plt.title('Chi-square pdf with 3 degrees of freedom')
plt.xlabel('x')
plt.ylabel('pdf')

# Draw vertical and horizontal lines at 0
plt.axvline(0, color='k', linestyle='-')
plt.axhline(0, color='k', linestyle='-')

# Calculate critical value for 95% confidence interval
alpha = 0.05
critical_value = chi2.ppf(1 - alpha, 3)

# Add critical value annotation to the plot
plt.text(critical_value, 0.05, f'Critical value for 5% significance: {critical_value:.2f}')

# Shade the area to the right of the critical value
x_fill = np.linspace(critical_value, 20, 1000)
y_fill = chi2.pdf(x_fill, 3)
plt.fill_between(x_fill, y_fill, color='blue', alpha=0.5)

# Draw critical value on the plot
plt.axvline(critical_value, color='g', linestyle='--')

# Display explanatory information
print("To find the critical value, we find $x$ such that $P(X > x) = 0.05$ for a chi-square distribution with 3 degrees of freedom. In the figure below, this is the area to the right of the green dashed line.")
print("If your chi-square value is greater than the critical value, you can reject the null hypothesis.")

# Show the plot
plt.show()
```

### Mono-bit test

The mono-bit test is a two-sided statistical test that checks whether the number of 0s and 1s in the sequence is approximately equal. In a uniform binary sequence, roughly half the bits are 0s and half the bits are 1s. 

Let $X_i$ be the $i$-th bit in the sequence. Then $X_i$ is a Bernoulli random variable with probability $p = 0.5$. Because the rv's $X_i$ are i.i.d., by the \emph{central limit theorem}, their average

\begin{equation}
  X = \dfrac{X_1 + X_2 + \cdots + X_k}{k}
\end{equation}

approaches a normal distribution with mean $0.5$ and variance $1/(4k)$ as $k$ approaches infinity. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by

\begin{equation}
  Z = \dfrac{X - 0.5}{\sqrt{1/(4k)}}.
\end{equation}

The critical value of the z-test statistic depends on the significance level $\alpha$. In python, you can use the `scipy.stats.norm.ppf` function to perform the z-test.
Note that because this is a two-sided test, we reject the null hypothesis if $Z > z_{\alpha/2}$ or $Z < -z_{\alpha/2}$ where $z_{\alpha/2}$ is the critical value of the z-test statistic at the significance level $\alpha/2$.

```{python}
#| echo: false

# Generate x values and plot normal pdf
x = np.linspace(-4, 4, 1000)
y = norm.pdf(x, 0, 1)

# Plot normal pdf
plt.plot(x, y, 'r-', lw=2)
plt.title('Standard normal pdf')
plt.xlabel('x')
plt.ylabel('pdf')

# Draw vertical and horizontal lines at 0
plt.axvline(0, color='k', linestyle='-')
plt.axhline(0, color='k', linestyle='-')

# Calculate critical value for 95% confidence interval
alpha = 0.05
critical_value_1 = norm.ppf(1 - alpha/2)
critical_value_2 = norm.ppf(alpha/2)

# Add critical value annotations to the plot
plt.text(critical_value_1, 0.25, f'x = {critical_value_1:.2f}')
plt.text(critical_value_2, 0.25, f'x = {critical_value_2:.2f}')

# Shade the area to the right of the positive critical value
x_fill = np.linspace(critical_value_1, 4, 1000)
y_fill = norm.pdf(x_fill, 0, 1)
plt.fill_between(x_fill, y_fill, color='blue', alpha=0.5)

# Shade the area to the left of the negative critical value
x_fill = np.linspace(-4, critical_value_2, 1000)
y_fill = norm.pdf(x_fill, 0, 1)
plt.fill_between(x_fill, y_fill, color='blue', alpha=0.5)

# Draw critical value on the plot
plt.axvline(critical_value_1, color='g', linestyle='--')
plt.axvline(critical_value_2, color='g', linestyle='--')

print("To find the critical value, we find $x$ such that $P(-x < X < x) = 0.95$ for a standard normal distribution. In the figure below, this is the area between the green dashed lines.")
print("If your z-score is outside of the critical values, you can reject the null hypothesis.")
```

### Runs test

The Wald-Wolfowitz runs test is a two-sided statistical test that checks whether the number of runs of 0s and 1s in the sequence is consistent with a random sequence. A run is a sequence of consecutive 0s or 1s. In a random sequence, the number of runs of 0s and 1s should be consistent with a random sequence.

For example, in the sequence 0011100110, there are 5 runs: 00, 111, 00, 11, 0. The runs test checks whether the number of runs of 0s and 1s is consistent with a random sequence.

Consider a binary sequence of length $k$.
Define a random variable 

\begin{equation}
  Z_i = \begin{cases}
  1 & \text{ if the ${i+1}^{st}$ bit is different from the $i^{th}$ bit} \\
  0 & \text{ otherwise}
  \end{cases}
\end{equation}

One can check that the number of runs in the sequence is given by the random variable 

\begin{equation}
  R = 1 + \sum_{i=1}^{k-1} Z_i.
\end{equation}

Note that the random variables $Z_i$ are independent. Hence, 

\begin{align*}
  \text{E}[R]
  &= 1 + \sum_{i=1}^{k-1} \text{E}[Z_i] \\
  \text{Var}[R] 
  &= \sum_{i=1}^{k-1} \text{Var}[Z_i].
\end{align*}

The calculation of this expectation and variance is left as an exercise.


We assume that for large $k$ the number of runs is approximately normally distributed. We can hence use the z-test to test the null hypothesis that the sequence is random. The z-test statistic is given by

\begin{equation}
  Z = \dfrac{R - E[R]}{\sigma[R]}.
\end{equation}

We reject the null hypothesis if $Z > z_{\alpha/2}$ or $Z < -z_{\alpha/2}$ where $z_{\alpha/2}$ is the critical value of the z-test statistic at the significance level $\alpha/2$. Note that here we use $\alpha/2$ instead of $\alpha$ because this is a two-sided test.

### Spectral test

The previous tests fail to detect some of the problems with generating vectors using LCGs. One test for detecting these problems is the spectral test. The spectral test uses the Fast Fourier Transform (FFT) to analyze the spectral properties of the sequence. This test is beyond the scope of this class but you can see some examples in the homework. (This would be a good topic for a project!)

### Final remarks

Note that *by definition*, a PRNG is not "random" in an absolute sense. The PRNG used in Python, the Mersenne Twister, is sufficiently random for most applications involving simulations. However, it is not suitable for cryptographic applications as it is possible to predict the entire sequence of numbers if you know a limited set of numbers. For cryptographic applications, you should use a cryptographically secure PRNG whose future numbers cannot be predicted easily from past numbers.
