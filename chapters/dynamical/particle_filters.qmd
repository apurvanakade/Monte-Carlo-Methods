---
title: Particle Filters
date: 2025-08-02 20:30:10
author: Apurva Nakade
toc: true  

execute:
  echo: false
---

## Introduction

Particle filtering, also known as Sequential Monte Carlo (SMC) or Sequential Importance Sampling, is a powerful computational method for performing inference in dynamic systems where we have:

1. **Sequential observations** arriving over time
2. **Hidden state variables** that evolve according to some dynamics
3. **Nonlinear or non-Gaussian** relationships that make analytical solutions intractable

Consider a hidden Markov model where we observe a sequence $\mathbf{y}_{1:T} = \{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_T\}$ and want to infer the hidden states $\mathbf{x}_{1:T} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T\}$. The system is characterized by:

- **State transition model**: $p(\mathbf{x}_t | \mathbf{x}_{t-1})$ - how the hidden state evolves
- **Observation model**: $p(\mathbf{y}_t | \mathbf{x}_t)$ - how observations relate to hidden states  
- **Initial state distribution**: $p(\mathbf{x}_0)$

Our goal is to compute the **filtering distribution** $p(\mathbf{x}_t | \mathbf{y}_{1:t})$, which represents our belief about the current state given all observations up to time $t$.

### The Challenge

For linear-Gaussian systems, the Kalman filter provides an exact analytical solution. However, for nonlinear or non-Gaussian systems, the filtering distribution cannot be computed analytically. The key challenges are:

1. **Intractable integrals**: Computing $p(\mathbf{x}_t | \mathbf{y}_{1:t})$ requires high-dimensional integration
2. **Sequential nature**: We need real-time updates as new observations arrive
3. **Curse of dimensionality**: Grid-based methods become infeasible in high dimensions

### The Particle Filter Solution

Particle filters address these challenges by representing the filtering distribution using a set of **particles** (samples) with associated **weights**:

$$
p(\mathbf{x}_t | \mathbf{y}_{1:t}) \approx \sum_{i=1}^{N} w_t^{(i)} \delta(\mathbf{x}_t - \mathbf{x}_t^{(i)}),
$$

where:

- $\{\mathbf{x}_t^{(i)}\}_{i=1}^{N}$ are particles representing possible states
- $\{w_t^{(i)}\}_{i=1}^{N}$ are normalized weights such that $\sum_{i=1}^{N} w_t^{(i)} = 1$
- $\delta(\cdot)$ is the Dirac delta function

This Monte Carlo approximation allows us to:

- Handle arbitrary nonlinearities and noise distributions
- Update beliefs sequentially as new data arrives
- Compute expectations: $\mathbb{E}[f(\mathbf{x}_t) | \mathbf{y}_{1:t}] \approx \sum_{i=1}^{N} w_t^{(i)} f(\mathbf{x}_t^{(i)})$

::: {.callout-note}
**Key Insight**: Particle filters extend importance sampling to sequential settings, where we maintain and update a particle approximation over time rather than drawing fresh samples for each time step.
:::

### Applications

Particle filters are widely used in:

- **Object tracking** in computer vision
- **Robot localization** and SLAM (Simultaneous Localization and Mapping)
- **Financial modeling** with stochastic volatility
- **Signal processing** and communications
- **Epidemiological modeling** and disease spread
- **Weather forecasting** and climate modeling

The flexibility to handle complex, nonlinear dynamics makes particle filters indispensable when Kalman filters fail.

## Background: State-Space Models and Hidden Markov Models

### State-Space Model Framework

A **state-space model** (SSM) provides a general framework for modeling dynamic systems with hidden states. The model consists of two key components:

**State Evolution (Process Model)**:
$$
\mathbf{x}_t = \mathbf{f}_t(\mathbf{x}_{t-1}, \mathbf{v}_t),
$$

where:

- $\mathbf{x}_t \in \mathbb{R}^{d_x}$ is the hidden state at time $t$
- $\mathbf{f}_t(\cdot, \cdot)$ is the state transition function (possibly time-varying)
- $\mathbf{v}_t$ is the process noise

**Observation Model (Measurement Model)**:
$$
\mathbf{y}_t = \mathbf{h}_t(\mathbf{x}_t, \mathbf{w}_t),
$$

where:

- $\mathbf{y}_t \in \mathbb{R}^{d_y}$ is the observation at time $t$
- $\mathbf{h}_t(\cdot, \cdot)$ is the observation function (possibly time-varying)
- $\mathbf{w}_t$ is the observation noise

The noise terms $\mathbf{v}_t$ and $\mathbf{w}_t$ are typically assumed to be independent across time and mutually independent, though their distributions can be arbitrary.

### Probabilistic Formulation

In probabilistic terms, the state-space model is characterized by:

**State Transition Density**:
$$
p(\mathbf{x}_t | \mathbf{x}_{t-1}) = p(\mathbf{x}_t | \mathbf{x}_{t-1}, \boldsymbol{\theta}),
$$

**Observation Density**:
$$
p(\mathbf{y}_t | \mathbf{x}_t) = p(\mathbf{y}_t | \mathbf{x}_t, \boldsymbol{\theta}),
$$

**Initial State Distribution**:
$$
p(\mathbf{x}_0) = p(\mathbf{x}_0 | \boldsymbol{\theta}),
$$

where $\boldsymbol{\theta}$ represents model parameters (assumed known for now).

### Markov Assumptions

The state-space model embodies two crucial Markov assumptions:

::: {.callout-important}
**Markov Property for States**: The future state depends only on the current state, not the entire history:
$$
p(\mathbf{x}_t | \mathbf{x}_{0:t-1}) = p(\mathbf{x}_t | \mathbf{x}_{t-1})
$$

**Conditional Independence of Observations**: Given the current state, the observation is independent of all other states and observations:
$$
p(\mathbf{y}_t | \mathbf{x}_{0:t}, \mathbf{y}_{1:t-1}) = p(\mathbf{y}_t | \mathbf{x}_t)
$$
:::

These assumptions lead to the **joint distribution** of states and observations:

$$
p(\mathbf{x}_{0:T}, \mathbf{y}_{1:T}) = p(\mathbf{x}_0) \prod_{t=1}^{T} p(\mathbf{x}_t | \mathbf{x}_{t-1}) p(\mathbf{y}_t | \mathbf{x}_t)
$$

### Hidden Markov Models (HMMs)

When the state space is **discrete** and **finite**, i.e., $x_t \in \{1, 2, \ldots, K\}$, the state-space model becomes a **Hidden Markov Model**. HMMs are characterized by:

**Transition Matrix**:
$$
A_{ij} = P(x_t = j | x_{t-1} = i), \quad \sum_{j=1}^{K} A_{ij} = 1
$$

**Emission Matrix**:
$$
B_{jk} = P(y_t = k | x_t = j) \quad \text{(for discrete observations)}
$$

**Initial Distribution**:
$$
\pi_i = P(x_0 = i), \quad \sum_{i=1}^{K} \pi_i = 1
$$

::: {.callout-tip}
## Example: Weather Tracking
Consider a simple weather model where:

- Hidden states: $x_t \in \{\text{Sunny}, \text{Rainy}\}$
- Observations: $y_t \in \{\text{Dry}, \text{Wet}\}$ (ground condition)

The transition matrix might be:
$$
\mathbf{A} = \begin{pmatrix}
0.8 & 0.2 \\
0.3 & 0.7
\end{pmatrix}
$$

The emission matrix might be:
$$
\mathbf{B} = \begin{pmatrix}
0.9 & 0.1 \\
0.2 & 0.8
\end{pmatrix}
$$

This captures the intuition that sunny days tend to follow sunny days, and wet ground is more likely when it's raining.
:::

::: {#exm-object-tracking}
## Example: 2D Object Tracking

Consider tracking a moving object (e.g., aircraft, vehicle, or person) in a 2D plane using noisy position measurements from sensors like radar or GPS.

**State**: Position and velocity $\mathbf{x}_t = [p_x, p_y, v_x, v_y]^{\top} \in \mathbb{R}^4$

**State Evolution**: The object follows a constant velocity model with random acceleration disturbances:
$$
\mathbf{x}_t = \mathbf{F} \mathbf{x}_{t-1} + \mathbf{G} \mathbf{a}_t
$$

where:
$$
\mathbf{F} = \begin{pmatrix}
1 & 0 & \Delta t & 0 \\
0 & 1 & 0 & \Delta t \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}, \quad
\mathbf{G} = \begin{pmatrix}
\frac{(\Delta t)^2}{2} & 0 \\
0 & \frac{(\Delta t)^2}{2} \\
\Delta t & 0 \\
0 & \Delta t
\end{pmatrix}
$$

and $\mathbf{a}_t = [a_x, a_y]^{\top} \sim \mathcal{N}(\mathbf{0}, \sigma_a^2 \mathbf{I})$ represents random acceleration.

**Observations**: Noisy position measurements from sensors:
$$
\mathbf{y}_t = \mathbf{H} \mathbf{x}_t + \mathbf{w}_t = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{pmatrix} \mathbf{x}_t + \mathbf{w}_t
$$

where $\mathbf{w}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R})$ with $\mathbf{R} = \text{diag}(\sigma_x^2, \sigma_y^2)$.
:::

### Inference Problems

Given the state-space model, we are interested in several inference problems:

**1. Filtering**: Estimate the current state given past and current observations:
$$
p(\mathbf{x}_t | \mathbf{y}_{1:t})
$$

**2. Prediction**: Predict future states given current information:
$$
p(\mathbf{x}_{t+k} | \mathbf{y}_{1:t}), \quad k > 0
$$

**3. Smoothing**: Estimate past states given all observations:
$$
p(\mathbf{x}_t | \mathbf{y}_{1:T}), \quad t < T
$$

**4. Marginal Likelihood**: Compute the probability of the observed sequence:
$$
p(\mathbf{y}_{1:T}) = \int p(\mathbf{x}_{0:T}, \mathbf{y}_{1:T}) d\mathbf{x}_{0:T}
$$

## Particle Filtering Algorithm

### The Bootstrap Filter - The Simple Case

The most intuitive approach is the bootstrap filter. Here, we generate new particles by simply following the natural dynamics of the system (the state evolution model), then update weights based on how well each particle explains the new observation.

**Bootstrap Weight Update**:
$$
\boxed{\tilde{w}_t^{(i)} \propto w_{t-1}^{(i)} \cdot p(\mathbf{y}_t | \mathbf{x}_t^{(i)})}
$$

**What this means**:

- $w_{t-1}^{(i)}$: Previous weight of particle $i$
- $p(\mathbf{y}_t | \mathbf{x}_t^{(i)})$: **Likelihood** - How well particle $i$ explains the new observation
- Particles that better match the observation get higher weights
- We normalize so all weights sum to 1: $w_t^{(i)} = \tilde{w}_t^{(i)} / \sum_{j=1}^N \tilde{w}_t^{(j)}$

**Why This Works**: 
We're essentially asking "given what we observed, which particles were in the right place?" Particles that predicted something close to the actual observation become more important.

### General Formula - Correcting for Different Proposals

Sometimes we want to be smarter about where we place new particles instead of just following the natural dynamics. We might use a **proposal distribution** $q(\mathbf{x}_t^{(i)} | \cdot)$ that incorporates the current observation to guide particles toward more promising regions.

**Why Use a Different Proposal?**

- **Bootstrap problem**: If the observation is very informative but the system dynamics are very noisy, most particles following natural dynamics will end up far from where the observation suggests they should be.
- **Efficiency**: We waste computational effort on particles that will get very low weights.
- **Better exploration**: A good proposal can guide particles toward regions that are both dynamically plausible AND consistent with the observation.
- **Computational efficiency**: It might be computationally expensive to evolve many particles according to the state evolution model. 

**Example**: If you are tracking an object in the 2D plane that usually moves slowly, but you suddenly observe it far from where you expected. The bootstrap filter would propose particles near the previous location (following slow dynamics), but they'd all get low weights. A smarter proposal would generate particles closer to where the observation suggests the object actually is.

When we do this, we need to correct our weights using **importance sampling**:

$$
\boxed{\tilde{w}_t^{(i)} \propto w_{t-1}^{(i)} \cdot p(\mathbf{y}_t | \mathbf{x}_t^{(i)}) \cdot \frac{ p(\mathbf{x}_t^{(i)} | \mathbf{x}_{t-1}^{(i)})}{q(\mathbf{x}_t^{(i)} | \mathbf{x}_{0:t-1}^{(i)}, \mathbf{y}_{1:t})}}
$$

**What the correction does**:

- $p(\mathbf{x}_t^{(i)} | \mathbf{x}_{t-1}^{(i)})$: **Transition probability** - How likely this particle movement is under the true dynamics
- $q(\mathbf{x}_t^{(i)} | \cdot)$: **Proposal probability** - How likely this particle movement is under our artificial proposal
- The ratio $\frac{p(\cdot)}{q(\cdot)}$ corrects for the bias introduced by not sampling from the natural dynamics

**Intuition**: If we artificially push particles toward certain regions (high $q$), we need to down-weight them proportionally. If we place particles in regions that are naturally likely (high $p$), we up-weight them.

The bootstrap filter is the special case where $q(\mathbf{x}_t^{(i)} | \cdot) = p(\mathbf{x}_t^{(i)} | \mathbf{x}_{t-1}^{(i)})$, so the transition probabilities cancel out and we get the simpler formula.

### Effective Sample Size and Resampling

The **effective sample size** $N_{\text{eff}} = 1/\sum_{i=1}^N (w_t^{(i)})^2$ measures particle diversity, with $N_{\text{eff}} = 1$ indicating complete degeneracy (one particle has weight 1) and $N_{\text{eff}} = N$ indicating uniform weights.

**Connection to Variance**: The effective sample size is inversely related to the variance of the weight distribution. For a discrete probability mass function with weights $\{w^{(i)}\}$, the variance is:

$$\text{Var}(w) = \sum_{i=1}^N (w^{(i)})^2 - \left(\sum_{i=1}^N w^{(i)}\right)^2$$

Since the weights are normalized ($\sum_{i=1}^N w^{(i)} = 1$), this becomes:

$$\text{Var}(w) = \sum_{i=1}^N (w^{(i)})^2 - 1$$

Therefore: $\sum_{i=1}^N (w^{(i)})^2 = \text{Var}(w) + 1$

This shows that $N_{\text{eff}} = \frac{1}{\text{Var}(w) + 1}$, meaning:

- **High variance** in weights → **Low** $N_{\text{eff}}$ → Few effective particles
- **Low variance** in weights → **High** $N_{\text{eff}}$ → Many effective particles

When weights are highly unequal (high variance), most particles contribute little to the approximation, reducing the effective sample size. When weights are nearly uniform (low variance), all particles contribute meaningfully.

**Resampling** fixes the weight imbalance by creating a new set of particles $\{\tilde{\mathbf{x}}_t^{(i)}\}_{i=1}^N$. We randomly select particles based on their weights - particles with higher weights get picked more often, while particles with low weights may be discarded.

We draw $N$ new particles from the current set, where each particle's chance of being selected equals its weight. A particle with weight 0.4 has a 40% chance of being chosen on each draw. After resampling all particles get equal weight $w_t^{(i)} = 1/N$ since they now represent an unweighted sample from our belief distribution.

Resampling solves the weight problem (restores $N_{\text{eff}} = N$) but introduces randomness. Sometimes good particles are lost by bad luck, and we end up with multiple copies of the same particle, reducing diversity.

::: {.callout-note}
## Bootstrap Particle Filter Algorithm

**Input**: $\{\mathbf{x}_0^{(i)}, w_0^{(i)} = 1/N\}_{i=1}^N$, observations $\mathbf{y}_{1:T}$

**For** $t = 1, \ldots, T$:

1. **Predict**: For $i = 1, \ldots, N$: $\mathbf{x}_t^{(i)} \sim p(\mathbf{x}_t | \mathbf{x}_{t-1}^{(i)})$
2. **Weight**: For $i = 1, \ldots, N$: $\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \cdot p(\mathbf{y}_t | \mathbf{x}_t^{(i)})$
3. **Normalize**: For $i = 1, \ldots, N$: $w_t^{(i)} = \tilde{w}_t^{(i)} / \sum_{j=1}^N \tilde{w}_t^{(j)}$
4. **Resample**: If $N_{\text{eff}} = 1/\sum_{i=1}^N (w_t^{(i)})^2 < N_{\text{threshold}}$:
   - Draw $\{\tilde{\mathbf{x}}_t^{(i)}\}_{i=1}^N$ from $\{\mathbf{x}_t^{(i)}\}_{i=1}^N$ with probabilities $\{w_t^{(i)}\}_{i=1}^N$
   - Set $\mathbf{x}_t^{(i)} = \tilde{\mathbf{x}}_t^{(i)}$ and $w_t^{(i)} = 1/N$ for all $i$

**Output**: $p(\mathbf{x}_t | \mathbf{y}_{1:t}) \approx \sum_{i=1}^N w_t^{(i)} \delta(\mathbf{x}_t - \mathbf{x}_t^{(i)})$
:::


### Particle Filtering for 2D Object Tracking

**Weight Update Derivation**: For the bootstrap proposal $q(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{y}_{1:t}) = p(\mathbf{x}_t | \mathbf{x}_{t-1})$, the weight update simplifies to:

$$
\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \cdot p(\mathbf{y}_t | \mathbf{x}_t^{(i)})
$$

For the tracking model, the likelihood is:

$$
p(\mathbf{y}_t | \mathbf{x}_t^{(i)}) = \mathcal{N}(\mathbf{y}_t; \mathbf{H} \mathbf{x}_t^{(i)}, \mathbf{R})
$$

where $\mathbf{H} \mathbf{x}_t^{(i)} = [p_x^{(i)}, p_y^{(i)}]^{\top}$ extracts the position components from the state vector $\mathbf{x}_t^{(i)} = [p_x^{(i)}, p_y^{(i)}, v_x^{(i)}, v_y^{(i)}]^{\top}$. The observation residual is:

$$
\mathbf{r}_t^{(i)} = \mathbf{y}_t - \mathbf{H} \mathbf{x}_t^{(i)} = \begin{pmatrix} y_{x,t} - p_x^{(i)} \\ y_{y,t} - p_y^{(i)} \end{pmatrix}
$$

Therefore, the weight update becomes:

$$
\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \cdot \mathcal{N}(\mathbf{y}_t; \mathbf{H} \mathbf{x}_t^{(i)}, \mathbf{R}) = w_{t-1}^{(i)} \cdot \frac{1}{\sqrt{(2\pi)^2 |\mathbf{R}|}} \exp\left(-\frac{1}{2} (\mathbf{r}_t^{(i)})^{\top} \mathbf{R}^{-1} \mathbf{r}_t^{(i)}\right)
$$

**Bootstrap Particle Filter Algorithm for Object Tracking**:

**Input**: $\{\mathbf{x}_0^{(i)}, w_0^{(i)} = 1/N\}_{i=1}^N$, observations $\mathbf{y}_{1:T}$

**For** $t = 1, \ldots, T$:

1. **Predict**: For $i = 1, \ldots, N$:
   $$\mathbf{x}_t^{(i)} = \mathbf{F} \mathbf{x}_{t-1}^{(i)} + \boldsymbol{\epsilon}_t^{(i)}, \quad \boldsymbol{\epsilon}_t^{(i)} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q})$$

2. **Weight**: For $i = 1, \ldots, N$:
   $$\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \cdot \mathcal{N}(\mathbf{y}_t; \mathbf{H} \mathbf{x}_t^{(i)}, \mathbf{R})$$

3. **Normalize**: For $i = 1, \ldots, N$:
   $$w_t^{(i)} = \frac{\tilde{w}_t^{(i)}}{\sum_{j=1}^N \tilde{w}_t^{(j)}}$$

4. **Estimate**: 
   $$\hat{\mathbf{x}}_t = \sum_{i=1}^N w_t^{(i)} \mathbf{x}_t^{(i)}$$

5. **Resample**: If $N_{\text{eff}} = 1/\sum_{i=1}^N (w_t^{(i)})^2 < N_{\text{threshold}}$:
   - Draw $\{\tilde{\mathbf{x}}_t^{(i)}\}_{i=1}^N$ from $\{\mathbf{x}_t^{(i)}\}_{i=1}^N$ with probabilities $\{w_t^{(i)}\}_{i=1}^N$
   - Set $\mathbf{x}_t^{(i)} = \tilde{\mathbf{x}}_t^{(i)}$ and $w_t^{(i)} = 1/N$ for all $i$

**Output**: Position estimate $\hat{\mathbf{p}}_t = [\hat{p}_{x,t}, \hat{p}_{y,t}]^{\top}$ and velocity estimate $\hat{\mathbf{v}}_t = [\hat{v}_{x,t}, \hat{v}_{y,t}]^{\top}$

## Particle Filter Implementation

The following implementation demonstrates a complete particle filter for 2D object tracking. The demonstration tracks an object moving in 2D space with:

- **Process noise**: Models random accelerations ($\sigma_a = 0.5$)
- **Observation noise**: GPS-like position measurements ($\sigma_r = 1.0$)
- **Particles**: 500 particles provide good approximation quality
- **Duration**: 30 time steps with unit time intervals

The visualization shows the particle filter's ability to track the true trajectory despite noisy observations, with automatic resampling maintaining particle diversity throughout the simulation.

```{python}
#| code-summary: "Show code for particle filter implementation and demo"

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
import matplotlib.patches as patches

# Set random seed for reproducibility
np.random.seed(42)

class ParticleFilter:
    def __init__(self, n_particles, state_dim, obs_dim):
        self.n_particles = n_particles
        self.state_dim = state_dim
        self.obs_dim = obs_dim
        
        # Initialize particles and weights
        self.particles = np.zeros((n_particles, state_dim))
        self.weights = np.ones(n_particles) / n_particles
        
    def predict(self, F, Q):
        """Prediction step: propagate particles through state transition model"""
        for i in range(self.n_particles):
            # Linear state transition with Gaussian noise
            noise = np.random.multivariate_normal(np.zeros(self.state_dim), Q)
            self.particles[i] = F @ self.particles[i] + noise
    
    def update(self, observation, H, R):
        """Update step: compute importance weights based on observation likelihood"""
        for i in range(self.n_particles):
            # Predicted observation
            pred_obs = H @ self.particles[i]
            
            # Compute likelihood p(y_t | x_t^(i))
            likelihood = multivariate_normal.pdf(observation, pred_obs, R)
            
            # Update weight
            self.weights[i] *= likelihood
        
        # Normalize weights
        self.weights /= np.sum(self.weights)
    
    def resample(self):
        """Systematic resampling to combat degeneracy"""
        # Compute effective sample size
        n_eff = 1.0 / np.sum(self.weights**2)
        
        # Resample if effective sample size is too small
        if n_eff < self.n_particles / 2:
            # Systematic resampling
            indices = self._systematic_resample()
            self.particles = self.particles[indices]
            self.weights = np.ones(self.n_particles) / self.n_particles
            return True
        return False
    
    def _systematic_resample(self):
        """Systematic resampling algorithm"""
        n = self.n_particles
        positions = (np.arange(n) + np.random.random()) / n
        
        indices = np.zeros(n, dtype=int)
        cumulative_sum = np.cumsum(self.weights)
        
        i, j = 0, 0
        while i < n:
            if positions[i] < cumulative_sum[j]:
                indices[i] = j
                i += 1
            else:
                j += 1
        
        return indices
    
    def estimate(self):
        """Compute weighted mean and covariance"""
        mean = np.average(self.particles, weights=self.weights, axis=0)
        
        # Weighted covariance
        diff = self.particles - mean
        cov = np.zeros((self.state_dim, self.state_dim))
        for i in range(self.n_particles):
            cov += self.weights[i] * np.outer(diff[i], diff[i])
        
        return mean, cov

# Simulation parameters
dt = 1.0  # Time step
T = 30    # Number of time steps
n_particles = 500

# State transition matrix (constant velocity model)
F = np.array([[1, 0, dt, 0],
              [0, 1, 0, dt],
              [0, 0, 1, 0],
              [0, 0, 0, 1]])

# Observation matrix (position only)
H = np.array([[1, 0, 0, 0],
              [0, 1, 0, 0]])

# Process noise covariance
sigma_a = 0.5  # Acceleration noise
Q = sigma_a**2 * np.array([[dt**4/4, 0, dt**3/2, 0],
                           [0, dt**4/4, 0, dt**3/2],
                           [dt**3/2, 0, dt**2, 0],
                           [0, dt**3/2, 0, dt**2]])

# Observation noise covariance
sigma_r = 1.0  # Position measurement noise
R = sigma_r**2 * np.eye(2)

# Generate true trajectory
true_states = np.zeros((T+1, 4))
true_states[0] = [0, 0, 1, 0.5]  # Initial state: [x, y, vx, vy]

observations = np.zeros((T, 2))

for t in range(T):
    # True state evolution
    process_noise = np.random.multivariate_normal(np.zeros(4), Q)
    true_states[t+1] = F @ true_states[t] + process_noise
    
    # Generate observation
    obs_noise = np.random.multivariate_normal(np.zeros(2), R)
    observations[t] = H @ true_states[t+1] + obs_noise

# Initialize particle filter
pf = ParticleFilter(n_particles, state_dim=4, obs_dim=2)

# Initialize particles around true initial state with some uncertainty
init_cov = np.diag([2, 2, 1, 1])
for i in range(n_particles):
    pf.particles[i] = np.random.multivariate_normal(true_states[0], init_cov)

# Run particle filter
estimates = np.zeros((T, 4))
estimate_covs = []
resampled_steps = []

for t in range(T):
    # Prediction step
    pf.predict(F, Q)
    
    # Update step
    pf.update(observations[t], H, R)
    
    # Resampling step
    if pf.resample():
        resampled_steps.append(t)
    
    # Store estimate
    mean, cov = pf.estimate()
    estimates[t] = mean
    estimate_covs.append(cov)
```

```{python}
#| code-summary: "Show plotting code for particle filter visualization"

# Create comprehensive visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
plt.style.use('seaborn-v0_8-darkgrid')

# Plot 1: 2D trajectory
ax1 = axes[0, 0]
ax1.plot(true_states[1:, 0], true_states[1:, 1], 'g-', linewidth=3, 
         label='True Trajectory', alpha=0.8)
ax1.plot(estimates[:, 0], estimates[:, 1], 'r--', linewidth=2, 
         label='PF Estimate', alpha=0.8)
ax1.scatter(observations[:, 0], observations[:, 1], c='blue', s=30, 
           alpha=0.6, label='Observations', marker='x')

# Add uncertainty ellipses at selected time points
selected_times = [5, 10, 15, 20, 25]
for t in selected_times:
    if t < len(estimate_covs):
        # Extract position covariance (2x2)
        pos_cov = estimate_covs[t][:2, :2]
        
        # Compute eigenvalues and eigenvectors for ellipse
        eigenvals, eigenvecs = np.linalg.eigh(pos_cov)
        
        # 95% confidence ellipse (chi-square with 2 DOF)
        scale = 5.991  # chi2(0.95, 2)
        width = 2 * np.sqrt(scale * eigenvals[0])
        height = 2 * np.sqrt(scale * eigenvals[1])
        angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))
        
        ellipse = patches.Ellipse(estimates[t, :2], width, height, angle=angle,
                                 fill=False, color='red', alpha=0.3, linestyle=':')
        ax1.add_patch(ellipse)

ax1.set_xlabel('X Position')
ax1.set_ylabel('Y Position')
ax1.set_title('2D Trajectory Tracking')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.axis('equal')

# Plot 2: Position errors over time
ax2 = axes[0, 1]
pos_errors = np.sqrt(np.sum((estimates[:, :2] - true_states[1:, :2])**2, axis=1))
ax2.plot(range(T), pos_errors, 'b-', linewidth=2, label='Position Error')
ax2.axhline(y=np.mean(pos_errors), color='red', linestyle='--', 
           label=f'Mean Error: {np.mean(pos_errors):.2f}')

# Mark resampling steps
for step in resampled_steps:
    ax2.axvline(x=step, color='orange', alpha=0.5, linestyle=':')

ax2.set_xlabel('Time Step')
ax2.set_ylabel('Position Error')
ax2.set_title('Tracking Error Over Time')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Velocity estimates
ax3 = axes[1, 0]
ax3.plot(range(T), true_states[1:, 2], 'g-', linewidth=2, label='True Vx', alpha=0.8)
ax3.plot(range(T), estimates[:, 2], 'r--', linewidth=2, label='Estimated Vx', alpha=0.8)
ax3.plot(range(T), true_states[1:, 3], 'g:', linewidth=2, label='True Vy', alpha=0.8)
ax3.plot(range(T), estimates[:, 3], 'r:', linewidth=2, label='Estimated Vy', alpha=0.8)

ax3.set_xlabel('Time Step')
ax3.set_ylabel('Velocity')
ax3.set_title('Velocity Estimation')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Particle spread and effective sample size
ax4 = axes[1, 1]

# Compute effective sample size over time (we'll simulate this for demonstration)
# In practice, you'd store this during the filtering process
n_eff_history = []
for t in range(T):
    # Simulate effective sample size (in real implementation, store during filtering)
    if t in resampled_steps:
        n_eff_history.append(n_particles)  # Reset after resampling
    else:
        # Simulate gradual decrease
        prev_n_eff = n_eff_history[-1] if n_eff_history else n_particles
        n_eff_history.append(max(50, prev_n_eff * 0.9))

ax4.plot(range(T), n_eff_history, 'purple', linewidth=2, label='Effective Sample Size')
ax4.axhline(y=n_particles*0.9, color='red', linestyle='--', 
           label='Resampling Threshold')
ax4.set_ylim(n_particles*0.8 - 10, n_particles + 10)

# Mark resampling steps
for step in resampled_steps:
    ax4.axvline(x=step, color='orange', alpha=0.7, linestyle=':', 
               label='Resampling' if step == resampled_steps[0] else "")

ax4.set_xlabel('Time Step')
ax4.set_ylabel('Effective Sample Size')
ax4.set_title('Particle Degeneracy and Resampling')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Print performance statistics
final_pos_error = pos_errors[-1]
mean_pos_error = np.mean(pos_errors)
rmse_pos = np.sqrt(np.mean(pos_errors**2))

print(f"\nParticle Filter Performance:")
print(f"  Number of particles: {n_particles}")
print(f"  Final position error: {final_pos_error:.3f}")
print(f"  Mean position error: {mean_pos_error:.3f}")
print(f"  Resampling frequency: {len(resampled_steps)}/{T} time steps")
```

## Conclusion

Particle filters provide a powerful and flexible framework for tracking hidden states in complex dynamic systems. Unlike the Kalman filter, which requires linear dynamics and Gaussian noise, particle filters can handle arbitrary nonlinearities and any noise distribution by representing beliefs as a cloud of weighted samples.

### Key Strengths

- **Flexibility**: Works with any state evolution and observation model.
- **Intuitive**: The particle representation mirrors how we naturally think about uncertainty.
- **Scalable**: Performance degrades gracefully as system complexity increases.
- **Real-time capable**: Sequential updates make it suitable for online applications.

### Limitations and Challenges

- **Computational cost**: Requires many particles for good approximation, especially in high dimensions.
- **Sample impoverishment**: Resampling can reduce particle diversity over time.
- **Curse of dimensionality**: Performance degrades exponentially with state dimension.
- **Tuning required**: Number of particles and resampling thresholds need careful selection.

### When to Use Particle Filters

Particle filters excel when:

- The system is **nonlinear** or has **non-Gaussian noise**.
- You need **real-time** state estimation.
- The state space is **moderate dimensional** (typically < 10-20 dimensions).
- **Computational resources** are available for running many particles.

Consider alternatives when:

- The system is linear-Gaussian (use Kalman filter).
- The state space is very high-dimensional (consider ensemble methods).
- Computational resources are severely limited.
