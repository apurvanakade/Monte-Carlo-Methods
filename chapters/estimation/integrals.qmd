---
title: "Estimating Integrals"

execute:
  echo: false
---

Monte Carlo integration transforms the problem of computing definite integrals into a sampling problem, making it particularly powerful for high-dimensional integration where traditional methods fail.

Consider the problem of estimating the integral $\ell = \int_a^b f(x) \, dx$. The key insight is to rewrite this integral as an expectation. If $X \sim \text{Uniform}(a, b)$, then:
$$\int_a^b f(x) \, dx = (b-a) \mathbb{E}[f(X)]$$

This reformulation allows us to estimate the integral using sample averages. Given $N$ independent samples $X_1, \ldots, X_N \sim \text{Uniform}(a, b)$, our Monte Carlo estimator is:
$$\hat{\ell}_N = \frac{b - a}{N} \sum_{i=1}^{N} f(X_i)$$

::: {.callout-note}
## Geometric Interpretation
We're approximating the area under $f(x)$ by averaging function values at random points and scaling by interval width.
:::

## Statistical Properties

The Monte Carlo integration estimator possesses several important statistical properties. First, it is unbiased:
$$\begin{aligned}
\mathbb{E}[\hat{\ell}_N] &= \frac{b - a}{N} \sum_{i=1}^{N} \mathbb{E}[f(X_i)] = \frac{b - a}{N} \sum_{i=1}^{N} \int_a^b f(x) \frac{1}{b - a} \, dx = \ell
\end{aligned}$$

The variance of our estimator is:
$$\text{Var}(\hat{\ell}_N) = \frac{(b - a)^2}{N} \cdot \sigma^2_f$$
where $\sigma^2_f = \text{Var}(f(X))$ is the variance of $f(X)$ under the uniform distribution.



::: {.callout-important}
## Key Properties

- **Standard Error**: $\text{SE} = \frac{(b-a)\sigma_f}{\sqrt{N}}$

- **Convergence Rate**: $O(1/\sqrt{N})$ regardless of dimension

- **CLT**: $\sqrt{N}(\hat{\ell}_N - \ell) \xrightarrow{d} \mathcal{N}(0, (b-a)^2\sigma^2_f)$
:::

## Function Characteristics and Performance

Monte Carlo integration's performance depends heavily on the function's characteristics. While the convergence rate is theoretically independent of function complexity, practical performance varies significantly with function smoothness and oscillatory behavior.

Functions with high-frequency oscillations present particular challenges for Monte Carlo methods because random sampling may miss important features or fail to adequately capture rapid variations. This example compares Monte Carlo integration performance on two trigonometric functions with dramatically different oscillation frequencies, illustrating how function characteristics affect integration accuracy and convergence behavior.

### Example: Low vs. High Frequency Functions

The following example integrates two functions over the interval $[0, \pi]$:

- **Smooth decay**: $f_1(x) = \sin(x)$ — a smooth, slowly varying function

- **Oscillatory**: $f_2(x) = 25\sin(25x)$ — a rapidly oscillating function with many periods

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

np.random.seed(0)

def monte_carlo_integrate(func, a, b, n_samples):
    x_samples = np.random.uniform(a, b, n_samples)
    return (b - a) * np.mean(func(x_samples))

# Function definitions and configuration
FUNCTIONS = {
    'smooth': {
        'func': lambda x: np.sin(x),
        'label': r'Smooth: $\sin(x)$',
        'color': 'blue',
        'marker': 'o',
        'true_value': 2.0
    },
    'oscillatory': {
        'func': lambda x: 25 * np.sin(25 * x),
        'label': r'Oscillatory: $25\sin(25x)$',
        'color': 'red', 
        'marker': 's',
        'true_value': 2.0
    }
}

# Integration bounds
a, b = 0, np.pi
x = np.linspace(a, b, 10000)
sample_sizes = np.logspace(2, 5, 20).astype(int)

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
ax1, ax2, ax3, ax4 = axes.flatten()

# Plot functions and calculate estimates/errors
estimates = {}
errors = {}

for i, (name, config) in enumerate(FUNCTIONS.items()):
    func = config['func']
    color = config['color']
    
    # Plot function (top row)
    ax = ax1 if i == 0 else ax2
    y_vals = func(x)
    sns.lineplot(x=x, y=y_vals, ax=ax, color=color, linewidth=2)
    ax.fill_between(x, 0, y_vals, alpha=0.3, color=color)
    ax.set_title(config['label'])
    ax.grid(True, alpha=0.3)
    
    # Calculate estimates and errors for all sample sizes
    estimates[name] = [monte_carlo_integrate(func, a, b, n) for n in sample_sizes]
    errors[name] = [abs(est - config['true_value']) for est in estimates[name]]

# Plot estimates vs sample size
ax3.set_title('Estimate vs Sample Size')
for name, config in FUNCTIONS.items():
    ax3.plot(sample_sizes, estimates[name], 
             color=config['color'], marker=config['marker'], 
             label=name.capitalize(), linestyle='-')
    ax3.axhline(config['true_value'], color=config['color'], 
                linestyle='--', label=f'True {name.capitalize()} Value')

ax3.set_xscale('log')
ax3.set_xlabel('Sample Size')
ax3.set_ylabel('Integral Estimate')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot convergence errors
ax4.set_title('Error vs Sample Size')
for name, config in FUNCTIONS.items():
    ax4.loglog(sample_sizes, errors[name], 
               color=config['color'], marker=config['marker'],
               label=name.capitalize(), linewidth=2)

ax4.set_xlabel('Sample Size')
ax4.set_ylabel('Absolute Error')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Multiple estimates at fixed sample size for statistical analysis
n_test = 10000
n_runs = 25

multiple_estimates = {}
for name, config in FUNCTIONS.items():
    multiple_estimates[name] = [
        monte_carlo_integrate(config['func'], a, b, n_test) 
        for _ in range(n_runs)
    ]

# Create comparison DataFrame
comparison_data = []
std_devs = {}

for name, config in FUNCTIONS.items():
    estimates_list = multiple_estimates[name]
    mean_est = np.mean(estimates_list)
    std_dev = np.std(estimates_list)
    std_devs[name] = std_dev
    
    comparison_data.append({
        "Function": name.capitalize(),
        "True Value": config['true_value'],
        "Mean Est": mean_est,
        "Error": abs(mean_est - config['true_value']),
        "Std Dev": std_dev
    })

df_compare = pd.DataFrame(comparison_data)

# Add std ratio (avoiding division by zero)
smooth_std = std_devs['smooth']
osc_std = std_devs['oscillatory']
df_compare['Std Ratio'] = ['-', osc_std / smooth_std if smooth_std > 0 else np.nan]

# Display formatted results
print(df_compare.round(4))
```

The contrast between the smooth $\sin(x)$ and rapidly oscillating $25\sin(25x)$ demonstrates why integration difficulty varies. The high-frequency function has many more sign changes and local extrema that random sampling must capture.

The log-log error plot reveals that while both functions follow the expected $O(1/\sqrt{n})$ convergence rate, the high-frequency function consistently maintains higher absolute errors across all sample sizes. The variance ratio indicates that the oscillatory function's variance is around 70 times higher than the smooth function's variance, illustrating how oscillations increase uncertainty in Monte Carlo estimates.

### Remarks

- **Smooth functions** are well-suited for Monte Carlo integration, converging quickly with relatively few samples.

- **Highly oscillatory functions** present challenges, requiring larger sample sizes and exhibiting higher variance.

- For oscillatory integrals, consider **alternative approaches** such as:
  - Stratified sampling to ensure adequate coverage of oscillation periods
  - Importance sampling with distributions that account for function behavior
  - Transformation techniques to reduce oscillations.

This example demonstrates why understanding your function's characteristics is crucial for effective Monte Carlo integration.

### Variance Bounds Using Derivatives

For smooth functions, we can bound the variance using derivative information. 

::: {.callout-important}
## Theorem (Variance Bound via Derivatives)
If $g$ is differentiable on $[a,b]$ with derivative $g'$, then:
$$\text{Var}(g(X)) \leq (b-a)^2 \cdot \max_{x \in [a,b]} |g'(x)|^2$$
:::

::: {.proof}
Let $M = \max_{x \in [a,b]} |g'(x)|$ and $\mu = \mathbb{E}[g(X)]$. 

Since $g$ is continuous on the compact interval $[a,b]$, it attains its minimum and maximum values. Let $g_{\min} = \min_{x \in [a,b]} g(x)$ and $g_{\max} = \max_{x \in [a,b]} g(x)$.

By the mean value theorem, for any two points in $[a,b]$:
$$g_{\max} - g_{\min} \leq M(b - a)$$

Since $\mu = \mathbb{E}[g(X)]$ lies between $g_{\min}$ and $g_{\max}$, we have for any $x \in [a,b]$:
$$|g(x) - \mu| \leq g_{\max} - g_{\min} \leq M(b - a)$$

Squaring both sides and taking expectations:
$$\text{Var}(g(X)) = \mathbb{E}[(g(X) - \mu)^2] \leq [M(b - a)]^2$$

This gives us the desired bound.

:::

<hr/>


Note that this is a very weak bound and not useful in practice. There are no universal techniques that give us good bounds on $\text{Var}(g(X))$. In practice, we rely on the heuristic that oscillatory functions are high variance. 

## Multidimensional Integration

One of Monte Carlo integration's greatest advantages becomes apparent in higher dimensions. While traditional numerical integration methods (like the trapezoidal rule or Simpson's rule) suffer from the "curse of dimensionality"—requiring exponentially more grid points as dimensions increase—Monte Carlo methods maintain their $O(1/\sqrt{n})$ convergence rate regardless of dimension.

In $d$ dimensions, a grid-based method with $m$ points per dimension requires $m^d$ total evaluations. For even modest accuracy in high dimensions, this becomes computationally prohibitive. Monte Carlo integration, however, uses the same number of random samples regardless of dimension, making it the method of choice for high-dimensional problems in physics, finance, and machine learning.

### Example: Gaussian-like Function in Multiple Dimensions

The following example demonstrates Monte Carlo integration of the function $f(\mathbf{x}) = e^{-\|\mathbf{x}\|^2}$ over the unit hypercube $[0,1]^d$ for dimensions $d = 1, 2, 3, 4, 5$.

```{python}
#| label: fig-multidimensional
#| fig-cap: "Monte Carlo advantage in higher dimensions"

def multidimensional_example():
    """Demonstrate MC integration in higher dimensions"""
    
    # Function: exp(-||x||²) over unit hypercube
    def f_nd(points):
        # Ensure points is always 2D: (n_samples, n_dimensions)
        if points.ndim == 1:
            points = points.reshape(-1, 1)
        return np.exp(-np.sum(points**2, axis=1))
    
    dimensions = [1, 2, 3, 4, 5]
    sample_sizes = np.logspace(2, 3.5, 15).astype(int)  # From 100 to ~3000 samples
    n_runs = 30  # Number of runs for each sample size
    
    colors = ['blue', 'red', 'green', 'orange', 'purple']
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    
    for dim_idx, dim in enumerate(dimensions):
        estimates_std = []
        rel_errors = []
              
        # Get a reference value using the largest sample size
        ref_estimates = []
        ref_n_samples = sample_sizes[-1] * 2  # Use even larger sample for reference
        for _ in range(10):
            if dim == 1:
                points = np.random.uniform(0, 1, (ref_n_samples, 1))
            else:
                points = np.random.uniform(0, 1, (ref_n_samples, dim))
            f_values = f_nd(points)
            ref_estimates.append(np.mean(f_values))
        reference_value = np.mean(ref_estimates)
        
        for n_samples in sample_sizes:
            estimates = []
            for run in range(n_runs):
                # Ensure consistent sampling for all dimensions
                if dim == 1:
                    points = np.random.uniform(0, 1, (n_samples, 1))
                else:
                    points = np.random.uniform(0, 1, (n_samples, dim))
                
                f_values = f_nd(points)
                estimate = np.mean(f_values)
                estimates.append(estimate)
            
            mean_est = np.mean(estimates)
            std_est = np.std(estimates)
            rel_error = abs(mean_est - reference_value) / abs(reference_value)
            
            estimates_std.append(std_est)
            rel_errors.append(rel_error)
        
        # Plot 1: Relative error vs sample size
        ax1.loglog(sample_sizes, rel_errors, 'o-', 
                  color=colors[dim_idx], label=f'Dim {dim}', 
                  linewidth=2, markersize=4)
        
        # Plot 2: Standard deviation vs sample size
        ax2.loglog(sample_sizes, estimates_std, 'o-', 
                  color=colors[dim_idx], label=f'Dim {dim}', 
                  linewidth=2, markersize=4)
    
    # Formatting
    ax1.set_xlabel('Number of Samples')
    ax1.set_ylabel('')
    ax1.set_title('Relative Error vs Sample Size')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    ax2.set_xlabel('Number of Samples')
    ax2.set_ylabel('Standard Deviation')
    ax2.set_title('Standard Deviation vs Sample Size')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # Add theoretical 1/sqrt(n) reference line to std plot
    # Use average std from first point to scale the reference line
    first_stds = []
    for dim_idx, dim in enumerate(dimensions):
        estimates = []
        for run in range(n_runs):
            if dim == 1:
                points = np.random.uniform(0, 1, (sample_sizes[0], 1))
            else:
                points = np.random.uniform(0, 1, (sample_sizes[0], dim))
            f_values = f_nd(points)
            estimate = np.mean(f_values)
            estimates.append(estimate)
        first_stds.append(np.std(estimates))
    
    avg_first_std = np.mean(first_stds)
    scaling_factor = avg_first_std * np.sqrt(sample_sizes[0])
    theoretical_std = scaling_factor / np.sqrt(sample_sizes)
    ax2.loglog(sample_sizes, theoretical_std, 'k--', alpha=0.5, 
              linewidth=2, label='∝ 1/√n')
    ax2.legend()
    
    plt.tight_layout()
    plt.show()

multidimensional_example()
```


The example reveals several important properties of multidimensional integration:

1. **Dimension-Independent Convergence**: The relative error plots demonstrate Monte Carlo's key advantage - convergence behavior remains consistent across all dimensions, avoiding the curse of dimensionality that plagues grid-based methods.

2. **Theoretical Validation**: The standard deviation follows the expected $1/\sqrt{n}$ scaling (shown by the dashed reference line) regardless of dimension, confirming Monte Carlo's theoretical foundation.

3. **Constant Computational Cost**: Unlike deterministic methods requiring $n^d$ evaluations, Monte Carlo uses the same number of function evaluations per sample regardless of dimension.

This dimensional robustness makes Monte Carlo indispensable for high-dimensional applications in physics, statistics, and finance where traditional methods become computationally intractable.

::: {.callout-tip}
## When to Use Monte Carlo Integration

**Advantages:**

- **High dimensions**: Performance doesn't degrade with dimension

- **Complex domains**: Easy to handle irregular integration regions

- **Rough functions**: Works with discontinuous or highly oscillatory functions

**Disadvantages:**

- **Slow convergence**: $O(1/\sqrt{N})$ rate

- **Random error**: Introduces stochastic uncertainty

- **Low dimensions**: Often outperformed by deterministic methods

**Practical Considerations:**

- **Sample size**: Need large $N$ for high precision

- **Function smoothness**: Smoother functions → lower variance → better estimates

- **Variance reduction**: Techniques like antithetic variables can improve efficiency

- **Error estimation**: Use sample variance to estimate uncertainty
:::

## Conclusion

Monte Carlo integration transforms integration into a sampling problem, making it invaluable for high-dimensional problems where traditional quadrature fails. While it converges slowly, its dimension-independent performance and ability to handle complex functions make it essential for modern computational statistics and machine learning applications.
