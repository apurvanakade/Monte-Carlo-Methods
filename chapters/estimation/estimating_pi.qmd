---
title: Estimating $\pi$
date: 2025-08-04 00:38:57
author: Apurva Nakade
toc: true  
execute:
  echo: false
---

```{python}
#| label: fig-estimate-pi-combined
#| fig-cap: "Estimating π using Monte Carlo method with different sample sizes"
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Parameters
np.random.seed(0)
r = 1
N_values = [100, 10000]

# Create side-by-side plots
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Store results for table
results = []

for i, N in enumerate(N_values):
    # Generate N random points in [-r, r] × [-r, r]
    X = np.random.uniform(-r, r, N)
    Y = np.random.uniform(-r, r, N)
    
    # Classify points: inside circle (red) or outside (blue)
    inside_circle = X**2 + Y**2 <= r**2
    colors = np.where(inside_circle, 'red', 'blue')
    
    # Plot on the appropriate subplot
    ax = axes[i]
    
    # Draw circle and square
    theta = np.linspace(0, 2*np.pi, 100)
    ax.plot(r*np.cos(theta), r*np.sin(theta), 'k-', linewidth=2)
    ax.plot([-r, -r, r, r, -r], [-r, r, r, -r, -r], 'k-', linewidth=2)
    
    # Plot points
    ax.scatter(X, Y, c=colors, s=4, alpha=0.6)
    
    # Estimate π
    n_inside = np.sum(inside_circle)
    pi_estimate = 4 * n_inside / N
    
    ax.set_title(f'N = {N:,}\nπ ≈ {pi_estimate:.4f}')
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.axis('equal')
    ax.grid(True, alpha=0.3)
    
    # Store results
    abs_error = abs(pi_estimate - np.pi)
    std_error = np.sqrt(pi_estimate * (4 - pi_estimate) / N)
    
    results.append({
        'Sample Size': f"{N:,}",
        'Points Inside': f"{n_inside:,}",
        'π Estimate': f"{pi_estimate:.4f}",
        'Absolute Error': f"{abs_error:.4f}",
        'Standard Error': f"{std_error:.4f}"
    })

plt.tight_layout()
plt.show()

# Create and display results table
df = pd.DataFrame(results)
print("Monte Carlo π Estimation Results")
print("=" * 50)
print(df.to_string(index=False))
```


We begin with a classic Monte Carlo application: estimating the value of $\pi$ using geometric probability. This example illustrates the core principles of Monte Carlo simulation while providing an intuitive geometric interpretation.

Consider a unit circle inscribed in a square with side length 2, both centered at the origin. The circle has radius $r = 1$ and area $A_{\text{circle}} = \pi$, while the square has area $A_{\text{square}} = 4$. The ratio of these areas is $\pi/4$.

::: {.callout-note}
## Geometric Relationships
- **Circle**: radius $r = 1$, area $A_{\text{circle}} = \pi r^2 = \pi$
- **Square**: side length $2r = 2$, area $A_{\text{square}} = (2r)^2 = 4$
- **Area ratio**: $\frac{A_{\text{circle}}}{A_{\text{square}}} = \frac{\pi}{4}$
:::

If we randomly sample points uniformly within the square, the probability that any point falls inside the inscribed circle equals this area ratio. This geometric probability provides our pathway to estimating $\pi$.

To formulate this as a Monte Carlo problem, let $(X, Y)$ be uniformly distributed on $[-1, 1] \times [-1, 1]$ and define the indicator random variable:
$$I(X, Y) = \mathbf{1}_{\{X^2 + Y^2 \leq 1\}} = \begin{cases}
1 & \text{if } X^2 + Y^2 \leq 1 \text{ (inside circle)} \\
0 & \text{if } X^2 + Y^2 > 1 \text{ (outside circle)}
\end{cases}$$

The expected value of this indicator function gives us the desired probability:
$$\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \leq 1) = \frac{\text{Area of unit circle}}{\text{Area of square}} = \frac{\pi}{4}$$

Therefore $\pi = 4\mathbb{E}[I(X, Y)]$, and given $N$ independent samples $(X_1, Y_1), \ldots, (X_N, Y_N)$, our Monte Carlo estimator is:
$$\hat{\pi}_N = 4 \cdot \frac{1}{N}\sum_{i=1}^{N} I(X_i, Y_i) = \frac{4 \cdot \text{(number of points inside circle)}}{N}$$

## Statistical Properties

This estimator possesses several important statistical properties. First, it is unbiased:
$$\mathbb{E}[\hat{\pi}_N] = 4\mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N} I_i\right] = 4\mathbb{E}[I] = 4 \cdot \frac{\pi}{4} = \pi$$

Since $I \sim \text{Bernoulli}(\pi/4)$, we can compute the variance. The indicator has variance $\text{Var}(I) = \frac{\pi}{4}(1 - \frac{\pi}{4}) = \frac{\pi(4-\pi)}{16}$, which gives our estimator variance:
$$\text{Var}(\hat{\pi}_N) = 16 \cdot \frac{\text{Var}(I)}{N} = \frac{\pi(4-\pi)}{N}$$

The standard error is therefore $\text{SE}(\hat{\pi}_N) = \sqrt{\frac{\pi(4-\pi)}{N}}$.

::: {.callout-important}
## Convergence Rate
The standard error decreases as $O(1/\sqrt{N})$, which is the typical Monte Carlo convergence rate. To gain one decimal place of accuracy, we need approximately 100 times more samples.
:::

By the Central Limit Theorem, for large $N$ we have the asymptotic distribution:
$$\sqrt{N}(\hat{\pi}_N - \pi) \xrightarrow{d} \mathcal{N}(0, \pi(4-\pi))$$

This provides approximate confidence intervals:
$$\hat{\pi}_N \pm z_{\alpha/2} \sqrt{\frac{\pi(4-\pi)}{N}}$$

::: {.callout-tip}
## Monte Carlo Principle
This example demonstrates the fundamental Monte Carlo approach: reformulate a deterministic problem (computing $\pi$) as the expectation of a random variable, then estimate that expectation using sample averages.
:::