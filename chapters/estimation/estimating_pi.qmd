---
title: Estimating $\pi$
date: 2025-08-04 00:38:57
author: Apurva Nakade
toc: true  
execute:
  echo: false
---

We begin with a classic Monte Carlo application: estimating the value of $\pi$ using geometric probability. This example beautifully illustrates the core principles of Monte Carlo simulation while providing an intuitive geometric interpretation.

# The Geometric Method

## Problem Setup

Consider a unit circle inscribed in a square with side length 2, both centered at the origin:

::: {.callout-note}
## Key Geometric Relationships
- **Circle**: radius $r = 1$, area $A_{\text{circle}} = \pi r^2 = \pi$
- **Square**: side length $2r = 2$, area $A_{\text{square}} = (2r)^2 = 4$
- **Area ratio**: $\frac{A_{\text{circle}}}{A_{\text{square}}} = \frac{\pi}{4}$
:::

## The Monte Carlo Insight

**Key insight**: If we randomly sample points uniformly within the square, the probability that any point falls inside the inscribed circle equals the ratio of their areas: $\pi/4$.

This geometric probability provides our pathway to estimating $\pi$.

```{python}
#| label: fig-estimate-pi-combined
#| fig-cap: "Estimating π using Monte Carlo method with different sample sizes"
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Parameters
np.random.seed(0)
r = 1
N_values = [100, 10000]

# Create side-by-side plots
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Store results for table
results = []

for i, N in enumerate(N_values):
    # Generate N random points in [-r, r] × [-r, r]
    X = np.random.uniform(-r, r, N)
    Y = np.random.uniform(-r, r, N)
    
    # Classify points: inside circle (red) or outside (blue)
    inside_circle = X**2 + Y**2 <= r**2
    colors = np.where(inside_circle, 'red', 'blue')
    
    # Plot on the appropriate subplot
    ax = axes[i]
    
    # Draw circle and square
    theta = np.linspace(0, 2*np.pi, 100)
    ax.plot(r*np.cos(theta), r*np.sin(theta), 'k-', linewidth=2)
    ax.plot([-r, -r, r, r, -r], [-r, r, r, -r, -r], 'k-', linewidth=2)
    
    # Plot points
    ax.scatter(X, Y, c=colors, s=4, alpha=0.6)
    
    # Estimate π
    n_inside = np.sum(inside_circle)
    pi_estimate = 4 * n_inside / N
    
    ax.set_title(f'N = {N:,}\nπ ≈ {pi_estimate:.4f}')
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.axis('equal')
    ax.grid(True, alpha=0.3)
    
    # Store results
    abs_error = abs(pi_estimate - np.pi)
    std_error = np.sqrt(pi_estimate * (4 - pi_estimate) / N)
    
    results.append({
        'Sample Size': f"{N:,}",
        'Points Inside': f"{n_inside:,}",
        'π Estimate': f"{pi_estimate:.4f}",
        'Absolute Error': f"{abs_error:.4f}",
        'Standard Error': f"{std_error:.4f}"
    })

plt.tight_layout()
plt.show()

# Create and display results table
df = pd.DataFrame(results)
print("Monte Carlo π Estimation Results")
print("=" * 50)
print(df.to_string(index=False))
```


# Mathematical Formulation

## Random Variable Definition

To apply Monte Carlo estimation, we formulate this as an expectation problem:

**Setup**: Let $(X, Y)$ be uniformly distributed on $[-1, 1] \times [-1, 1]$. 

Define the indicator random variable:
$$I(X, Y) = \mathbf{1}_{\{X^2 + Y^2 \leq 1\}} = \begin{cases}
1 & \text{if } X^2 + Y^2 \leq 1 \text{ (inside circle)} \\
0 & \text{if } X^2 + Y^2 > 1 \text{ (outside circle)}
\end{cases}$$

## The Expectation

The expected value of our indicator function gives us the desired probability:

$$\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \leq 1) = \frac{\text{Area of unit circle}}{\text{Area of square}} = \frac{\pi}{4}$$

Therefore: $\pi = 4\mathbb{E}[I(X, Y)]$

## Monte Carlo Estimator

Given $N$ independent samples $(X_1, Y_1), \ldots, (X_N, Y_N)$, our estimator is:

$$\hat{\pi}_N = 4 \cdot \frac{1}{N}\sum_{i=1}^{N} I(X_i, Y_i) = \frac{4 \cdot \text{(number of points inside circle)}}{N}$$

# Theoretical Properties

## Unbiasedness

Our estimator is unbiased:
$$\mathbb{E}[\hat{\pi}_N] = 4\mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N} I_i\right] = 4\mathbb{E}[I] = 4 \cdot \frac{\pi}{4} = \pi$$

## Variance Analysis

Since $I \sim \text{Bernoulli}(p)$ with $p = \pi/4$:

- **Variance of indicator**: $\text{Var}(I) = p(1-p) = \frac{\pi}{4}\left(1 - \frac{\pi}{4}\right) = \frac{\pi(4-\pi)}{16}$

- **Variance of estimator**: $\text{Var}(\hat{\pi}_N) = 16 \cdot \frac{\text{Var}(I)}{N} = \frac{\pi(4-\pi)}{N}$

- **Standard error**: $\text{SE}(\hat{\pi}_N) = \sqrt{\frac{\pi(4-\pi)}{N}} \approx \frac{1.64}{\sqrt{N}}$

::: {.callout-important}
## Convergence Rate
The standard error decreases as $O(1/\sqrt{N})$, which is the typical Monte Carlo convergence rate. To gain one decimal place of accuracy, we need approximately 100 times more samples.
:::

## Asymptotic Distribution

By the Central Limit Theorem:
$$\sqrt{N}(\hat{\pi}_N - \pi) \xrightarrow{d} \mathcal{N}(0, \pi(4-\pi))$$

This gives us approximate confidence intervals for large $N$:
$$\hat{\pi}_N \pm z_{\alpha/2} \sqrt{\frac{\pi(4-\pi)}{N}}$$
