---
title: Estimating $\pi$
date: 2025-08-04 00:38:57
author: Apurva Nakade
toc: true  
execute:
  echo: false
resources:
  - app/*
---

Adjust the slider below to see how the Monte Carlo estimates are calculated for $\pi$.


```{=html}
<!-- Load Pyodide (shared across apps) -->
<script src="app/pyodide_loader.js"></script>

<style>
    .pi-app-container {
        max-width: 100%;
        margin: 20px 0;
    }
    
    .pi-slider-container {
        margin: 20px 0;
        display: flex;
        align-items: center;
        gap: 10px;
        flex-wrap: wrap;
    }
    
    .pi-slider-container input[type="range"] {
        flex: 1;
        min-width: 200px;
        max-width: 400px;
    }
    
    .pi-viz-container {
        display: flex;
        gap: 20px;
        align-items: flex-start;
        flex-wrap: wrap;
    }
    
    .pi-canvas-wrapper {
        flex: 1;
        min-width: 280px;
        max-width: 600px;
    }
    
    #pi-canvas {
        border: 1px solid #ddd;
        width: 100%;
        height: auto;
        display: block;
    }
    
    .pi-stats-table {
        flex: 0 1 auto;
        min-width: 250px;
        border-collapse: collapse;
        margin-top: 20px;
    }
    
    .pi-stats-table th {
        text-align: left;
        padding: 8px;
        border-bottom: 2px solid #333;
    }
    
    .pi-stats-table td {
        padding: 8px;
        border-bottom: 1px solid #ddd;
    }
    
    .pi-stats-table td:last-child {
        text-align: right;
        font-family: monospace;
    }
    
    .pi-stats-table tr:last-child td {
        font-weight: bold;
    }
    
    /* Mobile responsive adjustments */
    @media (max-width: 768px) {
        .pi-viz-container {
            flex-direction: column;
        }
        
        .pi-canvas-wrapper {
            max-width: 100%;
        }
        
        .pi-stats-table {
            width: 100%;
            margin-top: 0;
        }
    }
</style>

<div class="pi-app-container">
    <div class="pi-slider-container">
        <label for="pi-n-slider">Sample Size: </label>
        <input type="range" id="pi-n-slider" min="100" max="10000" step="100" value="1000">
        <span id="pi-n-value" style="font-weight: bold;">1,000</span>
    </div>

    <div id="pi-loading" style="padding: 20px; text-align: center; color: #666;">
        <p>Loading interactive visualization...</p>
    </div>

    <div id="pi-viz" style="display: none;">
        <div class="pi-viz-container">
            <div class="pi-canvas-wrapper">
                <canvas id="pi-canvas"></canvas>
            </div>
            <table class="pi-stats-table">
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Points Inside Circle</td>
                    <td id="pi-points-inside">-</td>
                </tr>
                <tr>
                    <td>Total Points</td>
                    <td id="pi-total-points">-</td>
                </tr>
                <tr>
                    <td>Ratio (= π/4)</td>
                    <td id="pi-ratio">-</td>
                </tr>
                <tr>
                    <td>π Estimate</td>
                    <td id="pi-estimate">-</td>
                </tr>
            </table>
        </div>
    </div>
</div>

<!-- Load app-specific code -->
<script src="app/pi_app.js"></script>
```


We begin with a classic Monte Carlo application: estimating the value of $\pi$ using geometric probability. This example illustrates the core principles of Monte Carlo simulation while providing an intuitive geometric interpretation.

Consider a unit circle inscribed in a square with side length 2, both centered at the origin. The circle has radius $r = 1$ and area $A_{\text{circle}} = \pi$, while the square has area $A_{\text{square}} = 4$. The ratio of these areas is $\pi/4$.

::: {.callout-note}
## Geometric Relationships
- **Circle**: radius $r = 1$, area $A_{\text{circle}} = \pi r^2 = \pi$
- **Square**: side length $2r = 2$, area $A_{\text{square}} = (2r)^2 = 4$
- **Area ratio**: $\frac{A_{\text{circle}}}{A_{\text{square}}} = \frac{\pi}{4}$
:::

If we randomly sample points uniformly within the square, the probability that any point falls inside the inscribed circle equals this area ratio. This geometric probability provides our pathway to estimating $\pi$.

To formulate this as a Monte Carlo problem, let $(X, Y)$ be uniformly distributed on $[-1, 1] \times [-1, 1]$ and define the indicator random variable:
$$I(X, Y) = \mathbf{1}_{\{X^2 + Y^2 \leq 1\}} = \begin{cases}
1 & \text{if } X^2 + Y^2 \leq 1 \text{ (inside circle)} \\
0 & \text{if } X^2 + Y^2 > 1 \text{ (outside circle)}
\end{cases}$$

The expected value of this indicator function gives us the desired probability:
$$\mathbb{E}[I(X, Y)] = P(X^2 + Y^2 \leq 1) = \frac{\text{Area of unit circle}}{\text{Area of square}} = \frac{\pi}{4}$$

Therefore $\pi = 4\mathbb{E}[I(X, Y)]$, and given $N$ independent samples $(X_1, Y_1), \ldots, (X_N, Y_N)$, our Monte Carlo estimator is:
$$\hat{\pi}_N = 4 \cdot \frac{1}{N}\sum_{i=1}^{N} I(X_i, Y_i) = \frac{4 \cdot \text{(number of points inside circle)}}{N}$$

## Statistical Properties

This estimator possesses several important statistical properties. First, it is unbiased:
$$\mathbb{E}[\hat{\pi}_N] = 4\mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N} I_i\right] = 4\mathbb{E}[I] = 4 \cdot \frac{\pi}{4} = \pi$$

Since $I \sim \text{Bernoulli}(\pi/4)$, we can compute the variance. The indicator has variance $\text{Var}(I) = \frac{\pi}{4}(1 - \frac{\pi}{4}) = \frac{\pi(4-\pi)}{16}$, which gives our estimator variance:
$$\text{Var}(\hat{\pi}_N) = 16 \cdot \frac{\text{Var}(I)}{N} = \frac{\pi(4-\pi)}{N}$$

The standard error is therefore $\text{SE}(\hat{\pi}_N) = \sqrt{\frac{\pi(4-\pi)}{N}}$.

::: {.callout-important}
## Convergence Rate
The standard error decreases as $O(1/\sqrt{N})$, which is the typical Monte Carlo convergence rate. To gain one decimal place of accuracy, we need approximately 100 times more samples.
:::

By the Central Limit Theorem, for large $N$ we have the asymptotic distribution:
$$\sqrt{N}(\hat{\pi}_N - \pi) \xrightarrow{d} \mathcal{N}(0, \pi(4-\pi))$$

This provides approximate confidence intervals:
$$\hat{\pi}_N \pm z_{\alpha/2} \sqrt{\frac{\pi(4-\pi)}{N}}$$

## Convergence Analysis

To empirically demonstrate the convergence properties, we generate multiple π estimates for various sample sizes and examine how the confidence intervals narrow as $N$ increases.

```{python}
#| label: fig-pi-convergence
#| fig-cap: "Convergence analysis of Monte Carlo π estimation. Top: single run estimates with theoretical 95% confidence intervals. Bottom: confidence interval width decay showing the O(1/√N) convergence rate on log-log scale."

import sys
sys.path.insert(0, 'app')
from monte_carlo_pi import analyze_convergence

import numpy as np
import matplotlib.pyplot as plt

# Analysis parameters
N_MIN = 100
N_MAX = 10000
N_STEP = 100
CONFIDENCE_LEVEL = 0.95
RANDOM_SEED = 42

# Generate sample sizes and run analysis
sample_sizes = np.arange(N_MIN, N_MAX + 1, N_STEP)
results = analyze_convergence(sample_sizes, CONFIDENCE_LEVEL, RANDOM_SEED)

# Unpack results
estimates = results['estimates']
ci_widths = results['ci_widths']
lower_bounds = results['lower_bounds']
upper_bounds = results['upper_bounds']

# Create visualization with two vertically stacked subplots
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))

# Top plot: Convergence with confidence intervals
ax1.fill_between(sample_sizes, lower_bounds, upper_bounds, 
                 alpha=0.3, color='blue', label=f'{int(CONFIDENCE_LEVEL*100)}% CI')
ax1.plot(sample_sizes, estimates, 'b-', linewidth=2, label='Single Run Estimate', zorder=5)
ax1.axhline(y=np.pi, color='red', linestyle='--', linewidth=2, 
           label=f'True π = {np.pi:.6f}', zorder=10)

ax1.set_xlabel('Sample Size (N)', fontsize=11)
ax1.set_ylabel('π Estimate', fontsize=11)
ax1.set_title('Convergence to π', fontsize=12, fontweight='bold')
ax1.legend(loc='upper right', fontsize=10)
ax1.grid(True, alpha=0.3)
ax1.set_ylim([2.8, 3.6])

# Bottom plot: CI width decay (log-log scale)
ax2.loglog(sample_sizes, ci_widths, 'blue', linewidth=2,
          marker='o', markersize=3, label=f'{int(CONFIDENCE_LEVEL*100)}% CI Width', alpha=0.7)

# Add reference line showing 1/sqrt(N) slope
reference_line = ci_widths[0] * (sample_sizes[0] / sample_sizes)**0.5
ax2.loglog(sample_sizes, reference_line, 'k--', linewidth=1.5, 
          label='Reference: O(1/√N)', alpha=0.6)

ax2.set_xlabel('Sample Size (N)', fontsize=11)
ax2.set_ylabel('CI Width', fontsize=11)
ax2.set_title('Confidence Interval Decay', fontsize=12, fontweight='bold')
ax2.legend(loc='upper right', fontsize=10)
ax2.grid(True, alpha=0.3, which='both')

plt.tight_layout()
plt.show()
```

The figure demonstrates that as the sample size $N$ increases, the mean estimate approaches the true value of $\pi$ and the confidence interval narrows, confirming the $O(1/\sqrt{N})$ convergence rate.

::: {.callout-tip}
## Monte Carlo Principle
This example demonstrates the fundamental Monte Carlo approach: reformulate a deterministic problem (computing $\pi$) as the expectation of a random variable, then estimate that expectation using sample averages.
:::